{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import hash, dump, load\n",
    "import os\n",
    "\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.CRAR_torch import CRAR\n",
    "from figure8_env import MyEnv as figure8_env\n",
    "import deer.experiment.base_controllers as bc\n",
    "\n",
    "from deer.policies import EpsilonGreedyPolicy, FixedFigure8Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure8_give_rewards = True\n",
    "nn_yaml = 'network_noconv.yaml'\n",
    "high_dim_obs = True\n",
    "internal_dim = 10\n",
    "fname = '2d_obs'\n",
    "set_network = None #['expanded_tcm', 15, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Setup Parameters (copied for convenience)\n",
    "    # ----------------------\n",
    "    figure8_give_rewards = figure8_give_rewards\n",
    "    nn_yaml = nn_yaml\n",
    "    high_dim_obs = high_dim_obs\n",
    "    internal_dim = internal_dim\n",
    "    fname = fname\n",
    "    \n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    steps_per_epoch = 5000\n",
    "    epochs = 50\n",
    "    steps_per_test = 1000\n",
    "    period_btw_summary_perfs = 1\n",
    "\n",
    "    # ----------------------\n",
    "    # Temporal Processing Parameters\n",
    "    # ----------------------\n",
    "    nstep = 15 #20\n",
    "    nstep_decay = 0.8\n",
    "    expand_tcm = True\n",
    "    encoder_type = 'regular'\n",
    "    \n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    frame_skip = 2\n",
    "    show_rewards = False\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    learning_rate = 1*1E-4\n",
    "    learning_rate_decay = 1.0\n",
    "    discount = 0.9\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 1.0\n",
    "    epsilon_decay = 1000\n",
    "    update_frequency = 1\n",
    "    replay_memory_size = 100000 #50000\n",
    "    batch_size = 64\n",
    "    freeze_interval = 1000\n",
    "    deterministic = False\n",
    "    \n",
    "    # ----------------------\n",
    "    # Learning algo parameters\n",
    "    # ----------------------\n",
    "    # T, entropy_neighbor, entropy_random, volume, gamma, R, Q, variational\n",
    "    #loss_weights = [5E-3, 1E-3, 5E-3, 5E-3, 5E-3, 5E-3, 1.]\n",
    "    loss_weights = [0, 0, 0, 0, 0, 0, 1., 0.]\n",
    "    #loss_weights = [0., 0., 0., 0., 0., 0., 1., 2E-4]\n",
    "    #loss_weights = [5E-3, 1E-3, 5E-3, 5E-3, 5E-3, 5E-3, 0., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = Defaults()\n",
    "with open(f'params/{fname}.p', 'wb') as f:\n",
    "    pickle.dump(parameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end gathering data\n"
     ]
    }
   ],
   "source": [
    "if parameters.deterministic:\n",
    "    rng = np.random.RandomState(123456)\n",
    "else:\n",
    "    rng = np.random.RandomState()\n",
    "\n",
    "# --- Instantiate environment ---\n",
    "env = figure8_env(\n",
    "    give_rewards=figure8_give_rewards,\n",
    "    intern_dim=internal_dim,\n",
    "    high_dim_obs=high_dim_obs,\n",
    "    show_rewards=parameters.show_rewards\n",
    "    )\n",
    "\n",
    "# --- Instantiate learning_algo ---\n",
    "learning_algo = CRAR(\n",
    "    env,\n",
    "    parameters.freeze_interval,\n",
    "    parameters.batch_size,\n",
    "    rng,\n",
    "    high_int_dim=False,\n",
    "    internal_dim=internal_dim, lr=parameters.learning_rate,\n",
    "    nn_yaml=nn_yaml, double_Q=True,\n",
    "    loss_weights=parameters.loss_weights,\n",
    "    nstep=parameters.nstep, nstep_decay=parameters.nstep_decay,\n",
    "    encoder_type=parameters.encoder_type,\n",
    "    expand_tcm=parameters.expand_tcm\n",
    "    )\n",
    "\n",
    "if figure8_give_rewards:\n",
    "    train_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.2,\n",
    "        consider_valid_transitions=False\n",
    "        )\n",
    "    test_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.\n",
    "        )\n",
    "else:\n",
    "    train_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng, epsilon=0.2,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "    test_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "\n",
    "# --- Instantiate agent ---\n",
    "agent = NeuralAgent(\n",
    "    env, learning_algo,\n",
    "    parameters.replay_memory_size,\n",
    "    1, parameters.batch_size, rng,\n",
    "    train_policy=train_policy, test_policy=test_policy)\n",
    "if set_network is not None:\n",
    "    agent.setNetwork(\n",
    "        f'{set_network[0]}/fname', nEpoch=set_network[1],\n",
    "        encoder_only=set_network[2]\n",
    "        )\n",
    "\n",
    "agent.run(10, 500)\n",
    "print(\"end gathering data\")\n",
    "\n",
    "# --- Bind controllers to the agent ---\n",
    "# Before every training epoch (periodicity=1), we want to print a summary of the agent's epsilon, discount and \n",
    "# learning rate as well as the training epoch number.\n",
    "agent.attach(bc.VerboseController(\n",
    "    evaluate_on='epoch', \n",
    "    periodicity=1))\n",
    "\n",
    "# Learning rate may follow a scheduler\n",
    "agent.attach(bc.LearningRateController(\n",
    "    initial_learning_rate=parameters.learning_rate, \n",
    "    learning_rate_decay=parameters.learning_rate_decay,\n",
    "    periodicity=1))\n",
    "\n",
    "# During training epochs, we want to train the agent after every [parameters.update_frequency] action it takes.\n",
    "# Plus, we also want to display after each training episode (!= than after every training) the average bellman\n",
    "# residual and the average of the V values obtained during the last episode, hence the two last arguments.\n",
    "agent.attach(bc.TrainerController(\n",
    "    evaluate_on='action', \n",
    "    periodicity=parameters.update_frequency, \n",
    "    show_episode_avg_V_value=True, \n",
    "    show_avg_Bellman_residual=True))\n",
    "\n",
    "# We wish to discover, among all versions of our neural network (i.e., after every training epoch), which one \n",
    "# has the highest validation score.\n",
    "# To achieve this goal, one can use the FindBestController along with an InterleavedTestEpochControllers. It is \n",
    "# important that the validationID is the same than the id argument of the InterleavedTestEpochController.\n",
    "# The FindBestController will dump on disk the validation scores for each and every network, as well as the \n",
    "# structure of the neural network having the best validation score. These dumps can then used to plot the evolution \n",
    "# of the validation and test scores (see below) or simply recover the resulting neural network for your \n",
    "# application.\n",
    "agent.attach(bc.FindBestController(\n",
    "    validationID=figure8_env.VALIDATION_MODE,\n",
    "    testID=None,\n",
    "    unique_fname=fname, savefrequency=5))\n",
    "\n",
    "# All previous controllers control the agent during the epochs it goes through. However, we want to interleave a \n",
    "# \"validation epoch\" between each training epoch. For each validation epoch, we want also to display the sum of all \n",
    "# rewards obtained, hence the showScore=True. Finally, we want to call the summarizePerformance method of ALE_env \n",
    "# every [parameters.period_btw_summary_perfs] *validation* epochs.\n",
    "agent.attach(bc.InterleavedTestEpochController(\n",
    "    id=figure8_env.VALIDATION_MODE, \n",
    "    epoch_length=parameters.steps_per_test,\n",
    "    periodicity=1,\n",
    "    show_score=True,\n",
    "    summarize_every=1,\n",
    "    unique_fname=fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1984,  0.0676, -0.3974,  0.0646,  0.0165, -0.1044, -0.1921, -0.0171,\n",
      "         0.1847,  0.2534]) tensor([ 0.4255,  0.4206, -0.1759, -0.0484,  0.2495, -0.0982, -0.3931,  0.3953,\n",
      "         0.0755,  0.7056]) tensor([ 0.1984,  0.0676, -0.3974,  0.0646,  0.0165, -0.1044, -0.1921, -0.0171,\n",
      "         0.1847,  0.2534])\n",
      "R[0]\n",
      "tensor([-0.1524], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07336307527124881; R = 0.02658372480235994;                 Gamma = 1.1592149279117585; Q = 0.012506741979392246;\n",
      "Entropy Neighbor = 0.9994328215122222;                 Entropy Random = 0.9961826401948929;                 Volume = 0.0560624424405396; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1831, -0.0947, -0.4664, -0.4208, -0.4226,  0.1740, -0.0348,  0.3075,\n",
      "        -0.1394,  0.0595]) tensor([ 0.4099,  0.2549, -0.2516, -0.5381, -0.1874,  0.1722, -0.2431,  0.7178,\n",
      "        -0.2437,  0.5190]) tensor([ 0.1831, -0.0948, -0.4664, -0.4208, -0.4226,  0.1740, -0.0348,  0.3075,\n",
      "        -0.1394,  0.0595])\n",
      "R[0]\n",
      "tensor([-0.1579], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07216914600133896; R = 0.02150456706061959;                 Gamma = 1.159385747909546; Q = 0.007986104957624775;\n",
      "Entropy Neighbor = 0.9998947006464004;                 Entropy Random = 0.9988085718154908;                 Volume = 0.06092936134338379; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7960, -0.0584, -0.5877,  0.1216, -0.0056, -0.5399, -0.2438,  0.1423,\n",
      "        -0.4283, -0.0984]) tensor([-0.5687,  0.3131, -0.3650,  0.0131,  0.2400, -0.5370, -0.4544,  0.5495,\n",
      "        -0.5410,  0.3748]) tensor([-0.7960, -0.0584, -0.5877,  0.1216, -0.0056, -0.5399, -0.2438,  0.1423,\n",
      "        -0.4283, -0.0984])\n",
      "R[0]\n",
      "tensor([-0.1789], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07365882696211339; R = 0.029443158477544785;                 Gamma = 1.0833148711919784; Q = 0.0015367479360920697;\n",
      "Entropy Neighbor = 0.9999972363710403;                 Entropy Random = 0.9999802882671356;                 Volume = 0.0; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7718, -0.0343, -0.6311,  0.0884, -0.0202, -0.4951, -0.2489,  0.1282,\n",
      "        -0.3605, -0.1469]) tensor([-0.5438,  0.3368, -0.4081, -0.0218,  0.2262, -0.4933, -0.4589,  0.5360,\n",
      "        -0.4721,  0.3262]) tensor([-0.7718, -0.0344, -0.6311,  0.0884, -0.0202, -0.4951, -0.2489,  0.1282,\n",
      "        -0.3605, -0.1469])\n",
      "R[0]\n",
      "tensor([-0.1737], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07366525894403457; R = 0.028886558551341295;                 Gamma = 1.0792569761276245; Q = 0.001644832006483739;\n",
      "Entropy Neighbor = 0.9999970759153366;                 Entropy Random = 0.9998546788692474;                 Volume = 0.0; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7727, -0.0323, -0.6299,  0.0894, -0.0194, -0.4956, -0.2479,  0.1278,\n",
      "        -0.3597, -0.1461]) tensor([-0.5447,  0.3388, -0.4069, -0.0208,  0.2270, -0.4937, -0.4578,  0.5356,\n",
      "        -0.4712,  0.3270]) tensor([-0.7727, -0.0323, -0.6299,  0.0894, -0.0194, -0.4956, -0.2479,  0.1278,\n",
      "        -0.3597, -0.1461])\n",
      "R[0]\n",
      "tensor([-0.1736], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.0739708525389433; R = 0.027834037523716687;                 Gamma = 1.0855565092563628; Q = 0.001636919232805667;\n",
      "Entropy Neighbor = 0.9999758633375168;                 Entropy Random = 0.9995644147396088;                 Volume = 0.0; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7403, -0.0439, -0.6576,  0.0530, -0.0334, -0.4555, -0.2570,  0.1279,\n",
      "        -0.3304, -0.1750]) tensor([-0.5118,  0.3271, -0.4347, -0.0582,  0.2136, -0.4546, -0.4667,  0.5362,\n",
      "        -0.4414,  0.2985]) tensor([-0.7403, -0.0439, -0.6576,  0.0530, -0.0334, -0.4555, -0.2570,  0.1279,\n",
      "        -0.3304, -0.1750])\n",
      "R[0]\n",
      "tensor([-0.1718], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07407484991848469; R = 0.028193564385175705;                 Gamma = 1.0833170807361603; Q = 0.002072640499482304;\n",
      "Entropy Neighbor = 0.9999964009523392;                 Entropy Random = 0.9999113404750823;                 Volume = 0.0; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7446, -0.0368, -0.6587,  0.0494, -0.0347, -0.4493, -0.2536,  0.1304,\n",
      "        -0.3275, -0.1765]) tensor([-0.5161,  0.3340, -0.4358, -0.0619,  0.2123, -0.4484, -0.4634,  0.5386,\n",
      "        -0.4383,  0.2969]) tensor([-0.7446, -0.0368, -0.6587,  0.0494, -0.0347, -0.4493, -0.2536,  0.1304,\n",
      "        -0.3275, -0.1765])\n",
      "R[0]\n",
      "tensor([-0.1714], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07408488245308399; R = 0.02779258005321026;                 Gamma = 1.0850141527652741; Q = 0.00231722879510653;\n",
      "Entropy Neighbor = 0.9998226860761642;                 Entropy Random = 0.9972378560304642;                 Volume = 0.0; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7120, -0.0386, -0.6801,  0.0206, -0.0348, -0.4133, -0.2621,  0.1262,\n",
      "        -0.2937, -0.2046]) tensor([-0.4860,  0.3189, -0.5041, -0.0321,  0.1818, -0.3705, -0.5163,  0.4897,\n",
      "        -0.4157,  0.2750]) tensor([-0.7119, -0.0386, -0.6801,  0.0206, -0.0348, -0.4134, -0.2621,  0.1262,\n",
      "        -0.2937, -0.2046])\n",
      "R[0]\n",
      "tensor([-0.1291], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07411441412568093; R = 0.027530845172703267;                 Gamma = 1.0850992681980134; Q = 0.0022929690704122548;\n",
      "Entropy Neighbor = 0.9990145256519317;                 Entropy Random = 0.9771144757270813;                 Volume = 0.0; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7150, -0.0427, -0.6657,  0.0155, -0.0420, -0.4034, -0.2499,  0.1411,\n",
      "        -0.3092, -0.1855]) tensor([-0.4865,  0.3273, -0.4431, -0.0965,  0.2050, -0.4032, -0.4595,  0.5494,\n",
      "        -0.4193,  0.2877]) tensor([-0.7152, -0.0426, -0.6657,  0.0159, -0.0418, -0.4037, -0.2500,  0.1408,\n",
      "        -0.3091, -0.1854])\n",
      "R[0]\n",
      "tensor([-0.1704], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07408417844772339; R = 0.02712017071992159;                 Gamma = 1.0894097819328308; Q = 0.0025256606770431064;\n",
      "Entropy Neighbor = 0.9988265620470047;                 Entropy Random = 0.9592576593160629;                 Volume = 0.0; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6656, -0.0595, -0.6692, -0.0160, -0.0703, -0.3699, -0.2411,  0.1445,\n",
      "        -0.2791, -0.1894]) tensor([-0.4386,  0.3158, -0.4639, -0.0970,  0.1744, -0.3504, -0.4736,  0.5305,\n",
      "        -0.4071,  0.2901]) tensor([-0.6656, -0.0595, -0.6692, -0.0160, -0.0703, -0.3699, -0.2411,  0.1445,\n",
      "        -0.2791, -0.1894])\n",
      "R[0]\n",
      "tensor([-0.1400], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07403911215066909; R = 0.026283010326325892;                 Gamma = 1.0906685535907745; Q = 0.002110410812225382;\n",
      "Entropy Neighbor = 0.9982702516317368;                 Entropy Random = 0.9462881664037704;                 Volume = 0.0; VAE = 0.0\n",
      "Average (on the epoch) training loss: 0.0036630255966668074\n",
      "Episode average V value: 0.10800681788697838\n",
      "epoch 1:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 12\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m set_network \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     agent\u001b[38;5;241m.\u001b[39msetNetwork(\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mset_network[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/fname\u001b[39m\u001b[38;5;124m'\u001b[39m, nEpoch\u001b[38;5;241m=\u001b[39mset_network[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     10\u001b[0m         encoder_only\u001b[38;5;241m=\u001b[39mset_network[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     11\u001b[0m         )\n\u001b[0;32m---> 12\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# --- Show results ---\u001b[39;00m\n\u001b[1;32m     15\u001b[0m basename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fname\n",
      "File \u001b[0;32m~/Code/deer/deer/agent.py:298\u001b[0m, in \u001b[0;36mNeuralAgent.run\u001b[0;34m(self, n_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03mThis function encapsulates the inference and the learning.\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03mIf the agent is in train mode (mode = -1):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m    maximum number of steps for a given epoch\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_non_train(n_epochs, epoch_length)\n",
      "File \u001b[0;32m~/Code/deer/deer/agent.py:325\u001b[0m, in \u001b[0;36mNeuralAgent._run_train\u001b[0;34m(self, n_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    323\u001b[0m         nbr_steps_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runEpisode(nbr_steps_left)\n\u001b[1;32m    324\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 325\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_controllers: \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monEpochEnd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_environment\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_controllers: c\u001b[38;5;241m.\u001b[39monEnd(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Code/deer/deer/experiment/base_controllers.py:346\u001b[0m, in \u001b[0;36mInterleavedTestEpochController.onEpochEnd\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summary_periodicity \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summary_counter \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summary_periodicity \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 346\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummarizeTestPerformance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unique_fname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m agent\u001b[38;5;241m.\u001b[39mresumeTrainingMode()\n",
      "File \u001b[0;32m~/Code/deer/deer/agent.py:188\u001b[0m, in \u001b[0;36mNeuralAgent.summarizeTestPerformance\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AgentError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot summarize test performance outside test environment.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummarizePerformance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tmp_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learning_algo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/deer/examples/test_CRAR/figure8_env.py:190\u001b[0m, in \u001b[0;36mMyEnv.summarizePerformance\u001b[0;34m(self, test_data_set, learning_algo, fname)\u001b[0m\n\u001b[1;32m    188\u001b[0m observations_tcm\u001b[38;5;241m.\u001b[39mappend(tcm_obs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    189\u001b[0m reward_locs_tcm\u001b[38;5;241m.\u001b[39mappend(reward_locs[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 190\u001b[0m agent_location \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    191\u001b[0m x_locations\u001b[38;5;241m.\u001b[39mappend(agent_location \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m MyEnv\u001b[38;5;241m.\u001b[39mHEIGHT)\n\u001b[1;32m    192\u001b[0m y_locations\u001b[38;5;241m.\u001b[39mappend(agent_location \u001b[38;5;241m%\u001b[39m MyEnv\u001b[38;5;241m.\u001b[39mHEIGHT)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"params\")\n",
    "except Exception:\n",
    "    pass\n",
    "dump(vars(parameters), \"params/\" + fname + \".jldump\")\n",
    "#agent.gathering_data=False\n",
    "if set_network is not None:\n",
    "    agent.setNetwork(\n",
    "        f'{set_network[0]}/fname', nEpoch=set_network[1],\n",
    "        encoder_only=set_network[2]\n",
    "        )\n",
    "agent.run(parameters.epochs, parameters.steps_per_epoch)\n",
    "\n",
    "# --- Show results ---\n",
    "basename = \"scores/\" + fname\n",
    "scores = load(basename + \"_scores.jldump\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.setNetwork(fname, nEpoch=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._in_episode = True\n",
    "agent._mode = 0 # Testing mode with plan_depth=0\n",
    "initState = env.reset(agent._mode)\n",
    "inputDims = env.inputDimensions()\n",
    "\n",
    "for i in range(len(inputDims)):\n",
    "    if inputDims[i][0] > 1:\n",
    "        agent._state[i][1:] = initState[i][1:]\n",
    "agent._Vs_on_last_episode = []\n",
    "is_terminal = False\n",
    "reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for i in range(100):\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    plt.figure()\n",
    "    plt.imshow(np.flip(_obs.squeeze()))\n",
    "    plt.show()\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "    V, action, reward, _ = agent._step()\n",
    "    print(action)\n",
    "    agent._Vs_on_last_episode.append(V)\n",
    "    is_terminal = env.inTerminalState()\n",
    "    if is_terminal: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "obs = env.observe()\n",
    "_obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "_obs = np.flip(_obs.squeeze())\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "im = ax.imshow(np.zeros(_obs.shape))\n",
    "\n",
    "def init():\n",
    "    plt.cla()\n",
    "    im = ax.imshow(_obs)\n",
    "    return [im]\n",
    "\n",
    "def animate(i, *args, **kwargs):\n",
    "    plt.cla()\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    _obs = np.flip(_obs.squeeze())\n",
    "    im = ax.imshow(_obs)\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "        V, action, reward, _ = agent._step()\n",
    "        agent._Vs_on_last_episode.append(V)\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init, \n",
    "     frames=100, blit=False, repeat=True)\n",
    "ani.save(f'figs/{fname}/behavior.gif', writer=\"ffmpeg\", fps = 15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
