{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import hash, dump, load\n",
    "import os\n",
    "\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.CRAR_torch import CRAR\n",
    "from figure8_env import MyEnv as figure8_env\n",
    "import deer.experiment.base_controllers as bc\n",
    "\n",
    "from deer.policies import EpsilonGreedyPolicy, FixedFigure8Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure8_give_rewards = True\n",
    "nn_yaml = 'network_noconv.yaml'\n",
    "higher_dim_obs = False\n",
    "internal_dim = 10\n",
    "fname = 'expanded_tcm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Setup Parameters (copied for convenience)\n",
    "    # ----------------------\n",
    "    figure8_give_rewards = figure8_give_rewards\n",
    "    nn_yaml = nn_yaml\n",
    "    higher_dim_obs = higher_dim_obs\n",
    "    internal_dim = internal_dim\n",
    "    fname = fname\n",
    "    \n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    steps_per_epoch = 5000\n",
    "    epochs = 50\n",
    "    steps_per_test = 1000\n",
    "    period_btw_summary_perfs = 1\n",
    "\n",
    "    # ----------------------\n",
    "    # Temporal Processing Parameters\n",
    "    # ----------------------\n",
    "    nstep = 15 #20\n",
    "    nstep_decay = 0.8\n",
    "    expand_tcm = True\n",
    "    encoder_type = 'regular'\n",
    "    \n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    frame_skip = 2\n",
    "    show_rewards = False\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    learning_rate = 1*1E-4\n",
    "    learning_rate_decay = 1.0\n",
    "    discount = 0.9\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 1.0\n",
    "    epsilon_decay = 1000\n",
    "    update_frequency = 1\n",
    "    replay_memory_size = 100000 #50000\n",
    "    batch_size = 64\n",
    "    freeze_interval = 1000\n",
    "    deterministic = False\n",
    "    \n",
    "    # ----------------------\n",
    "    # Learning algo parameters\n",
    "    # ----------------------\n",
    "    # T, entropy_neighbor, entropy_random, volume, gamma, R, Q, variational\n",
    "    #loss_weights = [5E-3, 1E-3, 5E-3, 5E-3, 5E-3, 5E-3, 1.]\n",
    "    loss_weights = [0, 0, 0, 0, 0, 0, 1., 0.]\n",
    "    #loss_weights = [0., 0., 0., 0., 0., 0., 1., 2E-4]\n",
    "    #loss_weights = [5E-3, 1E-3, 5E-3, 5E-3, 5E-3, 5E-3, 0., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = Defaults()\n",
    "with open(f'params/{fname}.p', 'wb') as f:\n",
    "    pickle.dump(parameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end gathering data\n"
     ]
    }
   ],
   "source": [
    "if parameters.deterministic:\n",
    "    rng = np.random.RandomState(123456)\n",
    "else:\n",
    "    rng = np.random.RandomState()\n",
    "\n",
    "# --- Instantiate environment ---\n",
    "env = figure8_env(\n",
    "    give_rewards=figure8_give_rewards,\n",
    "    intern_dim=internal_dim,\n",
    "    higher_dim_obs=higher_dim_obs,\n",
    "    show_rewards=parameters.show_rewards\n",
    "    )\n",
    "\n",
    "# --- Instantiate learning_algo ---\n",
    "learning_algo = CRAR(\n",
    "    env,\n",
    "    parameters.freeze_interval,\n",
    "    parameters.batch_size,\n",
    "    rng,\n",
    "    high_int_dim=False,\n",
    "    internal_dim=internal_dim, lr=parameters.learning_rate,\n",
    "    nn_yaml=nn_yaml, double_Q=True,\n",
    "    loss_weights=parameters.loss_weights,\n",
    "    nstep=parameters.nstep, nstep_decay=parameters.nstep_decay,\n",
    "    encoder_type=parameters.encoder_type,\n",
    "    expand_tcm=parameters.expand_tcm\n",
    "    )\n",
    "\n",
    "if figure8_give_rewards:\n",
    "    train_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.2,\n",
    "        consider_valid_transitions=False\n",
    "        )\n",
    "    test_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.\n",
    "        )\n",
    "else:\n",
    "    train_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng, epsilon=0.2,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "    test_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "\n",
    "# --- Instantiate agent ---\n",
    "agent = NeuralAgent(\n",
    "    env, learning_algo,\n",
    "    parameters.replay_memory_size,\n",
    "    1, parameters.batch_size, rng,\n",
    "    train_policy=train_policy, test_policy=test_policy)\n",
    "#agent.setNetwork('test', nEpoch=7)\n",
    "\n",
    "agent.run(10, 500)\n",
    "print(\"end gathering data\")\n",
    "\n",
    "# --- Bind controllers to the agent ---\n",
    "# Before every training epoch (periodicity=1), we want to print a summary of the agent's epsilon, discount and \n",
    "# learning rate as well as the training epoch number.\n",
    "agent.attach(bc.VerboseController(\n",
    "    evaluate_on='epoch', \n",
    "    periodicity=1))\n",
    "\n",
    "# Learning rate may follow a scheduler\n",
    "agent.attach(bc.LearningRateController(\n",
    "    initial_learning_rate=parameters.learning_rate, \n",
    "    learning_rate_decay=parameters.learning_rate_decay,\n",
    "    periodicity=1))\n",
    "\n",
    "# During training epochs, we want to train the agent after every [parameters.update_frequency] action it takes.\n",
    "# Plus, we also want to display after each training episode (!= than after every training) the average bellman\n",
    "# residual and the average of the V values obtained during the last episode, hence the two last arguments.\n",
    "agent.attach(bc.TrainerController(\n",
    "    evaluate_on='action', \n",
    "    periodicity=parameters.update_frequency, \n",
    "    show_episode_avg_V_value=True, \n",
    "    show_avg_Bellman_residual=True))\n",
    "\n",
    "# We wish to discover, among all versions of our neural network (i.e., after every training epoch), which one \n",
    "# has the highest validation score.\n",
    "# To achieve this goal, one can use the FindBestController along with an InterleavedTestEpochControllers. It is \n",
    "# important that the validationID is the same than the id argument of the InterleavedTestEpochController.\n",
    "# The FindBestController will dump on disk the validation scores for each and every network, as well as the \n",
    "# structure of the neural network having the best validation score. These dumps can then used to plot the evolution \n",
    "# of the validation and test scores (see below) or simply recover the resulting neural network for your \n",
    "# application.\n",
    "agent.attach(bc.FindBestController(\n",
    "    validationID=figure8_env.VALIDATION_MODE,\n",
    "    testID=None,\n",
    "    unique_fname=fname))\n",
    "\n",
    "# All previous controllers control the agent during the epochs it goes through. However, we want to interleave a \n",
    "# \"validation epoch\" between each training epoch. For each validation epoch, we want also to display the sum of all \n",
    "# rewards obtained, hence the showScore=True. Finally, we want to call the summarizePerformance method of ALE_env \n",
    "# every [parameters.period_btw_summary_perfs] *validation* epochs.\n",
    "agent.attach(bc.InterleavedTestEpochController(\n",
    "    id=figure8_env.VALIDATION_MODE, \n",
    "    epoch_length=parameters.steps_per_test,\n",
    "    periodicity=1,\n",
    "    show_score=True,\n",
    "    summarize_every=1,\n",
    "    unique_fname=fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0742, -0.0394, -0.2896,  0.2517,  0.2032,  0.0651, -0.2315,  0.1072,\n",
      "        -0.0397,  0.2292]) tensor([ 0.0715, -0.4078, -0.3881,  0.3793,  0.1039,  0.0154, -0.2242,  0.5148,\n",
      "        -0.2953,  0.2068]) tensor([ 0.0742, -0.0394, -0.2896,  0.2517,  0.2032,  0.0651, -0.2315,  0.1072,\n",
      "        -0.0397,  0.2292])\n",
      "R[0]\n",
      "tensor([0.0162], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.0425728373080492; R = 0.0009693134417029796;                 Gamma = 0.44797117495536803; Q = 0.0029143289459316294;\n",
      "Entropy Neighbor = 0.8682538073062896;                 Entropy Random = 0.7938048279285431;                 Volume = 2.7919329702854155e-05; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0417,  0.1378, -0.9789,  0.5386, -0.0500,  0.0150, -0.3547, -0.1946,\n",
      "        -0.8509,  0.1272]) tensor([-0.0447, -0.2511, -1.0928,  0.6454, -0.1482, -0.0363, -0.3515,  0.2202,\n",
      "        -1.1151,  0.1041]) tensor([-0.0417,  0.1378, -0.9789,  0.5386, -0.0500,  0.0150, -0.3547, -0.1946,\n",
      "        -0.8509,  0.1272])\n",
      "R[0]\n",
      "tensor([0.0430], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.043778159014880656; R = 0.0011643579704905278;                 Gamma = 0.44520412570238116; Q = 0.0025237562544825776;\n",
      "Entropy Neighbor = 0.9177943572998047;                 Entropy Random = 0.7708462195396424;                 Volume = 0.0; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 1.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0174, -0.3168, -0.7856,  0.0554,  0.6956,  0.4586, -0.2622,  0.5141,\n",
      "        -0.2315, -0.0971]) tensor([ 0.0197, -0.7050, -0.9112,  0.1965,  0.6163,  0.4151, -0.2566,  0.9435,\n",
      "        -0.5226, -0.1310]) tensor([ 0.0174, -0.3168, -0.7856,  0.0554,  0.6956,  0.4586, -0.2622,  0.5141,\n",
      "        -0.2315, -0.0971])\n",
      "R[0]\n",
      "tensor([-0.0037], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.04395565143972635; R = 0.0005640636324096704;                 Gamma = 0.4244343801736832; Q = 0.00054527670709831;\n",
      "Entropy Neighbor = 0.8780286538600922;                 Entropy Random = 0.6082572311162948;                 Volume = 0.0; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0108, -0.3349, -0.8684, -0.0136,  0.7199,  0.3846, -0.2081,  0.4713,\n",
      "        -0.2695, -0.1438]) tensor([ 0.0133, -0.7230, -0.9928,  0.1261,  0.6396,  0.3386, -0.2032,  0.9005,\n",
      "        -0.5596, -0.1779]) tensor([ 0.0108, -0.3349, -0.8684, -0.0136,  0.7199,  0.3846, -0.2081,  0.4713,\n",
      "        -0.2695, -0.1438])\n",
      "R[0]\n",
      "tensor([0.0001], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.04526228424906731; R = 0.000888309401110746;                 Gamma = 0.4310867203474045; Q = 0.0008014093081201282;\n",
      "Entropy Neighbor = 0.8684646495580673;                 Entropy Random = 0.5497848035097123;                 Volume = 0.0; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0032, -0.1078, -0.6794,  0.3369,  0.3277,  0.2786, -0.2556,  0.2807,\n",
      "        -0.3740,  0.1208]) tensor([-0.0034, -0.4920, -0.7958,  0.4645,  0.2379,  0.2308, -0.2500,  0.7012,\n",
      "        -0.6501,  0.0920]) tensor([ 0.1107, -0.1704, -0.6525,  0.3207,  0.2533,  0.3295, -0.3485,  0.2488,\n",
      "        -0.3920,  0.0680])\n",
      "R[0]\n",
      "tensor([0.0068], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.046030235283076766; R = 0.0007835719484501169;                 Gamma = 0.4353845887184143; Q = 0.00042643492131446694;\n",
      "Entropy Neighbor = 0.8810318472385407;                 Entropy Random = 0.5215545375943184;                 Volume = 0.0; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0870, -0.3368, -0.7679,  0.1111,  0.5270,  0.4263, -0.2477,  0.3779,\n",
      "        -0.3300, -0.1017]) tensor([ 0.0863, -0.7126, -0.8665,  0.2508,  0.4351,  0.3503, -0.2459,  0.8058,\n",
      "        -0.5927, -0.1230]) tensor([ 0.0941, -0.3320, -0.7770,  0.1015,  0.5142,  0.4104, -0.2428,  0.3591,\n",
      "        -0.3284, -0.1005])\n",
      "R[0]\n",
      "tensor([0.0185], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.046624218232929705; R = 0.0009068641717749414;                 Gamma = 0.4370938049554825; Q = 0.00031302807750125793;\n",
      "Entropy Neighbor = 0.8907460689544677;                 Entropy Random = 0.4936377935409546;                 Volume = 3.556271269917488e-05; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0895, -0.1553, -0.7772,  0.3050,  0.2348,  0.2623, -0.2856,  0.1859,\n",
      "        -0.5191,  0.0261]) tensor([ 0.0872, -0.5437, -0.8944,  0.4296,  0.1447,  0.2140, -0.2803,  0.6075,\n",
      "        -0.7956, -0.0014]) tensor([ 0.1195, -0.2509, -0.7928,  0.2405,  0.2742,  0.3185, -0.2973,  0.1681,\n",
      "        -0.5132, -0.0551])\n",
      "R[0]\n",
      "tensor([0.0219], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.04674660465121269; R = 0.0006468988537380937;                 Gamma = 0.43567454314231874; Q = 0.00014979240517277504;\n",
      "Entropy Neighbor = 0.8909724644422531;                 Entropy Random = 0.47239267981052396;                 Volume = 0.00011419627442955971; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0829, -0.3978, -0.8521,  0.0547,  0.5369,  0.4194, -0.2142,  0.3535,\n",
      "        -0.3536, -0.1458]) tensor([ 0.0823, -0.7748, -0.9509,  0.1937,  0.4452,  0.3415, -0.2135,  0.7827,\n",
      "        -0.6173, -0.1675]) tensor([ 0.1014, -0.4178, -0.8296,  0.0270,  0.5381,  0.4075, -0.2066,  0.3314,\n",
      "        -0.3412, -0.1694])\n",
      "R[0]\n",
      "tensor([0.0206], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.04705111975222826; R = 0.0007468738209427102;                 Gamma = 0.43496842014789583; Q = 0.00011056461763564584;\n",
      "Entropy Neighbor = 0.8923780769109726;                 Entropy Random = 0.45828850388526915;                 Volume = 0.00016139786317944527; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0451, -0.3587, -0.9512,  0.0095,  0.8147,  0.5377, -0.2038,  0.6616,\n",
      "        -0.2527, -0.1559]) tensor([-0.0214, -0.7536, -1.0869,  0.1430,  0.7386,  0.4588, -0.2122,  1.1096,\n",
      "        -0.5366, -0.1889]) tensor([-0.0451, -0.3587, -0.9512,  0.0095,  0.8147,  0.5377, -0.2038,  0.6616,\n",
      "        -0.2527, -0.1559])\n",
      "R[0]\n",
      "tensor([-0.0166], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.047825131952762605; R = 0.0013635529709572437;                 Gamma = 0.4349770477414131; Q = 0.0001802454197750194;\n",
      "Entropy Neighbor = 0.8891787949800491;                 Entropy Random = 0.46925223088264467;                 Volume = 0.005064270690083503; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1189, -0.3166, -1.0220,  0.0270,  0.8524,  0.5002, -0.1515,  0.6896,\n",
      "        -0.2730, -0.1165]) tensor([-0.1125, -0.7084, -1.1541,  0.1691,  0.7772,  0.4547, -0.1477,  1.1237,\n",
      "        -0.5705, -0.1541]) tensor([-0.1121, -0.3204, -1.0160,  0.0255,  0.8540,  0.5117, -0.1599,  0.6967,\n",
      "        -0.2673, -0.1210])\n",
      "R[0]\n",
      "tensor([-0.0180], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.04805188730359077; R = 0.001338005953002721;                 Gamma = 0.4311671987175941; Q = 7.665310212178156e-05;\n",
      "Entropy Neighbor = 0.89808804500103;                 Entropy Random = 0.4809666141867638;                 Volume = 0.05370508002489805; VAE = 0.0\n",
      "Average (on the epoch) training loss: 0.0008041489759153592\n",
      "Episode average V value: 0.051040578685700895\n",
      "epoch 1:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:325: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:371: RuntimeWarning: invalid value encountered in divide\n",
      "  dist_matrix = dist_matrix/np.nanpercentile(dist_matrix.flatten(), 99)\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:408: RuntimeWarning: All-NaN axis encountered\n",
      "  ylim_max = np.nanmax(self._separability_tracking)*1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1381, -0.3051, -1.0395,  0.0224,  0.8875,  0.5021, -0.1429,  0.7360,\n",
      "        -0.2644, -0.1069]) tensor([-0.1310, -0.6973, -1.1726,  0.1652,  0.8131,  0.4566, -0.1390,  1.1708,\n",
      "        -0.5626, -0.1449]) tensor([-0.1397, -0.2986, -1.0385,  0.0279,  0.8876,  0.5054, -0.1439,  0.7431,\n",
      "        -0.2632, -0.1018])\n",
      "R[0]\n",
      "tensor([-0.0206], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.048167141780257224; R = 0.0011424852175987326;                 Gamma = 0.4307994582653046; Q = 0.0001769192118126739;\n",
      "Entropy Neighbor = 0.8925018134117126;                 Entropy Random = 0.4923352209329605;                 Volume = 0.11911391105502844; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1945, -0.2653, -1.0869,  0.0272,  0.9260,  0.4553, -0.0987,  0.7684,\n",
      "        -0.2784, -0.0702]) tensor([-0.1864, -0.6571, -1.2199,  0.1694,  0.8515,  0.4082, -0.0952,  1.2032,\n",
      "        -0.5760, -0.1087]) tensor([-0.1930, -0.2585, -1.0834,  0.0337,  0.9248,  0.4625, -0.1032,  0.7794,\n",
      "        -0.2760, -0.0649])\n",
      "R[0]\n",
      "tensor([-0.0240], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"params\")\n",
    "except Exception:\n",
    "    pass\n",
    "dump(vars(parameters), \"params/\" + fname + \".jldump\")\n",
    "#agent.gathering_data=False\n",
    "#agent.setNetwork('test', nEpoch=7)\n",
    "agent.run(parameters.epochs, parameters.steps_per_epoch)\n",
    "\n",
    "# --- Show results ---\n",
    "basename = \"scores/\" + fname\n",
    "scores = load(basename + \"_scores.jldump\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._learning_algo._nstep_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.setNetwork(fname, nEpoch=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._in_episode = True\n",
    "agent._mode = 0 # Testing mode with plan_depth=0\n",
    "initState = env.reset(agent._mode)\n",
    "inputDims = env.inputDimensions()\n",
    "\n",
    "for i in range(len(inputDims)):\n",
    "    if inputDims[i][0] > 1:\n",
    "        agent._state[i][1:] = initState[i][1:]\n",
    "agent._Vs_on_last_episode = []\n",
    "is_terminal = False\n",
    "reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for i in range(100):\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    plt.figure()\n",
    "    plt.imshow(np.flip(_obs.squeeze()))\n",
    "    plt.show()\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "    V, action, reward, _ = agent._step()\n",
    "    print(action)\n",
    "    agent._Vs_on_last_episode.append(V)\n",
    "    is_terminal = env.inTerminalState()\n",
    "    if is_terminal: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "obs = env.observe()\n",
    "_obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "_obs = np.flip(_obs.squeeze())\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "im = ax.imshow(np.zeros(_obs.shape))\n",
    "\n",
    "def init():\n",
    "    plt.cla()\n",
    "    im = ax.imshow(_obs)\n",
    "    return [im]\n",
    "\n",
    "def animate(i, *args, **kwargs):\n",
    "    plt.cla()\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    _obs = np.flip(_obs.squeeze())\n",
    "    im = ax.imshow(_obs)\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "        V, action, reward, _ = agent._step()\n",
    "        agent._Vs_on_last_episode.append(V)\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init, \n",
    "     frames=100, blit=False, repeat=True)\n",
    "ani.save(f'figs/{fname}/behavior.gif', writer=\"ffmpeg\", fps = 15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
