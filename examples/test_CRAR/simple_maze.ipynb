{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "import yaml\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import hash, dump, load\n",
    "import os\n",
    "\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.CRAR_torch import CRAR\n",
    "from simple_maze_env import MyEnv as simple_maze_env\n",
    "import deer.experiment.base_controllers as bc\n",
    "\n",
    "from deer.policies import EpsilonGreedyPolicy, FixedFigure8Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_yaml = 'network.yaml'\n",
    "internal_dim = 5\n",
    "fname = 'foraging'\n",
    "set_network = None #['expanded_tcm', 15, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'nn_yaml': nn_yaml,\n",
    "    'higher_dim_obs': True,\n",
    "    'internal_dim': internal_dim,\n",
    "    'fname': fname,\n",
    "    'steps_per_epoch': 2500,\n",
    "    'epochs': 20,\n",
    "    'steps_per_test': 1000,\n",
    "    'period_btw_summary_perfs': 1,\n",
    "    'encoder_type': 'regular',\n",
    "    'frame_skip': 2,\n",
    "    'learning_rate': 1*1E-4,\n",
    "    'learning_rate_decay': 1.0,\n",
    "    'discount': 0.9,\n",
    "    'epsilon_start': 1.0,\n",
    "    'epsilon_min': 1.0,\n",
    "    'epsilon_decay': 1000,\n",
    "    'update_frequency': 1,\n",
    "    'replay_memory_size': 100000, #50000\n",
    "    'batch_size': 64,\n",
    "    'freeze_interval': 1000,\n",
    "    'deterministic': False,\n",
    "    'loss_weights': [0, 0, 0, 0, 0, 0, 1., 0.],\n",
    "    'foraging_give_rewards': True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState()\n",
    "env = simple_maze_env(\n",
    "    rng, reward=parameters['foraging_give_rewards'],\n",
    "    higher_dim_obs=parameters['higher_dim_obs'], plotfig=False\n",
    "    )\n",
    "\n",
    "# Algorithm\n",
    "learning_algo = CRAR(\n",
    "    env, parameters['freeze_interval'], parameters['batch_size'], rng,\n",
    "    high_int_dim=False, internal_dim=parameters['internal_dim'],\n",
    "    lr=parameters['learning_rate'], nn_yaml=parameters['nn_yaml'],\n",
    "    double_Q=True, loss_weights=parameters['loss_weights'],\n",
    "    encoder_type=parameters['encoder_type']\n",
    "    )\n",
    "\n",
    "# Policies\n",
    "train_policy = EpsilonGreedyPolicy(learning_algo, env.nActions(), rng, 0.2)\n",
    "test_policy = EpsilonGreedyPolicy(learning_algo, env.nActions(), rng, 0.)\n",
    "\n",
    "# Initialize Agent\n",
    "agent = NeuralAgent(\n",
    "    env, learning_algo, parameters['replay_memory_size'], 1,\n",
    "    parameters['batch_size'], rng,\n",
    "    train_policy=train_policy, test_policy=test_policy)\n",
    "if set_network is not None:\n",
    "    agent.setNetwork(\n",
    "        f'{set_network[0]}/fname', nEpoch=set_network[1],\n",
    "        encoder_only=set_network[2]\n",
    "        )\n",
    "agent.run(10, 500)\n",
    "\n",
    "# Attach controllers\n",
    "agent.attach(bc.VerboseController( evaluate_on='epoch', periodicity=1))\n",
    "agent.attach(bc.LearningRateController(\n",
    "    initial_learning_rate=parameters['learning_rate'],\n",
    "    learning_rate_decay=parameters['learning_rate_decay'],\n",
    "    periodicity=1))\n",
    "agent.attach(bc.TrainerController(\n",
    "    evaluate_on='action', periodicity=parameters['update_frequency'],\n",
    "    show_episode_avg_V_value=True, show_avg_Bellman_residual=True))\n",
    "best_controller = bc.FindBestController(\n",
    "    validationID=simple_maze_env.VALIDATION_MODE,\n",
    "    testID=None, unique_fname=fname, savefrequency=5)\n",
    "agent.attach(best_controller)\n",
    "agent.attach(bc.InterleavedTestEpochController(\n",
    "    id=simple_maze_env.VALIDATION_MODE, epoch_length=parameters['steps_per_test'],\n",
    "    periodicity=1, show_score=True, summarize_every=1, unique_fname=fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 False\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2102, -0.2841, -0.1561,  0.1779,  0.2260]) tensor([-0.0934,  0.0597, -0.3705,  0.4881,  0.2464]) tensor([-0.2102, -0.2841, -0.1561,  0.1779,  0.2260])\n",
      "R[0]\n",
      "tensor([-0.1695], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05211191838979721; R = 0.035388851385563615;                 Gamma = 1.3022133326530456; Q = 0.011360488799433369;\n",
      "Entropy Neighbor = 0.9999132175445556;                 Entropy Random = 0.999628319144249;                 Volume = 0.08457182178646326; VAE = 0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 False\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.8264, -0.7891,  0.6557,  0.4875,  1.0634]) tensor([-0.7052, -0.4414,  0.4548,  0.7182,  1.0432]) tensor([-0.8264, -0.7891,  0.6557,  0.4875,  1.0634])\n",
      "R[0]\n",
      "tensor([-0.2082], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if set_network is not None:\n",
    "    agent.setNetwork(\n",
    "        f'{set_network[0]}/fname', nEpoch=set_network[1],\n",
    "        encoder_only=set_network[2]\n",
    "        )\n",
    "agent.run(parameters['epochs'], parameters['steps_per_epoch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.setNetwork(f'{fname}/fname', nEpoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._in_episode = True\n",
    "agent._mode = 0 # Testing mode with plan_depth=0\n",
    "initState = env.reset(agent._mode)\n",
    "inputDims = env.inputDimensions()\n",
    "\n",
    "for i in range(len(inputDims)):\n",
    "    if inputDims[i][0] > 1:\n",
    "        agent._state[i][1:] = initState[i][1:]\n",
    "agent._Vs_on_last_episode = []\n",
    "is_terminal = False\n",
    "reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for i in range(100):\n",
    "    obs = env.observe()\n",
    "    width = height = int(np.sqrt(obs[0].size))\n",
    "    _obs = obs[0].reshape((width, height))\n",
    "    plt.figure()\n",
    "    plt.imshow(np.flip(_obs.squeeze()))\n",
    "    plt.show()\n",
    "    if is_terminal: break\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "    V, action, reward, _ = agent._step()\n",
    "    print(action)\n",
    "    agent._Vs_on_last_episode.append(V)\n",
    "    is_terminal = env.inTerminalState()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "obs = env.observe()\n",
    "_obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "_obs = np.flip(_obs.squeeze())\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "im = ax.imshow(np.zeros(_obs.shape))\n",
    "\n",
    "def init():\n",
    "    plt.cla()\n",
    "    im = ax.imshow(_obs)\n",
    "    return [im]\n",
    "\n",
    "def animate(i, *args, **kwargs):\n",
    "    plt.cla()\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    _obs = np.flip(_obs.squeeze())\n",
    "    im = ax.imshow(_obs)\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "        V, action, reward, _ = agent._step()\n",
    "        agent._Vs_on_last_episode.append(V)\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init, \n",
    "     frames=100, blit=False, repeat=True)\n",
    "ani.save(f'figs/{fname}/behavior.gif', writer=\"ffmpeg\", fps = 15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
