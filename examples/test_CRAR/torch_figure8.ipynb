{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import hash, dump, load\n",
    "import os\n",
    "\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.CRAR_torch import CRAR\n",
    "from figure8_env import MyEnv as figure8_env\n",
    "import deer.experiment.base_controllers as bc\n",
    "\n",
    "from deer.policies import EpsilonGreedyPolicy, FixedFigure8Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure8_give_rewards = True\n",
    "nn_yaml = 'network_noconv.yaml'\n",
    "higher_dim_obs = False\n",
    "internal_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    steps_per_epoch = 5000\n",
    "    epochs = 50\n",
    "    steps_per_test = 1000\n",
    "    period_btw_summary_perfs = 1\n",
    "    \n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    frame_skip = 2\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    update_rule = 'rmsprop'\n",
    "    learning_rate = 5 * 1E-4 # 1E-4\n",
    "    learning_rate_decay = 0.9\n",
    "    discount = 0.9\n",
    "    discount_inc = 1\n",
    "    discount_max = 0.99\n",
    "    rms_decay = 0.9\n",
    "    rms_epsilon = 0.0001\n",
    "    momentum = 0\n",
    "    clip_norm = 1.0\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 1.0\n",
    "    epsilon_decay = 10000\n",
    "    update_frequency = 1\n",
    "    replay_memory_size = 1000000 #replacing with 200000 will works just fine (in case you dont have 18gb of memory)\n",
    "    batch_size = 32\n",
    "    freeze_interval = 1000\n",
    "    deterministic = False\n",
    "    \n",
    "    # ----------------------\n",
    "    # Learning algo parameters\n",
    "    # ----------------------\n",
    "    loss_weights = [1E-2, 1E-3, 1E-3, 1E-3, 1E-3, 1E-3, 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters hash is: 62977be8e45d8a56a5537c11dfd5d2fd8dda69e0\n",
      "The parameters are: <__main__.Defaults object at 0x14e0557b0>\n",
      "end gathering data\n"
     ]
    }
   ],
   "source": [
    "parameters = Defaults()\n",
    "if parameters.deterministic:\n",
    "    rng = np.random.RandomState(123456)\n",
    "else:\n",
    "    rng = np.random.RandomState()\n",
    "\n",
    "# --- Instantiate environment ---\n",
    "env = figure8_env(\n",
    "    give_rewards=figure8_give_rewards,\n",
    "    intern_dim=internal_dim,\n",
    "    higher_dim_obs=higher_dim_obs,\n",
    "    )\n",
    "\n",
    "# --- Instantiate learning_algo ---\n",
    "learning_algo = CRAR(\n",
    "    env,\n",
    "    parameters.rms_decay,\n",
    "    parameters.rms_epsilon,\n",
    "    parameters.momentum,\n",
    "    parameters.clip_norm,\n",
    "    parameters.freeze_interval,\n",
    "    parameters.batch_size,\n",
    "    parameters.update_rule,\n",
    "    rng,\n",
    "    high_int_dim=False,\n",
    "    internal_dim=internal_dim, lr=parameters.learning_rate,\n",
    "    nn_yaml=nn_yaml, double_Q=True,\n",
    "    loss_weights=parameters.loss_weights,\n",
    "    nstep = 15\n",
    "    )\n",
    "\n",
    "if figure8_give_rewards:\n",
    "    train_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.2,\n",
    "        consider_valid_transitions=False\n",
    "        )\n",
    "    test_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.\n",
    "        )\n",
    "else:\n",
    "    train_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng, epsilon=0.2,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "    test_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "\n",
    "# --- Instantiate agent ---\n",
    "agent = NeuralAgent(\n",
    "    env,\n",
    "    learning_algo,\n",
    "    parameters.replay_memory_size,\n",
    "    1,\n",
    "    parameters.batch_size,\n",
    "    rng,\n",
    "    train_policy=train_policy,\n",
    "    test_policy=test_policy)\n",
    "\n",
    "# --- Create unique filename for FindBestController ---\n",
    "h = hash(vars(parameters), hash_name=\"sha1\")\n",
    "fname = \"test_\" + h\n",
    "print(\"The parameters hash is: {}\".format(h))\n",
    "print(\"The parameters are: {}\".format(parameters))\n",
    "\n",
    "# As for the discount factor and the learning rate, one can update periodically the parameter of the epsilon-greedy\n",
    "# policy implemented by the agent. This controllers has a bit more capabilities, as it allows one to choose more\n",
    "# precisely when to update epsilon: after every X action, episode or epoch. This parameter can also be reset every\n",
    "# episode or epoch (or never, hence the resetEvery='none').\n",
    "agent.attach(bc.EpsilonController(\n",
    "    initial_e=parameters.epsilon_start,\n",
    "    e_decays=parameters.epsilon_decay,\n",
    "    e_min=parameters.epsilon_min,\n",
    "    evaluate_on='action',\n",
    "    periodicity=1,\n",
    "    reset_every='none'))\n",
    "\n",
    "agent.run(10, 500)\n",
    "print(\"end gathering data\")\n",
    "\n",
    "# --- Bind controllers to the agent ---\n",
    "# Before every training epoch (periodicity=1), we want to print a summary of the agent's epsilon, discount and \n",
    "# learning rate as well as the training epoch number.\n",
    "agent.attach(bc.VerboseController(\n",
    "    evaluate_on='epoch', \n",
    "    periodicity=1))\n",
    "\n",
    "# Every epoch end, one has the possibility to modify the learning rate using a LearningRateController. Here we \n",
    "# wish to update the learning rate after every training epoch (periodicity=1), according to the parameters given.\n",
    "agent.attach(bc.LearningRateController(\n",
    "    initial_learning_rate=parameters.learning_rate, \n",
    "    learning_rate_decay=parameters.learning_rate_decay,\n",
    "    periodicity=1))\n",
    "\n",
    "# Same for the discount factor.\n",
    "agent.attach(bc.DiscountFactorController(\n",
    "    initial_discount_factor=parameters.discount, \n",
    "    discount_factor_growth=parameters.discount_inc, \n",
    "    discount_factor_max=parameters.discount_max,\n",
    "    periodicity=1))\n",
    "\n",
    "# During training epochs, we want to train the agent after every [parameters.update_frequency] action it takes.\n",
    "# Plus, we also want to display after each training episode (!= than after every training) the average bellman\n",
    "# residual and the average of the V values obtained during the last episode, hence the two last arguments.\n",
    "agent.attach(bc.TrainerController(\n",
    "    evaluate_on='action', \n",
    "    periodicity=parameters.update_frequency, \n",
    "    show_episode_avg_V_value=True, \n",
    "    show_avg_Bellman_residual=True))\n",
    "\n",
    "# We wish to discover, among all versions of our neural network (i.e., after every training epoch), which one \n",
    "# has the highest validation score.\n",
    "# To achieve this goal, one can use the FindBestController along with an InterleavedTestEpochControllers. It is \n",
    "# important that the validationID is the same than the id argument of the InterleavedTestEpochController.\n",
    "# The FindBestController will dump on disk the validation scores for each and every network, as well as the \n",
    "# structure of the neural network having the best validation score. These dumps can then used to plot the evolution \n",
    "# of the validation and test scores (see below) or simply recover the resulting neural network for your \n",
    "# application.\n",
    "agent.attach(bc.FindBestController(\n",
    "    validationID=figure8_env.VALIDATION_MODE,\n",
    "    testID=None,\n",
    "    unique_fname=fname))\n",
    "\n",
    "# All previous controllers control the agent during the epochs it goes through. However, we want to interleave a \n",
    "# \"validation epoch\" between each training epoch. For each validation epoch, we want also to display the sum of all \n",
    "# rewards obtained, hence the showScore=True. Finally, we want to call the summarizePerformance method of ALE_env \n",
    "# every [parameters.period_btw_summary_perfs] *validation* epochs.\n",
    "agent.attach(bc.InterleavedTestEpochController(\n",
    "    id=figure8_env.VALIDATION_MODE, \n",
    "    epoch_length=parameters.steps_per_test,\n",
    "    periodicity=1,\n",
    "    show_score=True,\n",
    "    summarize_every=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0850, -0.0433, -0.0346]) tensor([-0.1029, -0.3569,  0.2552]) tensor([-0.1108, -0.0419,  0.0006])\n",
      "R[0]\n",
      "tensor([-0.0350], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.007117753988946788 0.007440529691124539 0.10726601260673488 0.00945800293632783 0.6490954543948173 0.00037953240424394607 0.11348815647512675\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7134,  0.3747, -0.3202]) tensor([-0.7016,  0.3658, -0.3058]) tensor([-0.7248,  0.3372, -0.3104])\n",
      "R[0]\n",
      "tensor([-0.0078], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.007579714646330104 0.007061438488060958 0.021045680101262405 0.00872805702209007 0.5833257907032967 0.0043922697901725765 0.08702450754866004\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0267,  0.4814, -0.6216]) tensor([ 0.0291,  0.4785, -0.6010]) tensor([-0.0840,  0.3820, -0.6103])\n",
      "R[0]\n",
      "tensor([-0.0022], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.008423928915988654 0.006518836526782252 0.021098812693322543 0.0063159086442319675 0.6241799794435501 0.008196514576673508 0.16140531445294617\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6171, -0.0220, -1.2857]) tensor([-0.6043,  0.0426, -1.1768]) tensor([-0.5672, -0.0134, -1.3170])\n",
      "R[0]\n",
      "tensor([0.0937], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.009053274339064955 0.005384168216682156 0.017885028525488452 0.004609933868341614 0.6646886204481125 0.016390787079930306 0.20071241506934165\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.9255,  0.3459,  0.1743]) tensor([-0.9206,  0.3684,  0.1652]) tensor([-0.9125,  0.3238,  0.2098])\n",
      "R[0]\n",
      "tensor([0.0005], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.012490410330006852 0.0057283049641519024 0.019138484541152138 0.00759996827528812 0.6668189598917961 0.02356163564324379 0.25127305689454077\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 1.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1999, 0.9999, 0.3458]) tensor([0.1687, 0.9589, 0.2749]) tensor([0.1412, 0.9603, 0.3365])\n",
      "R[0]\n",
      "tensor([-0.0212], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01325613491795957 0.005144130565036903 0.020010931744938718 0.0051120828548446295 0.6469243001937867 0.02348519878834486 0.23357374557852745\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6635,  0.5775,  0.1513]) tensor([-0.6527,  0.5731,  0.1341]) tensor([-0.8868,  0.3623,  0.2380])\n",
      "R[0]\n",
      "tensor([0.0076], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.014799515322316438 0.006076460245654744 0.017888462631497532 0.008789006766630336 0.6447828162908554 0.02436897699534893 0.22843922825157642\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1213,  0.9270, -0.3078]) tensor([-0.1198,  0.9405, -0.2653]) tensor([-0.3992,  0.2311, -1.2249])\n",
      "R[0]\n",
      "tensor([0.0561], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.014096905319252983 0.004701529339228728 0.019652067781542427 0.006314976702386048 0.6627082904577255 0.020003325566649438 0.242525627553463\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5569,  0.6527,  0.2072]) tensor([-0.5452,  0.6392,  0.2273]) tensor([-0.6347,  0.5969,  0.2387])\n",
      "R[0]\n",
      "tensor([0.0087], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01483128244522959 0.004599982689916942 0.019965703161899 0.009197535320243333 0.6392459975481033 0.01845081489533186 0.22032345832884312\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2972, 0.9046, 0.1641]) tensor([0.2534, 0.8691, 0.1274]) tensor([-0.0227,  0.9383,  0.0327])\n",
      "R[0]\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.015281891711987555 0.005381067985772461 0.01955555308877956 0.006250822270696517 0.6325648121833801 0.02123601942509413 0.21919191317260264\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Average (on the epoch) training loss: 0.007237629466108047\n",
      "Episode average V value: -1\n",
      "epoch 1:\n",
      "Learning rate: 0.0005\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/Users/chingfang/Code/deer/deer/learning_algos/CRAR_torch.py:416: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  state = torch.as_tensor(state, device=self.device).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.7638,  1.1290, -0.2375]) tensor([ 0.6898,  1.0866, -0.1994]) tensor([ 0.9722,  1.0581, -0.1865])\n",
      "R[0]\n",
      "tensor([0.0058], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01739938668720424 0.005561300603087148 0.019531940314103848 0.006424581901228521 0.595351458132267 0.020160938546061515 0.2058855909258127\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2212,  0.9889, -0.2148]) tensor([ 0.1865,  0.9568, -0.1690]) tensor([ 0.2832,  0.9976, -0.3081])\n",
      "R[0]\n",
      "tensor([0.0258], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01782967510772869 0.00456771940962426 0.020792935890262015 0.004371014170523267 0.5839249566793442 0.022313695676624775 0.19270586398243905\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.9453, -0.3546,  0.0168]) tensor([ 0.7865, -0.3114,  0.0928]) tensor([ 0.6027, -0.8240, -0.1314])\n",
      "R[0]\n",
      "tensor([-0.0204], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01794121235469356 0.00511865792205208 0.019189437679247932 0.00732832919381326 0.579973523914814 0.0212046270146966 0.1880810538753867\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.8602,  0.3821,  0.4409]) tensor([-0.8406,  0.3981,  0.4245]) tensor([-0.8291,  0.4042,  0.4298])\n",
      "R[0]\n",
      "tensor([-0.0012], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018583689758554102 0.0056978371494478776 0.019121832632808946 0.005295032755704597 0.5741109556555748 0.0221966362670064 0.19145758044719696\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5401,  0.4844,  0.4385]) tensor([-0.5241,  0.4938,  0.3989]) tensor([-0.5844,  0.5147,  0.4184])\n",
      "R[0]\n",
      "tensor([-0.0048], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01915411833487451 0.005102337389032982 0.018285414189915174 0.00834483514551539 0.5636666032075882 0.019366308890283106 0.18209030051529407\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5663,  1.0220, -0.0237]) tensor([ 0.5069,  0.9741, -0.0106]) tensor([ 0.6813,  1.0685, -0.1365])\n",
      "R[0]\n",
      "tensor([0.0250], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01919612620398402 0.00593392397048774 0.018137552154134026 0.0064749486190266905 0.5654756294488907 0.019106996111571788 0.18934329955279827\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6078,  0.5649,  0.2428]) tensor([-0.5638,  0.5502,  0.2280]) tensor([-0.5478,  0.6210,  0.2116])\n",
      "R[0]\n",
      "tensor([0.0053], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.019019155633170158 0.005673015335465607 0.01924173966815579 0.007064365549129434 0.5616382926106452 0.016310949318110942 0.17882672809064387\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4223,  0.7420,  0.1734]) tensor([-0.4155,  0.7477,  0.1657]) tensor([-0.2283,  0.8510,  0.1176])\n",
      "R[0]\n",
      "tensor([-0.0047], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.019191760124173014 0.005869075283295388 0.019309514445485548 0.00496404493888258 0.5603471060395241 0.014001016184687614 0.18093505992740394\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2691,  0.8285,  0.1178]) tensor([-0.2926,  0.7859,  0.1261]) tensor([-0.0806,  0.9786,  0.0782])\n",
      "R[0]\n",
      "tensor([-0.0030], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.019198511058464648 0.00634379452750727 0.019837425701553002 0.007657290310249664 0.5473192501664161 0.009973834075033664 0.17267840576171875\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2011, 0.9409, 0.1868]) tensor([0.1693, 0.9075, 0.1567]) tensor([-0.3687,  0.7307,  0.2014])\n",
      "R[0]\n",
      "tensor([0.0141], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.02003398894146085 0.0061098310527595455 0.019267364300088956 0.006242162058246322 0.5477783178687096 0.01223321732878685 0.17171632243692875\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Average (on the epoch) training loss: 0.0064166604642319726\n",
      "Episode average V value: -1\n",
      "epoch 2:\n",
      "Learning rate: 0.00045000000000000004\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4494,  0.7054,  0.1326]) tensor([-0.4371,  0.7032,  0.1181]) tensor([-0.5902,  0.5930,  0.1658])\n",
      "R[0]\n",
      "tensor([0.0044], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.019249938691034914 0.006283692405984766 0.01887946564896265 0.006427210053312592 0.5483962619900703 0.009490227691829205 0.16429458524286747\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1050,  0.9059,  0.0526]) tensor([-0.1195,  0.8778,  0.0594]) tensor([-0.0789,  0.9318,  0.0219])\n",
      "R[0]\n",
      "tensor([0.0043], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01860289523564279 0.005446072159124014 0.018185611620428973 0.005064996570756194 0.5503161776065827 0.007982163116335868 0.17061276241391898\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0132,  1.0151,  0.0266]) tensor([-0.0515,  0.9727,  0.0390]) tensor([-0.0866,  0.9809,  0.0297])\n",
      "R[0]\n",
      "tensor([0.0055], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01824452265910804 0.005482301317155361 0.016829717090469785 0.006146144730271772 0.5471317631602287 0.0066461370512843135 0.16895207373797894\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4671,  0.7356,  0.1177]) tensor([-0.4437,  0.7492,  0.1217]) tensor([-0.2423,  0.9031,  0.0385])\n",
      "R[0]\n",
      "tensor([-0.0114], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018314801864326002 0.005166873633635987 0.01660521436133422 0.004282862950873096 0.5337199168205261 0.007334262102842331 0.1716641067713499\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2789,  0.8479,  0.0512]) tensor([-0.2885,  0.8218,  0.0539]) tensor([-6.2307e-02,  9.7514e-01, -9.2667e-04])\n",
      "R[0]\n",
      "tensor([-0.0225], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.019348503950051964 0.005630748018447775 0.015635665121022612 0.004361782378226053 0.5320506773591042 0.00852198264747858 0.1681518983244896\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5665,  0.6186,  0.1608]) tensor([-0.5665,  0.6320,  0.1576]) tensor([-0.3947,  0.7392,  0.0991])\n",
      "R[0]\n",
      "tensor([-0.0018], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018491198047064245 0.005949454602858168 0.015119304309017026 0.003944205928593874 0.5387013437747955 0.008514368906617165 0.16561243364214898\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3044,  0.7982,  0.0543]) tensor([-0.3305,  0.7988,  0.0557]) tensor([-0.4094,  0.7215,  0.0942])\n",
      "R[0]\n",
      "tensor([0.0198], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018308514155447483 0.005326766658188717 0.016597183697333092 0.0034297970586339944 0.535238293170929 0.006630942337214947 0.16876232002675534\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.8179,  0.3543,  0.2851]) tensor([-0.7681,  0.3770,  0.2717]) tensor([-0.4989,  0.5296,  0.1929])\n",
      "R[0]\n",
      "tensor([6.9693e-05], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018248353937640786 0.00513346082586213 0.015510479068092536 0.002751824464998208 0.5311699794530869 0.006791009463369847 0.17271189804375173\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2742,  0.8635,  0.0168]) tensor([-0.2905,  0.8598,  0.0247]) tensor([-0.2464,  0.8820,  0.0147])\n",
      "R[0]\n",
      "tensor([0.0059], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018596027655526995 0.005408439375140006 0.015399774556455668 0.002974446117761545 0.5285011245012283 0.005426341094076633 0.16428249484300614\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2760,  0.8190, -0.0147]) tensor([-0.2787,  0.8165, -0.0098]) tensor([-3.2826e-01,  7.8831e-01, -4.2017e-04])\n",
      "R[0]\n",
      "tensor([0.0005], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01775683949328959 0.004275657486141426 0.013806572662724647 0.002706529136688914 0.5274491360187531 0.005932089701294899 0.16732432222366334\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Average (on the epoch) training loss: 0.004208979939011624\n",
      "Episode average V value: -1\n",
      "epoch 3:\n",
      "Learning rate: 0.00040500000000000003\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5659,  0.5938,  0.1122]) tensor([-0.5273,  0.6012,  0.1171]) tensor([-0.3236,  0.7636,  0.0356])\n",
      "R[0]\n",
      "tensor([0.0131], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018489127626642586 0.004246006364017375 0.01274256799655268 0.00260176305368077 0.5185167140960694 0.00738369632512331 0.16420468361675739\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5044,  0.6661,  0.0672]) tensor([-0.4812,  0.6792,  0.0653]) tensor([-0.4856,  0.6798,  0.0608])\n",
      "R[0]\n",
      "tensor([0.0005], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018196872672997414 0.004511373040862963 0.01167245564298355 0.0023197310122195633 0.5208888802528381 0.0056398725360631945 0.16368457783013582\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.8386,  0.4103,  0.1518]) tensor([-0.7909,  0.4401,  0.1320]) tensor([-0.6423,  0.5748,  0.0811])\n",
      "R[0]\n",
      "tensor([0.0141], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01755831702798605 0.0044598671149869915 0.011043841722479556 0.0030343577787280084 0.5269467841982841 0.005527821153402329 0.16747937052696943\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2858,  0.7347, -0.0635]) tensor([-0.2910,  0.7300, -0.0419]) tensor([-0.4969,  0.5787,  0.0835])\n",
      "R[0]\n",
      "tensor([-0.0018], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.017367982948198916 0.003796329488046467 0.011139222561032511 0.00252668888727203 0.5274366437196731 0.0067630882039666175 0.16298920860141516\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4860,  0.7551, -0.4213]) tensor([ 0.4643,  0.6387, -0.3403]) tensor([ 0.3830,  0.6270, -0.3309])\n",
      "R[0]\n",
      "tensor([-0.0182], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01827521359315142 0.0038083634126232935 0.011178313836746383 0.0025616356351820285 0.5155356525182724 0.006669031709432602 0.16071042358875276\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4089,  0.6414,  0.0425]) tensor([-0.4040,  0.6460,  0.0401]) tensor([-0.1668,  0.7636, -0.0467])\n",
      "R[0]\n",
      "tensor([-0.0046], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01777832657843828 0.0038067521324555856 0.00992900952952914 0.001966422840021551 0.5196657547950745 0.005792720273137092 0.15827012280374766\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3806,  0.3695, -0.5602]) tensor([-0.3582,  0.3484, -0.4041]) tensor([-0.3071,  0.3578, -0.6197])\n",
      "R[0]\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018597124109044672 0.00426257815718418 0.009684993439965183 0.002349996309436392 0.5112560490369796 0.006406168676912785 0.1563382937759161\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.8143,  0.4728,  0.1197]) tensor([-0.7860,  0.4946,  0.1208]) tensor([-0.6882,  0.5563,  0.0915])\n",
      "R[0]\n",
      "tensor([0.0052], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.017314062209799887 0.0037779422488965794 0.008678996628383175 0.0020671306092990564 0.5145468901991844 0.006351333804428577 0.15457431318610906\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3867, -0.8260, -0.0054]) tensor([-0.6874, -0.4774,  0.0096]) tensor([-0.3486, -0.5340,  0.0259])\n",
      "R[0]\n",
      "tensor([0.2574], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018205464063212277 0.004409033805131912 0.006548698505022912 0.00242892395920353 0.5150091999173164 0.007147473096847534 0.14963071227073668\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3842,  0.7214, -0.0079]) tensor([-0.3866,  0.7308, -0.0082]) tensor([-0.3704,  0.7308, -0.0123])\n",
      "R[0]\n",
      "tensor([0.0182], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01791973547404632 0.003641829221662192 0.00719651530594274 0.0018497075283667073 0.5129118745923043 0.006942205250263214 0.14875205501914024\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Average (on the epoch) training loss: 0.0023706357613409636\n",
      "Episode average V value: -1\n",
      "epoch 4:\n",
      "Learning rate: 0.0003645\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2044,  0.7950, -0.1165]) tensor([-0.2164,  0.7826, -0.1074]) tensor([-0.2464,  0.7267, -0.1675])\n",
      "R[0]\n",
      "tensor([0.0048], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.017765240208245815 0.004165308630050276 0.007393364145085798 0.0029441021087695846 0.5148387131094933 0.0073292629644274715 0.15180743450671436\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2891, -0.1276, -0.2286]) tensor([ 0.1950, -0.0853, -0.1638]) tensor([ 0.1649, -0.1004, -0.3335])\n",
      "R[0]\n",
      "tensor([0.0195], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018376810389570893 0.0037593721835510224 0.007701884595226147 0.0021360543552436865 0.5054000231027603 0.008713876865804195 0.14630578938871622\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0962,  0.9723, -0.1303]) tensor([ 0.0404,  0.9589, -0.1323]) tensor([ 0.1886,  1.0062, -0.1413])\n",
      "R[0]\n",
      "tensor([0.0047], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.017958381524309515 0.003988033686560812 0.0075421332227560926 0.0027640365409315566 0.5091226860284805 0.008201069839298724 0.14586518581956626\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0253,  0.7713, -0.1190]) tensor([ 0.0103,  0.7476, -0.1125]) tensor([ 0.0268,  0.8012, -0.1488])\n",
      "R[0]\n",
      "tensor([-0.0127], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01733513232599944 0.0034591542398920865 0.008784378958356683 0.0027089780310634525 0.5153770983815193 0.006984610967338085 0.14402887473255396\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7234,  0.4716,  0.1196]) tensor([-0.6859,  0.4751,  0.1169]) tensor([-0.7145,  0.4790,  0.1154])\n",
      "R[0]\n",
      "tensor([0.0004], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01777655010856688 0.0037497547498423957 0.007162363665003795 0.0023506643964210524 0.5091994994878769 0.0069556693211197855 0.1518339668661356\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1001,  0.8256, -0.1282]) tensor([ 0.0902,  0.7929, -0.1291]) tensor([ 0.1550,  0.7818, -0.1412])\n",
      "R[0]\n",
      "tensor([-0.0152], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01789178000204265 0.003942735854179773 0.006249407831230201 0.0020216624289751055 0.5050531662702561 0.005942176714539528 0.14804174242913723\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3593,  0.7466, -0.0138]) tensor([-0.3679,  0.7474, -0.0146]) tensor([-0.4589,  0.6876,  0.0104])\n",
      "R[0]\n",
      "tensor([0.0034], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.017961915365420283 0.004169537324887642 0.008133236341134761 0.002772792891890276 0.5029563693404198 0.0063933275267481805 0.14442652713507415\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1260,  0.8696, -0.0768]) tensor([-0.1478,  0.8782, -0.0907]) tensor([-0.4776,  0.6808,  0.0048])\n",
      "R[0]\n",
      "tensor([-0.0018], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.017249385614879428 0.0034792398510253408 0.007021832402533619 0.001967866979102837 0.5038798171281814 0.00587126400321722 0.14365517838299274\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4848,  0.0204, -0.0799]) tensor([ 0.4267,  0.0245, -0.0724]) tensor([ 0.3366, -0.2180,  0.0122])\n",
      "R[0]\n",
      "tensor([0.0340], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01797666556574404 0.003996267054411874 0.007637401295156451 0.002620049804565497 0.5010303208231925 0.005654670611023903 0.14146623948216439\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6876,  0.5279,  0.1033]) tensor([-0.6550,  0.5543,  0.0876]) tensor([-0.5025,  0.6391, -0.0029])\n",
      "R[0]\n",
      "tensor([-0.0083], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018478187519125642 0.0039796202038269255 0.005450265166196914 0.001985690678062383 0.4923293197751045 0.0053065577447414395 0.1403045670092106\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Average (on the epoch) training loss: 0.0024271898215025432\n",
      "Episode average V value: -1\n",
      "epoch 5:\n",
      "Learning rate: 0.00032805000000000003\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3250,  0.4154,  0.0135]) tensor([-0.3019,  0.3929,  0.0234]) tensor([-0.3772,  0.5663, -0.0418])\n",
      "R[0]\n",
      "tensor([0.0123], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018257450085133315 0.003288539120534551 0.0059237412903166845 0.0023316762372269295 0.4887336502671242 0.005776664271950721 0.1381504393443465\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0755,  0.8170, -0.2088]) tensor([-0.1193,  0.8176, -0.1757]) tensor([-0.1196,  0.8108, -0.1838])\n",
      "R[0]\n",
      "tensor([0.0100], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018874817443080248 0.003951787518948549 0.0056235291107441295 0.0021212882796535267 0.4853644369840622 0.005198199473321438 0.13590203158557415\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6251,  0.4092, -0.3857]) tensor([-0.7642,  0.5863, -0.2085]) tensor([-0.8620,  0.3824,  0.1590])\n",
      "R[0]\n",
      "tensor([-0.0052], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018402705235406755 0.003362994062132202 0.005437088788203255 0.0022276143119670452 0.4930123181939125 0.004597331538796425 0.13460403037816285\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1057,  0.8657, -0.1693]) tensor([-0.1545,  0.8645, -0.1461]) tensor([-0.2151,  0.8164, -0.1245])\n",
      "R[0]\n",
      "tensor([0.0175], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018830374613404276 0.0038398171701555838 0.004904040542245639 0.0021626527994521895 0.4894678144454956 0.004723019860684872 0.13634055003523826\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0232,  0.8787, -0.2206]) tensor([ 0.0132,  0.8637, -0.2248]) tensor([ 0.0253,  0.8796, -0.2222])\n",
      "R[0]\n",
      "tensor([-0.0006], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018749344161711635 0.0037616928937568446 0.005180688099157124 0.0022603070670738816 0.48982811295986173 0.0058125521689653395 0.12858601263165473\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.8449,  0.3770,  0.1507]) tensor([-0.7643,  0.4111,  0.1179]) tensor([-0.8086,  0.3914,  0.1411])\n",
      "R[0]\n",
      "tensor([-0.0038], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018173040837980808 0.004482166570800473 0.00509409275832877 0.0019104114561341702 0.49349110263586043 0.00556373992562294 0.13076006595045328\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6428,  0.5040,  0.0844]) tensor([-0.6058,  0.5163,  0.0751]) tensor([-0.8611,  0.3511,  0.1727])\n",
      "R[0]\n",
      "tensor([-0.0024], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018630730505101382 0.004013585916029115 0.005057256106134446 0.0023878158622537738 0.48743772530555723 0.007048824250698089 0.12980955186113716\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1854,  0.9595, -0.2545]) tensor([ 0.1399,  0.9094, -0.2345]) tensor([ 0.1912,  0.9638, -0.2487])\n",
      "R[0]\n",
      "tensor([0.0134], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018333014118485154 0.003924487572865474 0.004450152028304728 0.001986535681586247 0.4902634611129761 0.006377857603132725 0.13111926690489054\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2369,  0.5480,  0.0067]) tensor([-0.2381,  0.5441, -0.0129]) tensor([-0.6032,  0.3336,  0.1950])\n",
      "R[0]\n",
      "tensor([-0.0032], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018999311208724976 0.004583703208758379 0.004595962019258877 0.0025528953270986675 0.48845569163560865 0.006169436685740948 0.12611840180307626\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2048,  0.3754, -0.0273]) tensor([-0.1704,  0.3371, -0.0306]) tensor([-0.0912,  0.4411,  0.0404])\n",
      "R[0]\n",
      "tensor([-0.0142], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01770110610127449 0.0038327692349303106 0.004518012523221842 0.002181352578045335 0.4907602388262749 0.006688997447490692 0.12860961851105093\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Average (on the epoch) training loss: 0.0022122549600491767\n",
      "Episode average V value: -1\n",
      "epoch 6:\n",
      "Learning rate: 0.000295245\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3722,  0.6407, -0.0669]) tensor([-0.3810,  0.6347, -0.0612]) tensor([-0.5334,  0.5610,  0.0278])\n",
      "R[0]\n",
      "tensor([0.0092], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01845563293900341 0.004076283581434836 0.004158718231530656 0.0022707007976132446 0.4822708678245544 0.007211603999137879 0.13006748886406422\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5116,  0.8918, -0.4401]) tensor([ 0.4781,  0.8230, -0.4096]) tensor([ 0.7101,  0.9269, -0.3917])\n",
      "R[0]\n",
      "tensor([0.0092], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01795801392849535 0.0034919907958283146 0.005059062932770758 0.002143009149003774 0.49024839645624163 0.005755145512521267 0.12760727479308845\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.8198,  0.8322, -0.1994]) tensor([ 0.7227,  0.8315, -0.2336]) tensor([ 0.4802,  0.8914, -0.1862])\n",
      "R[0]\n",
      "tensor([0.0158], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01820047390833497 0.004363647341459 0.005977937198789732 0.0024635285398107954 0.48557562625408174 0.0052139292806386945 0.12720705274492503\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4505,  0.6233, -0.0122]) tensor([-0.4302,  0.6184, -0.0141]) tensor([-0.2149,  0.7927, -0.1184])\n",
      "R[0]\n",
      "tensor([-0.0043], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01836073547694832 0.00425917452615613 0.005730725434877968 0.002163984917977359 0.48756592977046964 0.00586210010945797 0.12364431678131223\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2019,  0.7684, -0.0518]) tensor([-0.2164,  0.7701, -0.0751]) tensor([-0.7661,  0.4058,  0.1235])\n",
      "R[0]\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.019123555636033417 0.004369986644058372 0.0052092680159694284 0.0027011458897031842 0.4851778923273087 0.004735572129487991 0.12538613674789667\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0975,  0.9354, -0.1753]) tensor([ 0.0567,  0.9457, -0.1813]) tensor([ 0.0078,  0.9109, -0.1905])\n",
      "R[0]\n",
      "tensor([-0.0042], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018931842640042305 0.005430456053683884 0.005100933121881098 0.002435343557270244 0.4861738241314888 0.005351676225662232 0.12292512553185224\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5164, -0.2731, -0.0639]) tensor([ 0.3751, -0.2623, -0.0723]) tensor([ 0.6873, -0.1465, -0.0784])\n",
      "R[0]\n",
      "tensor([0.1316], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.017708418861962855 0.004490663143198617 0.005208677517563046 0.0025384356059366836 0.4903034917116165 0.0038554022908210755 0.1261069132387638\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2618,  0.7476, -0.1219]) tensor([-0.2768,  0.7295, -0.1099]) tensor([-0.3780,  0.6682, -0.0492])\n",
      "R[0]\n",
      "tensor([-0.0028], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.017936733517795803 0.004495240437699976 0.004502961510777823 0.0018927275973255745 0.4881769278645515 0.004509249158203602 0.12463668029755354\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3880,  0.5002, -0.0661]) tensor([-0.3695,  0.5046, -0.0659]) tensor([-0.1593,  0.6712, -0.1906])\n",
      "R[0]\n",
      "tensor([0.0005], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.0179694041069597 0.004261561603620066 0.003587260837062786 0.0023821882813936097 0.48629522866010666 0.004009705230593681 0.120643568854779\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4291,  0.5432, -0.0141]) tensor([-0.4104,  0.5457, -0.0254]) tensor([-0.3681,  0.5561, -0.0316])\n",
      "R[0]\n",
      "tensor([0.0007], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01838092380668968 0.0051978150527029355 0.004202628842380363 0.0021272687515593133 0.48662489259243014 0.004153791114687919 0.12339687542617321\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Average (on the epoch) training loss: 0.0023118333087593784\n",
      "Episode average V value: -1\n",
      "epoch 7:\n",
      "Learning rate: 0.0002657205\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1551,  0.8491, -0.1333]) tensor([-0.1591,  0.8402, -0.1400]) tensor([-0.3686,  0.7047, -0.0595])\n",
      "R[0]\n",
      "tensor([0.0107], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01825310271792114 0.005837050693888159 0.004293531995121157 0.00265486764756497 0.48426961863040924 0.005435596458613873 0.11931818625330926\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1897,  0.8164, -0.1282]) tensor([-0.2194,  0.8198, -0.1095]) tensor([-0.2316,  0.7959, -0.1141])\n",
      "R[0]\n",
      "tensor([-0.0030], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018141790844500065 0.004792546069420495 0.0048222999448007614 0.002368841823015828 0.4846172785758972 0.005059942275285721 0.11988127915561199\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4657,  0.5625, -0.0028]) tensor([-0.4460,  0.5588, -0.0023]) tensor([ 0.7380,  1.0585, -0.2926])\n",
      "R[0]\n",
      "tensor([0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.0182995170513168 0.004228128642920637 0.004082541474923346 0.0023841629463713615 0.4835191487073898 0.004911032937467098 0.11974373181164265\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4070,  0.6476, -0.0305]) tensor([-0.4037,  0.6511, -0.0330]) tensor([-0.3774,  0.6659, -0.0426])\n",
      "R[0]\n",
      "tensor([-0.0016], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.017607398410327733 0.0043440869425649 0.0050884176947874945 0.0022046517626731657 0.4858244194984436 0.004675142623484134 0.1221299623027444\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3399,  0.7086, -0.0553]) tensor([-0.3317,  0.7106, -0.0606]) tensor([-0.6192,  0.5177,  0.0689])\n",
      "R[0]\n",
      "tensor([-0.0230], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.017552983306348323 0.004029912792477262 0.0045125412515772045 0.002699307646136731 0.48631329846382143 0.005043192066252231 0.12065732645243406\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6617,  0.4761,  0.0750]) tensor([-0.6187,  0.5066,  0.0564]) tensor([-0.4647,  0.5981, -0.0187])\n",
      "R[0]\n",
      "tensor([0.0027], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01800513501930982 0.004228649514525387 0.0037754746787104524 0.0020290799856884406 0.47902120542526244 0.005232077822089195 0.11867531540617347\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0116,  0.8889, -0.1468]) tensor([-0.0196,  0.8777, -0.1612]) tensor([-0.3651,  0.6766, -0.0569])\n",
      "R[0]\n",
      "tensor([0.0005], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018050555576570333 0.0043528844923093855 0.005068114119832899 0.0023916350312065335 0.4760023937821388 0.004798673048615455 0.11956182570382953\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5964, -0.4137, -0.4445]) tensor([-0.6317, -0.3616, -0.2746]) tensor([-0.7459, -0.4080, -0.3469])\n",
      "R[0]\n",
      "tensor([0.0185], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.017766698864288628 0.004896042765587481 0.00409751368072466 0.0022497930352692493 0.48592723488807676 0.0043062386512756345 0.12270944752544165\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0889, -0.4873,  0.2024]) tensor([-0.2660, -0.3166,  0.1690]) tensor([-0.5683, -0.7342,  0.8231])\n",
      "R[0]\n",
      "tensor([0.1526], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018312669933773576 0.005641823697744257 0.003473278752167971 0.0023953896899474784 0.4767853022813797 0.005317070260643959 0.11836110775917769\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7069,  0.4213,  0.0941]) tensor([-0.6519,  0.4476,  0.0763]) tensor([-0.6933,  0.4285,  0.0911])\n",
      "R[0]\n",
      "tensor([0.0140], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018411895595490932 0.005970233364860178 0.0037643365639123657 0.002022263289778493 0.4765463874936104 0.004123018488287926 0.12032635790109635\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Average (on the epoch) training loss: 0.002339999285765225\n",
      "Episode average V value: -1\n",
      "epoch 8:\n",
      "Learning rate: 0.00023914845\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0106,  0.8755, -0.2439]) tensor([ 0.0140,  0.8442, -0.2329]) tensor([-0.4098,  0.6267, -0.0272])\n",
      "R[0]\n",
      "tensor([0.0027], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018617481115274132 0.003923908355778621 0.003940012338232919 0.002748285140725784 0.4709115989208221 0.0045885065272450445 0.11777739638462663\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0879,  0.7017, -0.1902]) tensor([ 0.0744,  0.6745, -0.1939]) tensor([ 0.2002,  0.7502, -0.3228])\n",
      "R[0]\n",
      "tensor([0.0042], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.01821558531932533 0.004799778534885263 0.003480835188423953 0.002037420489185024 0.47572056210041047 0.004181808985769748 0.11828518304228783\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1663,  0.4841, -0.4326]) tensor([ 0.0973,  0.5140, -0.3471]) tensor([ 0.0595,  0.4669, -0.3194])\n",
      "R[0]\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018269936870783567 0.00475143123241287 0.004450645365690434 0.002510640868800692 0.4724925339221954 0.004112325266003609 0.11819709803909063\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2942,  0.9556, -0.1786]) tensor([ 0.2585,  0.9127, -0.2048]) tensor([ 0.4675,  0.9592, -0.3410])\n",
      "R[0]\n",
      "tensor([0.0058], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018234137100167573 0.004153913739955896 0.0038719673255764066 0.001997363693371881 0.4717810502052307 0.00400723684579134 0.11484246815741062\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 1.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.9833,  0.2744, -0.1741]) tensor([-1.0395,  0.4407, -0.0567]) tensor([ 0.4070,  0.7404, -0.2749])\n",
      "R[0]\n",
      "tensor([0.0196], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018592853146605193 0.005623643365001044 0.0038405899160679838 0.0021803020282532086 0.4704583495259285 0.004670918405056 0.11736951550841332\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1664,  0.5759, -0.2732]) tensor([-0.2332,  0.6254, -0.2014]) tensor([-0.0941,  0.7270, -0.2397])\n",
      "R[0]\n",
      "tensor([-0.0021], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.019080283840186892 0.005228708908434783 0.00453861205607609 0.0020526607949868775 0.46653643089532854 0.004485703550279141 0.1160243261680007\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6032,  0.1253,  0.2010]) tensor([-0.5581,  0.1919,  0.1433]) tensor([-0.7173,  0.1013,  0.2619])\n",
      "R[0]\n",
      "tensor([-0.0436], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018568190058693288 0.004746656491774047 0.00359289398760302 0.002299082097481005 0.4685150279402733 0.004909868083894253 0.116123858820647\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5697,  0.1264,  0.4763]) tensor([-0.5507,  0.1396,  0.3552]) tensor([-0.2424,  0.2677,  0.2234])\n",
      "R[0]\n",
      "tensor([-0.1604], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018601437876932324 0.005421256931240351 0.0036882854867799322 0.002053768620651681 0.4675979161858559 0.003997654981911182 0.11720787382125855\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5063,  0.4652,  0.0545]) tensor([-0.4760,  0.4662,  0.0263]) tensor([-0.3905,  0.5048, -0.0550])\n",
      "R[0]\n",
      "tensor([0.0110], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018556732927449048 0.003941725496868457 0.003097424592549942 0.002057149088534061 0.46771089047193526 0.003935593910515308 0.11410808403789997\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5930,  0.1879,  0.0914]) tensor([-0.5296,  0.2064,  0.0654]) tensor([-0.2551,  0.3555,  0.0075])\n",
      "R[0]\n",
      "tensor([-0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.018566001273691655 0.005186719312754576 0.0030484880283620443 0.00196263149229344 0.4663358649611473 0.0035010659396648405 0.11619773460924625\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Average (on the epoch) training loss: 0.0021899304314283653\n",
      "Episode average V value: -1\n",
      "epoch 9:\n",
      "Learning rate: 0.000215233605\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1387,  0.6356, -0.2749]) tensor([ 0.1319,  0.5929, -0.2525]) tensor([ 0.3861,  0.7779, -0.4882])\n",
      "R[0]\n",
      "tensor([0.0015], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"params\")\n",
    "except Exception:\n",
    "    pass\n",
    "dump(vars(parameters), \"params/\" + fname + \".jldump\")\n",
    "agent.gathering_data=False\n",
    "agent.run(parameters.epochs, parameters.steps_per_epoch)\n",
    "\n",
    "# --- Show results ---\n",
    "basename = \"scores/\" + fname\n",
    "scores = load(basename + \"_scores.jldump\")\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.setNetwork(fname, nEpoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._in_episode = True\n",
    "agent._mode = 0 # Testing mode with plan_depth=0\n",
    "initState = env.reset(agent._mode)\n",
    "inputDims = env.inputDimensions()\n",
    "\n",
    "for i in range(len(inputDims)):\n",
    "    if inputDims[i][0] > 1:\n",
    "        agent._state[i][1:] = initState[i][1:]\n",
    "agent._Vs_on_last_episode = []\n",
    "is_terminal = False\n",
    "reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for i in range(100):\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    plt.figure()\n",
    "    plt.imshow(np.flip(_obs.squeeze()))\n",
    "    plt.show()\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "    V, action, reward = agent._step()\n",
    "    print(action)\n",
    "    agent._Vs_on_last_episode.append(V)\n",
    "    is_terminal = env.inTerminalState()\n",
    "    if is_terminal: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "obs = env.observe()\n",
    "_obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "_obs = np.flip(_obs.squeeze())\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "im = ax.imshow(np.zeros(_obs.shape))\n",
    "\n",
    "def init():\n",
    "    plt.cla()\n",
    "    im = ax.imshow(_obs)\n",
    "    return [im]\n",
    "\n",
    "def animate(i, *args, **kwargs):\n",
    "    plt.cla()\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    _obs = np.flip(_obs.squeeze())\n",
    "    im = ax.imshow(_obs)\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "        V, action, reward = agent._step()\n",
    "        agent._Vs_on_last_episode.append(V)\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init, \n",
    "     frames=100, blit=False, repeat=True)\n",
    "ani.save('behavior.gif', writer=\"ffmpeg\", fps = 15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
