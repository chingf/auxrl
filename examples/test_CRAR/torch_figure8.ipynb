{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import hash, dump, load\n",
    "import os\n",
    "\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.CRAR_torch import CRAR\n",
    "from figure8_env import MyEnv as figure8_env\n",
    "import deer.experiment.base_controllers as bc\n",
    "\n",
    "from deer.policies import EpsilonGreedyPolicy, FixedFigure8Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure8_give_rewards = True\n",
    "nn_yaml = 'network_noconv.yaml'\n",
    "higher_dim_obs = False\n",
    "internal_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    steps_per_epoch = 5000\n",
    "    epochs = 50\n",
    "    steps_per_test = 1000\n",
    "    period_btw_summary_perfs = 1\n",
    "\n",
    "    # ----------------------\n",
    "    # Temporal Processing Parameters\n",
    "    # ----------------------\n",
    "    nstep = 15\n",
    "    recurrent = False\n",
    "    \n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    frame_skip = 2\n",
    "    show_rewards = False\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    update_rule = 'rmsprop'\n",
    "    learning_rate = 1 * 1E-4 # 1E-4\n",
    "    learning_rate_decay = 0.9\n",
    "    discount = 0.9\n",
    "    discount_inc = 1\n",
    "    discount_max = 0.99\n",
    "    rms_decay = 0.9\n",
    "    rms_epsilon = 0.0001\n",
    "    momentum = 0\n",
    "    clip_norm = 1.0\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 1.0\n",
    "    epsilon_decay = 10000\n",
    "    update_frequency = 1\n",
    "    replay_memory_size = 1000000 #replacing with 200000 will works just fine (in case you dont have 18gb of memory)\n",
    "    batch_size = 32\n",
    "    freeze_interval = 1000\n",
    "    deterministic = False\n",
    "    \n",
    "    # ----------------------\n",
    "    # Learning algo parameters\n",
    "    # ----------------------\n",
    "    loss_weights = [1E-2, 1E-3, 1E-3, 1E-3, 1E-3, 1E-3, 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters hash is: 62977be8e45d8a56a5537c11dfd5d2fd8dda69e0\n",
      "The parameters are: <__main__.Defaults object at 0x124d1fbb0>\n",
      "end gathering data\n"
     ]
    }
   ],
   "source": [
    "parameters = Defaults()\n",
    "if parameters.deterministic:\n",
    "    rng = np.random.RandomState(123456)\n",
    "else:\n",
    "    rng = np.random.RandomState()\n",
    "\n",
    "# --- Instantiate environment ---\n",
    "env = figure8_env(\n",
    "    give_rewards=figure8_give_rewards,\n",
    "    intern_dim=internal_dim,\n",
    "    higher_dim_obs=higher_dim_obs,\n",
    "    show_rewards=parameters.show_rewards,\n",
    "    nstep=parameters.nstep\n",
    "    )\n",
    "\n",
    "# --- Instantiate learning_algo ---\n",
    "learning_algo = CRAR(\n",
    "    env,\n",
    "    parameters.rms_decay,\n",
    "    parameters.rms_epsilon,\n",
    "    parameters.momentum,\n",
    "    parameters.clip_norm,\n",
    "    parameters.freeze_interval,\n",
    "    parameters.batch_size,\n",
    "    parameters.update_rule,\n",
    "    rng,\n",
    "    high_int_dim=False,\n",
    "    internal_dim=internal_dim, lr=parameters.learning_rate,\n",
    "    nn_yaml=nn_yaml, double_Q=True,\n",
    "    loss_weights=parameters.loss_weights,\n",
    "    nstep=parameters.nstep,\n",
    "    recurrent=parameters.recurrent\n",
    "    )\n",
    "\n",
    "if figure8_give_rewards:\n",
    "    train_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.2,\n",
    "        consider_valid_transitions=False\n",
    "        )\n",
    "    test_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.\n",
    "        )\n",
    "else:\n",
    "    train_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng, epsilon=0.2,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "    test_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "\n",
    "# --- Instantiate agent ---\n",
    "agent = NeuralAgent(\n",
    "    env,\n",
    "    learning_algo,\n",
    "    parameters.replay_memory_size,\n",
    "    1,\n",
    "    parameters.batch_size,\n",
    "    rng,\n",
    "    train_policy=train_policy,\n",
    "    test_policy=test_policy)\n",
    "\n",
    "# --- Create unique filename for FindBestController ---\n",
    "h = hash(vars(parameters), hash_name=\"sha1\")\n",
    "fname = \"test_\" + h\n",
    "print(\"The parameters hash is: {}\".format(h))\n",
    "print(\"The parameters are: {}\".format(parameters))\n",
    "\n",
    "# As for the discount factor and the learning rate, one can update periodically the parameter of the epsilon-greedy\n",
    "# policy implemented by the agent. This controllers has a bit more capabilities, as it allows one to choose more\n",
    "# precisely when to update epsilon: after every X action, episode or epoch. This parameter can also be reset every\n",
    "# episode or epoch (or never, hence the resetEvery='none').\n",
    "agent.attach(bc.EpsilonController(\n",
    "    initial_e=parameters.epsilon_start,\n",
    "    e_decays=parameters.epsilon_decay,\n",
    "    e_min=parameters.epsilon_min,\n",
    "    evaluate_on='episode',\n",
    "    periodicity=1,\n",
    "    reset_every='none'))\n",
    "\n",
    "agent.run(10, 500)\n",
    "print(\"end gathering data\")\n",
    "\n",
    "# --- Bind controllers to the agent ---\n",
    "# Before every training epoch (periodicity=1), we want to print a summary of the agent's epsilon, discount and \n",
    "# learning rate as well as the training epoch number.\n",
    "agent.attach(bc.VerboseController(\n",
    "    evaluate_on='epoch', \n",
    "    periodicity=1))\n",
    "\n",
    "# Every epoch end, one has the possibility to modify the learning rate using a LearningRateController. Here we \n",
    "# wish to update the learning rate after every training epoch (periodicity=1), according to the parameters given.\n",
    "agent.attach(bc.LearningRateController(\n",
    "    initial_learning_rate=parameters.learning_rate, \n",
    "    learning_rate_decay=parameters.learning_rate_decay,\n",
    "    periodicity=1))\n",
    "\n",
    "# Same for the discount factor.\n",
    "agent.attach(bc.DiscountFactorController(\n",
    "    initial_discount_factor=parameters.discount, \n",
    "    discount_factor_growth=parameters.discount_inc, \n",
    "    discount_factor_max=parameters.discount_max,\n",
    "    periodicity=1))\n",
    "\n",
    "# During training epochs, we want to train the agent after every [parameters.update_frequency] action it takes.\n",
    "# Plus, we also want to display after each training episode (!= than after every training) the average bellman\n",
    "# residual and the average of the V values obtained during the last episode, hence the two last arguments.\n",
    "agent.attach(bc.TrainerController(\n",
    "    evaluate_on='action', \n",
    "    periodicity=parameters.update_frequency, \n",
    "    show_episode_avg_V_value=True, \n",
    "    show_avg_Bellman_residual=True))\n",
    "\n",
    "# We wish to discover, among all versions of our neural network (i.e., after every training epoch), which one \n",
    "# has the highest validation score.\n",
    "# To achieve this goal, one can use the FindBestController along with an InterleavedTestEpochControllers. It is \n",
    "# important that the validationID is the same than the id argument of the InterleavedTestEpochController.\n",
    "# The FindBestController will dump on disk the validation scores for each and every network, as well as the \n",
    "# structure of the neural network having the best validation score. These dumps can then used to plot the evolution \n",
    "# of the validation and test scores (see below) or simply recover the resulting neural network for your \n",
    "# application.\n",
    "agent.attach(bc.FindBestController(\n",
    "    validationID=figure8_env.VALIDATION_MODE,\n",
    "    testID=None,\n",
    "    unique_fname=fname))\n",
    "\n",
    "# All previous controllers control the agent during the epochs it goes through. However, we want to interleave a \n",
    "# \"validation epoch\" between each training epoch. For each validation epoch, we want also to display the sum of all \n",
    "# rewards obtained, hence the showScore=True. Finally, we want to call the summarizePerformance method of ALE_env \n",
    "# every [parameters.period_btw_summary_perfs] *validation* epochs.\n",
    "agent.attach(bc.InterleavedTestEpochController(\n",
    "    id=figure8_env.VALIDATION_MODE, \n",
    "    epoch_length=parameters.steps_per_test,\n",
    "    periodicity=1,\n",
    "    show_score=True,\n",
    "    summarize_every=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2334, -0.0143]) tensor([0.5849, 0.1674]) tensor([0.2188, 0.0185])\n",
      "R[0]\n",
      "tensor([-0.0825], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01104453498334624 0.01070014631824597 0.07545845989463851 0.01419798760942649 0.8061926189661026 0.0 0.3474926076829433\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3873, -0.1978]) tensor([ 0.3846, -0.1995]) tensor([ 0.3657, -0.2250])\n",
      "R[0]\n",
      "tensor([0.0152], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0037904320370871574 0.012158394954618415 0.020184367271780503 0.01212439457088476 0.7221172190904618 0.0 0.2705320022404194\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2972, -0.1289]) tensor([ 0.3157, -0.1118]) tensor([ 0.3660, -0.0242])\n",
      "R[0]\n",
      "tensor([0.0238], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.007386068703141064 0.01289527680298488 0.01710346641682554 0.012961058452492579 0.6451469676494599 0.0 0.22574475908279418\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8262, 0.5454]) tensor([0.8020, 0.5338]) tensor([0.8317, 0.7009])\n",
      "R[0]\n",
      "tensor([-0.0129], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.010685919626615942 0.012045614550195751 0.01706515057227807 0.011755103620118461 0.6091016295552254 3.230225294828415e-05 0.21748654359579087\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2757, 0.1705]) tensor([0.2814, 0.1842]) tensor([0.3460, 0.2588])\n",
      "R[0]\n",
      "tensor([0.0108], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014536697594448924 0.014096362714393762 0.01668615870634676 0.013561999807250686 0.584748245716095 0.0016089990735054017 0.19352859556674956\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.0570, 0.2039]) tensor([0.0746, 0.2262]) tensor([0.1604, 0.2397])\n",
      "R[0]\n",
      "tensor([0.0076], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014741255212575197 0.011561020286229904 0.01669093155520386 0.010978696930687875 0.590953697860241 0.003060466334223747 0.21586406716704368\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8497, 0.6355]) tensor([0.8172, 0.6087]) tensor([0.8547, 0.6893])\n",
      "R[0]\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.019123491559177638 0.012450191751297097 0.014162692566678743 0.011874346426688135 0.5570921086668968 0.0018792948946356773 0.1980230820477009\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8751, 0.7904]) tensor([0.8466, 0.7705]) tensor([0.9013, 0.8139])\n",
      "R[0]\n",
      "tensor([-0.0152], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.019968720342963934 0.012837269745476079 0.012374050879596326 0.011569172463961877 0.5617661861181259 0.004273933932185173 0.21687235648185016\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6012, 0.6790]) tensor([0.6015, 0.6775]) tensor([0.6879, 0.7179])\n",
      "R[0]\n",
      "tensor([0.0053], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.024270820470526814 0.011358356854267185 0.011910901417832066 0.011555782294017262 0.5435982899069786 0.005212254442274571 0.20163764610886573\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.0672, 0.2353]) tensor([0.1274, 0.2813]) tensor([0.3722, 0.3649])\n",
      "R[0]\n",
      "tensor([0.0436], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.023372726664878427 0.012736922983312979 0.011414168734030682 0.011711786265484988 0.5587719608545303 0.009958468072116375 0.2249231897443533\n",
      "Average (on the epoch) training loss: 0.012229032844101312\n",
      "Episode average V value: 0\n",
      "epoch 1:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2521, 0.3956]) tensor([0.2887, 0.4275]) tensor([0.3661, 0.5091])\n",
      "R[0]\n",
      "tensor([0.0432], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.026326301941648124 0.012023141971963924 0.012768864651807234 0.012416279860306531 0.5483728458285332 0.007371815621852875 0.21411279591917992\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3384, 0.5602]) tensor([0.3679, 0.5797]) tensor([0.4349, 0.6423])\n",
      "R[0]\n",
      "tensor([0.0265], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02471170519012958 0.011800693987344858 0.012125166388177604 0.012201135050039738 0.5614536709785461 0.012292089998722076 0.22927417716383933\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5570, 0.6231]) tensor([0.5476, 0.6098]) tensor([0.5695, 0.6373])\n",
      "R[0]\n",
      "tensor([0.0160], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.026171101436950266 0.011608018062106566 0.011732722132146592 0.012871286197099834 0.556017145216465 0.012815292693674565 0.2200612614750862\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6170, 0.6661]) tensor([0.6114, 0.6596]) tensor([0.3968, 0.5202])\n",
      "R[0]\n",
      "tensor([0.0233], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.025337679600343107 0.012295251471892697 0.012729924936720635 0.01267076067486778 0.5658637591004372 0.019722969248890877 0.2312289725393057\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8459, 0.9005]) tensor([0.7872, 0.8417]) tensor([0.8356, 0.8895])\n",
      "R[0]\n",
      "tensor([-0.0007], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.025867893459275364 0.011288573098994675 0.010573434302350505 0.013374097761232407 0.568858282148838 0.021697197295725345 0.22620732748508454\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8506, 0.8814]) tensor([0.8317, 0.8612]) tensor([0.7572, 0.7807])\n",
      "R[0]\n",
      "tensor([0.0003], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.025687381045892833 0.013157203028618824 0.010463154182536528 0.014167823448311537 0.5718216492533684 0.021276712574064733 0.23399207770824432\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([1.0125, 1.0111]) tensor([0.9792, 0.9838]) tensor([1.0251, 1.0398])\n",
      "R[0]\n",
      "tensor([-0.0023], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02720978146418929 0.01177836862381082 0.010625777885798016 0.014804091199766844 0.5634712438583374 0.017251791074872017 0.22667060726881028\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4270, 0.3185]) tensor([0.4514, 0.3491]) tensor([0.4168, 0.3675])\n",
      "R[0]\n",
      "tensor([0.0116], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.025855977267026903 0.012690364750567824 0.009943321799946716 0.014888481210218742 0.5720549385547637 0.01526405319571495 0.2334953460842371\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3246, 0.2819]) tensor([0.3623, 0.3235]) tensor([0.3004, 0.2317])\n",
      "R[0]\n",
      "tensor([0.0133], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02497395823895931 0.011699340194027172 0.008577623716613744 0.015081176585983486 0.5776574721336365 0.01677012475579977 0.23801931349933148\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2087, -0.5084]) tensor([-0.0959, -0.3766]) tensor([-0.1586, -0.6212])\n",
      "R[0]\n",
      "tensor([0.1355], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.023254968418739735 0.013575211468007182 0.008997498766322679 0.016430327062495052 0.5843360362648964 0.013166035138070584 0.24209684444963933\n",
      "Average (on the epoch) training loss: 0.013890545905032196\n",
      "Episode average V value: 0\n",
      "epoch 2:\n",
      "Learning rate: 9e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 1.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.9999000099990001 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5339, 0.2874]) tensor([0.5420, 0.3198]) tensor([0.6252, 0.5795])\n",
      "R[0]\n",
      "tensor([0.0520], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.024418396648019553 0.012561231231869897 0.009356364128019778 0.01661758646881208 0.57176496976614 0.0121889438778162 0.23008953416347502\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4592, 0.6119]) tensor([0.4799, 0.6134]) tensor([0.6889, 0.7237])\n",
      "R[0]\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.023128273741342128 0.013032151797640836 0.008515601994862664 0.015917165318969636 0.5819884407520294 0.010519404046237468 0.24058349414169788\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7708, 0.8465]) tensor([0.7606, 0.8303]) tensor([0.7748, 0.8502])\n",
      "R[0]\n",
      "tensor([-0.0028], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022145913540385663 0.011130786151421488 0.008780662820056023 0.01662523973919451 0.5783769028782845 0.007298795714974404 0.23139504301548003\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9735, 0.9771]) tensor([0.9407, 0.9497]) tensor([0.9129, 0.9447])\n",
      "R[0]\n",
      "tensor([-0.0020], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022002404493279754 0.012582119279366453 0.008677514350507409 0.016935264677740633 0.5798499153852463 0.006522465914487839 0.23373266366124154\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([1.0554, 1.0735]) tensor([1.0252, 1.0363]) tensor([1.0226, 1.0432])\n",
      "R[0]\n",
      "tensor([-0.0128], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021717701396904886 0.013199549788143486 0.008068200851648727 0.01750474740192294 0.5882835180163384 0.007800214529037476 0.2403308534771204\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0129,  0.0211]) tensor([0.0876, 0.1138]) tensor([0.3975, 0.3944])\n",
      "R[0]\n",
      "tensor([0.0925], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02055365171004087 0.013142125216763816 0.00795654788184038 0.017176574417389928 0.5934917776584625 0.007623046837747097 0.23840443572402\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3540, 0.3145]) tensor([0.3812, 0.3466]) tensor([-0.0083,  0.0463])\n",
      "R[0]\n",
      "tensor([0.0612], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.018872137325350195 0.011876422108762198 0.008570627135180985 0.01697086556768045 0.5993189978599548 0.007010039538145065 0.23827336411178113\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1619, 0.3666]) tensor([0.2208, 0.3849]) tensor([0.0979, 0.2710])\n",
      "R[0]\n",
      "tensor([-0.0152], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02039439857751131 0.014062508911942133 0.007700935641663818 0.01814104598481208 0.5965093182325363 0.006008960753679276 0.23524071031808852\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9690, 1.0172]) tensor([0.9436, 0.9998]) tensor([0.9676, 1.0225])\n",
      "R[0]\n",
      "tensor([-0.0152], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.019019589943811296 0.012405689573817654 0.00788463426585804 0.017379806213080884 0.5960799173712731 0.004265791654586792 0.23273964801430702\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3352, 0.4610]) tensor([0.3732, 0.4637]) tensor([0.1261, 0.2759])\n",
      "R[0]\n",
      "tensor([0.0361], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01975279479380697 0.014053423691861099 0.007693838384293486 0.017958371250424533 0.5940714952349663 0.0037699805796146393 0.2345575172752142\n",
      "Average (on the epoch) training loss: 0.017122666704002767\n",
      "Episode average V value: 0\n",
      "epoch 3:\n",
      "Learning rate: 8.1e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9310, 0.8998]) tensor([0.9058, 0.8876]) tensor([0.8758, 0.8426])\n",
      "R[0]\n",
      "tensor([-0.0152], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.019138910461217164 0.013637180394609458 0.008768227619111713 0.018236979353707283 0.6031555152535438 0.003958982639014721 0.2370158165693283\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4224, 0.4531]) tensor([0.4479, 0.4727]) tensor([0.4368, 0.4508])\n",
      "R[0]\n",
      "tensor([0.0389], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.018241334727499635 0.012148550540034194 0.007945655021132552 0.01793952153995633 0.6076872432231903 0.004067269086837769 0.23867130298912526\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9896, 1.0163]) tensor([0.9761, 0.9955]) tensor([0.9137, 0.9471])\n",
      "R[0]\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.018481335731223226 0.013678192685794784 0.007649583759168309 0.01821852633636445 0.6103257739543915 0.0053169019967317585 0.23519151540100575\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([1.0275, 1.0215]) tensor([1.0108, 0.9913]) tensor([1.0470, 1.0352])\n",
      "R[0]\n",
      "tensor([-0.0230], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.017740821281913666 0.013697114374750527 0.007021234311534499 0.018054082246962935 0.6155650132894516 0.005418513670563698 0.2351470248103142\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6622, 0.4191]) tensor([0.6505, 0.4408]) tensor([0.5603, 0.2617])\n",
      "R[0]\n",
      "tensor([0.0386], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.017464101992547512 0.012815801350312541 0.006348407095061703 0.016555491178762167 0.6121420296430587 0.004436480574309826 0.23846477837860583\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8658, 0.9086]) tensor([0.8476, 0.8829]) tensor([0.8611, 0.9117])\n",
      "R[0]\n",
      "tensor([-0.0013], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016532667928375305 0.01327208855896606 0.006615631871114602 0.0171873229816556 0.6176063735485077 0.0036294525489211082 0.23499474185705185\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6437, 0.3902]) tensor([0.6125, 0.4068]) tensor([0.6844, 0.5828])\n",
      "R[0]\n",
      "tensor([0.0494], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0167606196468696 0.012307811449543805 0.0074350186796255 0.017553283229004592 0.6161141488552093 0.003411253236234188 0.2307680386453867\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2923, 0.1654]) tensor([0.2896, 0.1755]) tensor([0.4291, 0.3679])\n",
      "R[0]\n",
      "tensor([0.0470], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01657350234966725 0.011640980473690433 0.007691515197249828 0.01732021727366373 0.6191235852837562 0.0031839996948838235 0.23143903237581254\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3554, 0.2256]) tensor([0.3533, 0.2291]) tensor([0.5406, 0.3451])\n",
      "R[0]\n",
      "tensor([0.0339], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01664364240411669 0.011252930500078947 0.007612115074709437 0.016363604772370307 0.6162975999712944 0.001810491442680359 0.23440811370313167\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8356, 0.8149]) tensor([0.8243, 0.8000]) tensor([0.8392, 0.7692])\n",
      "R[0]\n",
      "tensor([-0.0037], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.017004646003246308 0.012302611134859035 0.00557386562573447 0.017508627727162093 0.6145964237451553 0.0018212275058031083 0.23251116955280304\n",
      "Average (on the epoch) training loss: 0.01749376566396095\n",
      "Episode average V value: 0\n",
      "epoch 4:\n",
      "Learning rate: 7.290000000000001e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 1.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.9999000099990001 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9170, 0.9307]) tensor([0.9016, 0.9058]) tensor([0.9025, 0.9082])\n",
      "R[0]\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015751947396434842 0.011798032365564723 0.006926304009888554 0.01723188784066588 0.6184972515106201 0.0020303176790475845 0.23571173533797263\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8794, 0.7265]) tensor([0.8514, 0.7201]) tensor([0.7957, 0.7452])\n",
      "R[0]\n",
      "tensor([0.0062], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016088233693502842 0.013552709853945999 0.005397588441046537 0.017853448670823127 0.6159757412075997 0.002225563794374466 0.23068288215994834\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6392, 0.6798]) tensor([0.6364, 0.6671]) tensor([0.6194, 0.6739])\n",
      "R[0]\n",
      "tensor([0.0068], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016661721057724208 0.011865707146993372 0.005963191394184833 0.016981227081269024 0.6135488234162331 0.002526794396340847 0.22173676997423172\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 1.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3382, -0.2917]) tensor([-0.1543, -0.1316]) tensor([0.2930, 0.1692])\n",
      "R[0]\n",
      "tensor([0.1285], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016860558413900436 0.01280381777562434 0.006053958268086717 0.018347931358963253 0.6136803604960441 0.002689555920660496 0.22080631022155284\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9911, 0.9673]) tensor([0.9857, 0.9683]) tensor([0.9823, 0.9698])\n",
      "R[0]\n",
      "tensor([-0.0182], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016510605349205433 0.013633590134937548 0.00617288085634209 0.01838963393308222 0.6158586992621422 0.001830407440662384 0.22758206795156002\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7171, 0.5984]) tensor([0.6930, 0.5855]) tensor([0.8800, 0.8262])\n",
      "R[0]\n",
      "tensor([0.0090], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01631339613907039 0.013671578986948589 0.0052827872035413745 0.018003739791922272 0.6167341989874839 0.001847579389810562 0.22417803111672402\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4725, 0.5704]) tensor([0.4970, 0.5662]) tensor([0.5302, 0.6044])\n",
      "R[0]\n",
      "tensor([0.0283], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01584868374094367 0.013566438780893805 0.0053603789360167865 0.017299883821979164 0.6207624899744988 0.001636120468378067 0.23093323780596256\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9291, 0.9069]) tensor([0.9065, 0.8841]) tensor([0.9240, 0.9016])\n",
      "R[0]\n",
      "tensor([-0.0001], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015567935190163553 0.013446334520500387 0.0061097345250964285 0.01679619938414544 0.6209212012290954 0.0019355012774467468 0.23159715680778026\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7480, 0.7903]) tensor([0.7430, 0.7677]) tensor([0.7357, 0.7655])\n",
      "R[0]\n",
      "tensor([-0.0045], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016003151450306178 0.01329182733836933 0.005448919250073231 0.0172843449274078 0.6175858937501907 0.0013373730778694153 0.23559237755835055\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7014, 0.6547]) tensor([0.6974, 0.6554]) tensor([0.7042, 0.6566])\n",
      "R[0]\n",
      "tensor([0.0074], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016165338885039092 0.013406247499922756 0.0056705171871508355 0.016869522660505027 0.6221335658431053 0.001255889840424061 0.2304564998447895\n",
      "Average (on the epoch) training loss: 0.01750578194707632\n",
      "Episode average V value: 0\n",
      "epoch 5:\n",
      "Learning rate: 6.561000000000002e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 1.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6443, 0.0961]) tensor([0.5966, 0.1679]) tensor([0.7687, 0.7778])\n",
      "R[0]\n",
      "tensor([0.1182], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0168585600964725 0.012397700776607963 0.006736498765509168 0.017370630567427724 0.6145730403065681 0.001282649539411068 0.23164668099582195\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3612, 0.0942]) tensor([0.3647, 0.1521]) tensor([0.4137, 0.0841])\n",
      "R[0]\n",
      "tensor([0.0971], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016352456118445845 0.011912190260482021 0.005892596741148736 0.017584627324249594 0.6197105634212494 0.001383038692176342 0.23719579949975014\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5557, 0.3230]) tensor([0.5291, 0.3330]) tensor([0.7444, 0.6977])\n",
      "R[0]\n",
      "tensor([0.0470], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.017406240516342222 0.014267759435635526 0.007479465730546508 0.018447013286408038 0.6131478108763695 0.0012330594584345819 0.23286613170802592\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4833, 0.5059]) tensor([0.4803, 0.4854]) tensor([0.4608, 0.4758])\n",
      "R[0]\n",
      "tensor([0.0143], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016373519249260424 0.013406425746288732 0.0057687558271936725 0.017250391923822463 0.6199847284555435 0.0017214219719171523 0.2344331405609846\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8836, 0.7912]) tensor([0.8670, 0.7873]) tensor([0.9253, 0.8339])\n",
      "R[0]\n",
      "tensor([-0.0032], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.018069102888926863 0.014715010781539604 0.005994197019175772 0.01871011486183852 0.609320374071598 0.0008861039876937866 0.23005947124958037\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8694, 0.8595]) tensor([0.8436, 0.8354]) tensor([0.8039, 0.7907])\n",
      "R[0]\n",
      "tensor([-0.0009], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016670787432231007 0.012662096041458426 0.005665489540628186 0.01783983684470877 0.6164853744506836 0.0014290847703814506 0.23489455252885819\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7988, 0.7470]) tensor([0.7768, 0.7252]) tensor([0.7906, 0.7345])\n",
      "R[0]\n",
      "tensor([0.0046], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.018285885181743653 0.013717131989891641 0.006478387448070862 0.019053230352699756 0.6102526138424873 0.0015169944167137146 0.22208805441856383\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7967, 0.7714]) tensor([0.7912, 0.7683]) tensor([0.8368, 0.8283])\n",
      "R[0]\n",
      "tensor([-0.0013], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.017920444956980645 0.013461281382697053 0.005057394682939048 0.018811979339458048 0.6102219523787499 0.0020394989997148515 0.22479969449341297\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1711, 0.0089]) tensor([0.2218, 0.0891]) tensor([ 0.0100, -0.1261])\n",
      "R[0]\n",
      "tensor([0.1026], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.017233297879807652 0.013068539858533767 0.004857206103450153 0.018761933253146706 0.6145531249642372 0.002162380963563919 0.222756023183465\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2471, 0.0403]) tensor([0.2762, 0.0892]) tensor([0.3159, 0.0538])\n",
      "R[0]\n",
      "tensor([0.0780], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.017155201195739208 0.014102608188550221 0.005056874031892221 0.018774491192307324 0.6150754704475403 0.0018820408582687377 0.22221850894391537\n",
      "Average (on the epoch) training loss: 0.018260424894606695\n",
      "Episode average V value: 0\n",
      "epoch 6:\n",
      "Learning rate: 5.904900000000002e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8620, 0.8342]) tensor([0.8464, 0.8076]) tensor([0.8413, 0.8255])\n",
      "R[0]\n",
      "tensor([-0.0117], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.017466826546005905 0.013030266828893218 0.0044685646994585115 0.01888530354667455 0.6162555629014969 0.001931519478559494 0.22353161764144897\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3891, 0.3376]) tensor([0.3945, 0.3281]) tensor([0.4978, 0.4227])\n",
      "R[0]\n",
      "tensor([-0.0101], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.017184390576556324 0.013270132054109126 0.005931028405131656 0.01858478816598654 0.6152073990106582 0.002301888905465603 0.22490744023025036\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7394, 0.5999]) tensor([0.7153, 0.5929]) tensor([0.6765, 0.5135])\n",
      "R[0]\n",
      "tensor([-0.0009], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015911949812900276 0.012706702099705581 0.005316629228869715 0.01666547993524 0.6205591667890549 0.0020249655321240424 0.22470226787030698\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9697, 0.9087]) tensor([0.9531, 0.9059]) tensor([0.9001, 0.8456])\n",
      "R[0]\n",
      "tensor([-0.0168], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01604540141345933 0.013665083435771521 0.004642046038443368 0.01821325453836471 0.6220383859872818 0.002202905796468258 0.2234226139485836\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8396, 0.8475]) tensor([0.8355, 0.8318]) tensor([0.8785, 0.8602])\n",
      "R[0]\n",
      "tensor([-0.0042], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01605633543431759 0.012704234382981667 0.005095749186046305 0.017644133761757986 0.6202706528306008 0.0022948782816529273 0.22395382957160473\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9920, 0.9424]) tensor([0.9667, 0.9277]) tensor([0.8501, 0.8333])\n",
      "R[0]\n",
      "tensor([0.0066], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015761343303136527 0.011635342232970289 0.0055334458307770545 0.017341965389437972 0.6240653739571571 0.0030502505227923394 0.22346031534671784\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8167, 0.5613]) tensor([0.7871, 0.5561]) tensor([0.6491, 0.3999])\n",
      "R[0]\n",
      "tensor([0.0277], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016581270603463052 0.011842424693561043 0.005090359704274306 0.017866497841663657 0.617054969906807 0.002569320634007454 0.21991189766675234\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1713, 0.2333]) tensor([0.1908, 0.2164]) tensor([-0.2159, -0.2030])\n",
      "R[0]\n",
      "tensor([-0.0180], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016997380912303926 0.01301234223053325 0.004685283941867965 0.0181257574907504 0.6182272816300393 0.00309904845058918 0.21202264226973055\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9163, 0.9189]) tensor([0.9108, 0.8961]) tensor([0.9290, 0.9196])\n",
      "R[0]\n",
      "tensor([-0.0118], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016468726437538862 0.013689857531091547 0.0055304430147371025 0.017388010427355768 0.6206314071416855 0.0025923290476202966 0.22514823573827744\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.0046, 0.0999]) tensor([0.0403, 0.0840]) tensor([-0.0811, -0.0065])\n",
      "R[0]\n",
      "tensor([0.0266], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015367599210701882 0.011552304619632196 0.005223200978733075 0.016463708096183836 0.6280756732225418 0.00273071701079607 0.22762176370620726\n",
      "Average (on the epoch) training loss: 0.01771788991934154\n",
      "Episode average V value: 0\n",
      "epoch 7:\n",
      "Learning rate: 5.314410000000002e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([1.0294, 0.8949]) tensor([1.0109, 0.8965]) tensor([1.0610, 0.9397])\n",
      "R[0]\n",
      "tensor([-0.0117], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015625141235999764 0.01285052976058796 0.0051325881355041925 0.016996528825722636 0.6240837422609329 0.0028928418904542925 0.21896067740023137\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9756, 0.9575]) tensor([0.9666, 0.9333]) tensor([0.9827, 0.9622])\n",
      "R[0]\n",
      "tensor([-0.0056], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016645597793161868 0.01292450523727166 0.004198260310164187 0.017557853582315146 0.6218958770036698 0.0029366638511419295 0.22010338598489762\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3032, 0.2618]) tensor([0.3069, 0.2525]) tensor([0.1201, 0.1154])\n",
      "R[0]\n",
      "tensor([-0.0093], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015951367241330446 0.012711484055616893 0.005001275105902096 0.016644935772754253 0.625315419793129 0.002150026299059391 0.22558389706909657\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8162, 0.8551]) tensor([0.8149, 0.8401]) tensor([0.7908, 0.8404])\n",
      "R[0]\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015971204462461173 0.01329149638475792 0.004336992920128978 0.01743251585774124 0.6251790852546691 0.0021368865072727205 0.22510168571770192\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5834, 0.5223]) tensor([0.5702, 0.5008]) tensor([0.5524, 0.4530])\n",
      "R[0]\n",
      "tensor([0.0078], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016410826856270433 0.013687717837077798 0.004837140814441227 0.01743406093167141 0.6231636047363281 0.0018774332329630852 0.22342732179164887\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8977, 0.8643]) tensor([0.8869, 0.8466]) tensor([0.8884, 0.8262])\n",
      "R[0]\n",
      "tensor([0.0132], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015575551654677838 0.012502909338814788 0.004191013667459629 0.01663935798872262 0.6313986904621124 0.0018596151024103166 0.2250147754997015\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5286, 0.5133]) tensor([0.5254, 0.4967]) tensor([0.3158, 0.2366])\n",
      "R[0]\n",
      "tensor([0.0004], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015167180556803942 0.011190292319704895 0.004481994902791484 0.016303216342814265 0.6317437691688538 0.0020686969980597495 0.22839479252696038\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5605, 0.6247]) tensor([0.5667, 0.6046]) tensor([0.2394, 0.4108])\n",
      "R[0]\n",
      "tensor([-0.0028], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015721244568936525 0.013283648175362033 0.003911366426595123 0.017717694023624064 0.6255706610679627 0.0017461805567145347 0.21791453148424625\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8834, 0.8173]) tensor([0.8617, 0.8083]) tensor([0.8870, 0.8314])\n",
      "R[0]\n",
      "tensor([0.0063], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0157334409779869 0.012456920602184255 0.004647466207388789 0.016982791613787414 0.623690248310566 0.001714179441332817 0.21900157490372657\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8100, 0.6857]) tensor([0.7937, 0.6880]) tensor([0.9091, 0.6787])\n",
      "R[0]\n",
      "tensor([0.0045], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01581413739360869 0.012831390351333538 0.004202283802365855 0.01740913862036541 0.6204086135625839 0.0018064948916435243 0.21759084010124208\n",
      "Average (on the epoch) training loss: 0.017111809355951847\n",
      "Episode average V value: 0\n",
      "epoch 8:\n",
      "Learning rate: 4.782969000000002e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([1.0138, 0.9677]) tensor([1.0046, 0.9718]) tensor([0.9941, 0.9828])\n",
      "R[0]\n",
      "tensor([-0.0074], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01602203391585499 0.012843351928371704 0.005229424704615667 0.017347290227655323 0.6209842606186867 0.0027830098867416384 0.2132491347193718\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8877, 0.8466]) tensor([0.8673, 0.8299]) tensor([0.9180, 0.8460])\n",
      "R[0]\n",
      "tensor([0.0158], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015495899505913258 0.01177586597003392 0.005134323463291367 0.017045694980304688 0.6246976327896118 0.0026151671558618545 0.21706626649200916\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9149, 0.9350]) tensor([0.9011, 0.9132]) tensor([0.9354, 0.9400])\n",
      "R[0]\n",
      "tensor([0.0062], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01614094895729795 0.011424122938493384 0.004500943627852394 0.016667821107432246 0.6200673089027405 0.002577037878334522 0.21749798053503036\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8734, 0.8955]) tensor([0.8608, 0.8684]) tensor([0.8595, 0.8903])\n",
      "R[0]\n",
      "tensor([-0.0162], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01636602608859539 0.011444765104562975 0.0051781269227612935 0.017381448896136135 0.6170602616667747 0.0029953905642032624 0.20959105768799782\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8519, 0.8291]) tensor([0.8408, 0.8162]) tensor([0.8238, 0.8184])\n",
      "R[0]\n",
      "tensor([0.0096], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016015443881042302 0.012822446498808858 0.004304247502728686 0.016236527036409824 0.6271485520005227 0.002483565889298916 0.21633141128718852\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4971, 0.5346]) tensor([0.5107, 0.5289]) tensor([0.5658, 0.4928])\n",
      "R[0]\n",
      "tensor([0.0069], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01496696152817458 0.012231183357027475 0.004294118456462457 0.01671229282207787 0.6276380586624145 0.0021398568749427796 0.21763696129620075\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5029, 0.3973]) tensor([0.4837, 0.3909]) tensor([0.5068, 0.3958])\n",
      "R[0]\n",
      "tensor([0.0190], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015679716602899133 0.011519427863968304 0.004577681607644991 0.016574594274628908 0.622979477405548 0.0019202258065342903 0.2080055800229311\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2713, 0.1714]) tensor([0.2944, 0.1940]) tensor([0.4261, 0.3221])\n",
      "R[0]\n",
      "tensor([0.0537], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01680537118576467 0.014297454086306971 0.004340452048174484 0.01728940236195922 0.6211357820630073 0.0018635940626263619 0.2065507655441761\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6492, 0.5808]) tensor([0.6252, 0.5621]) tensor([0.6199, 0.5639])\n",
      "R[0]\n",
      "tensor([0.0025], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016235904334113004 0.013006020749598975 0.0037416004969163625 0.01743376392731443 0.6212573871016502 0.0025782454684376717 0.20654051335155965\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5359, 0.5626]) tensor([0.5377, 0.5501]) tensor([0.5386, 0.5665])\n",
      "R[0]\n",
      "tensor([0.0104], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01603611931949854 0.013183988627162762 0.004595713970480574 0.017726653698366134 0.6298447629809379 0.0027004369571805 0.21522833743691444\n",
      "Average (on the epoch) training loss: 0.017041548933228477\n",
      "Episode average V value: 0\n",
      "epoch 9:\n",
      "Learning rate: 4.304672100000002e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3456, 0.3363]) tensor([0.3432, 0.3162]) tensor([0.4043, 0.4437])\n",
      "R[0]\n",
      "tensor([0.0080], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015981801332905887 0.013373523977614241 0.004289819459352657 0.017591508427169174 0.622831092953682 0.0022536427974700927 0.20920969589054583\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4842, 0.4031]) tensor([0.4839, 0.4059]) tensor([0.5004, 0.4509])\n",
      "R[0]\n",
      "tensor([0.0198], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015627212908118963 0.013765607578898198 0.004623025848046382 0.01794156674761325 0.623820417881012 0.0023869416490197182 0.21089127680659295\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5469, 0.5342]) tensor([0.5433, 0.5261]) tensor([0.5756, 0.5630])\n",
      "R[0]\n",
      "tensor([0.0049], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015702429249417036 0.012176869544586225 0.0050532824838883245 0.016448084481991827 0.6265914750099182 0.003016102336347103 0.20849459934234618\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7891, 0.6980]) tensor([0.7724, 0.6929]) tensor([0.8734, 0.7613])\n",
      "R[0]\n",
      "tensor([0.0050], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015375593872275203 0.01172454197009938 0.00443809558499197 0.016608237112872303 0.6280789725780487 0.0026134886890649797 0.2110803886204958\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5300, 0.4585]) tensor([0.5321, 0.4619]) tensor([0.5961, 0.5650])\n",
      "R[0]\n",
      "tensor([0.0200], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016055307246744633 0.013161152561158814 0.004385289436888343 0.017032602957915515 0.6284858762025833 0.001959401659667492 0.2121572699546814\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8047, 0.8483]) tensor([0.8019, 0.8398]) tensor([0.8112, 0.8392])\n",
      "R[0]\n",
      "tensor([-0.0103], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0158997555160895 0.012832013808671036 0.003986784465536402 0.01681904882658273 0.6294453038573266 0.0018774091824889183 0.20883413834869863\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([1.0211, 0.9710]) tensor([1.0133, 0.9621]) tensor([0.9618, 0.9198])\n",
      "R[0]\n",
      "tensor([0.0081], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016046365913469345 0.012711586139877908 0.004560551972896064 0.017090566404163836 0.6237306516766549 0.001381988748908043 0.21224977073073387\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9037, 0.8978]) tensor([0.8935, 0.8923]) tensor([0.7719, 0.8330])\n",
      "R[0]\n",
      "tensor([-0.0191], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016560870950110256 0.014183924620985635 0.004637111814634409 0.018059812493156642 0.6213366593718529 0.001601794645190239 0.20812814344465733\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4139, 0.4436]) tensor([0.4106, 0.4231]) tensor([0.2518, 0.3019])\n",
      "R[0]\n",
      "tensor([0.0043], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016511893252376467 0.013506086917754146 0.004454210372176021 0.017619002395775168 0.6178191102147103 0.0015193130671977996 0.2046457232683897\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8484, 0.8351]) tensor([0.8230, 0.8113]) tensor([0.7777, 0.8022])\n",
      "R[0]\n",
      "tensor([0.0004], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01641504818852991 0.011723303121500066 0.00434777409437811 0.016801486879121513 0.6195002580881119 0.0013190745264291762 0.21126595552265645\n",
      "Average (on the epoch) training loss: 0.017201191672636194\n",
      "Episode average V value: 0\n",
      "epoch 10:\n",
      "Learning rate: 3.874204890000002e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9384, 0.8806]) tensor([0.9077, 0.8607]) tensor([0.7427, 0.7584])\n",
      "R[0]\n",
      "tensor([0.0081], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015620908722281456 0.011535757038996963 0.0037988597227486026 0.01702904667519033 0.623139391362667 0.0007247366085648537 0.20944791522622108\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4666, 0.3979]) tensor([0.4523, 0.3834]) tensor([0.4599, 0.4148])\n",
      "R[0]\n",
      "tensor([0.0079], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01567616871651262 0.012918488636336406 0.0037965415297512664 0.017719830183777957 0.6223414922356606 0.000632008783519268 0.20902313235402106\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4927, 0.4489]) tensor([0.4787, 0.4304]) tensor([0.4939, 0.4655])\n",
      "R[0]\n",
      "tensor([0.0013], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015609404461458325 0.011714305617220816 0.004589912701763751 0.017241799901239575 0.620103344142437 0.000691263847053051 0.2097168921083212\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1714, 0.3698]) tensor([0.2146, 0.3615]) tensor([-0.1076,  0.0680])\n",
      "R[0]\n",
      "tensor([-0.0048], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015426323008723557 0.011534101724406356 0.003958919783912279 0.015999974777456374 0.6249294065833092 0.0009126425459980964 0.2106129871159792\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9749, 0.9396]) tensor([0.9548, 0.9335]) tensor([0.9262, 0.9226])\n",
      "R[0]\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015865457099862396 0.013086085664501297 0.003735345506152953 0.017152091161813588 0.6239610943198204 0.0005681929662823677 0.2140246164649725\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7517, 0.7592]) tensor([0.7436, 0.7476]) tensor([0.7437, 0.7140])\n",
      "R[0]\n",
      "tensor([-0.0045], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015701708649285138 0.012621679408890486 0.004048982580017764 0.017181768339127303 0.6291970727443695 0.0007681602463126182 0.21177484737336635\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4774, 0.5770]) tensor([0.4777, 0.5498]) tensor([0.7103, 0.7469])\n",
      "R[0]\n",
      "tensor([-0.0037], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014573375152423977 0.012404876355081796 0.0033633042477686104 0.01585697380360216 0.6314414721727372 0.0007821736708283425 0.22052798493206502\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7469, 0.7845]) tensor([0.7354, 0.7624]) tensor([0.7953, 0.8057])\n",
      "R[0]\n",
      "tensor([-0.0058], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014561420241370797 0.011286609119619243 0.004399954530454124 0.01567400696221739 0.633107208609581 0.000894068330526352 0.22018846519291402\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4365, 0.4113]) tensor([0.4238, 0.3976]) tensor([0.3765, 0.3476])\n",
      "R[0]\n",
      "tensor([0.0016], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014870512927416712 0.012172586716413207 0.0041486549283854405 0.016031581333372742 0.630788615822792 0.0008212634474039078 0.21872336837649345\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3618, 0.3934]) tensor([0.3796, 0.3924]) tensor([0.3887, 0.4060])\n",
      "R[0]\n",
      "tensor([0.0265], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015332447279710323 0.012591424651312992 0.0039836939123724735 0.016513812423683702 0.6293616325855255 0.0008605036437511444 0.22387694795429708\n",
      "Average (on the epoch) training loss: 0.01664008855614811\n",
      "Episode average V value: 0\n",
      "epoch 11:\n",
      "Learning rate: 3.4867844010000016e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4180, 0.3362]) tensor([0.4050, 0.3278]) tensor([0.4323, 0.2424])\n",
      "R[0]\n",
      "tensor([0.0151], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014262225795537233 0.012484287043640506 0.00482571738709521 0.016574598556384445 0.6327699041366577 0.0006916213408112526 0.21936820735037327\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5721, 0.6247]) tensor([0.5628, 0.5988]) tensor([0.1269, 0.2341])\n",
      "R[0]\n",
      "tensor([-0.0033], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014897064411081374 0.01304526689159684 0.004170286875631064 0.01644273873534985 0.6322340327501297 0.0007822454869747162 0.21768603236973286\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5150, 0.5245]) tensor([0.5087, 0.5077]) tensor([0.5001, 0.4983])\n",
      "R[0]\n",
      "tensor([0.0079], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01494089392758906 0.01181368244132318 0.004396701226773075 0.01552382933627814 0.6329159939289093 0.0010044053271412849 0.21882066290080548\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6342, 0.6360]) tensor([0.6349, 0.6303]) tensor([0.5609, 0.5903])\n",
      "R[0]\n",
      "tensor([0.0047], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014712404203601181 0.012068713342247066 0.0033575168356419455 0.016629207122139632 0.627324646115303 0.0009906613156199454 0.21423914155364038\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.0846, 0.0881]) tensor([0.0965, 0.0866]) tensor([0.1274, 0.1615])\n",
      "R[0]\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014070249278564007 0.011637819874755224 0.0043938893462491255 0.015723610074725003 0.6338843108415604 0.001073466882109642 0.21671756123006344\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3159, 0.3210]) tensor([0.3317, 0.3314]) tensor([0.3506, 0.3796])\n",
      "R[0]\n",
      "tensor([0.0179], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014614874927327036 0.012025394961921848 0.004705225573645293 0.016101044578943402 0.6308378585577011 0.0010663725435733796 0.21461070071160793\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.0677, 0.1430]) tensor([0.1164, 0.1765]) tensor([0.2618, 0.3355])\n",
      "R[0]\n",
      "tensor([0.0334], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014254537628963589 0.012725056932802545 0.0036504640074290365 0.016079238456208258 0.6356005687713623 0.0009799353405833245 0.2198556292951107\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1746, 0.1529]) tensor([0.1799, 0.1470]) tensor([0.1690, 0.1877])\n",
      "R[0]\n",
      "tensor([0.0209], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014332347461022436 0.013006830219317636 0.003827646010955505 0.015782128211576492 0.633105448961258 0.001143557347357273 0.2173758733421564\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6730, 0.6964]) tensor([0.6722, 0.6820]) tensor([0.7084, 0.7171])\n",
      "R[0]\n",
      "tensor([0.0032], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014165643606334925 0.013382565339794383 0.003649806334391542 0.01653257963201031 0.6322784070372581 0.001338460937142372 0.21624960163235665\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([1.0026, 0.9249]) tensor([0.9897, 0.9239]) tensor([1.0060, 0.9424])\n",
      "R[0]\n",
      "tensor([-0.0125], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01454722287133336 0.011964100052107823 0.004340787791532421 0.015218877544626594 0.6333152587413788 0.0013543098121881485 0.21647541123628616\n",
      "Average (on the epoch) training loss: 0.016060785224824212\n",
      "Episode average V value: 0\n",
      "epoch 12:\n",
      "Learning rate: 3.138105960900002e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3306, 0.3606]) tensor([0.3284, 0.3423]) tensor([0.2247, 0.2781])\n",
      "R[0]\n",
      "tensor([0.0047], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014728657400701195 0.013851031253885595 0.004932931650975661 0.016661236498970538 0.6323191702365876 0.0013658534362912178 0.21536604806780815\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5057, 0.5303]) tensor([0.5169, 0.5234]) tensor([0.4985, 0.5174])\n",
      "R[0]\n",
      "tensor([0.0080], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015209269892424345 0.013349716249576886 0.0038145874946421826 0.01687895056884736 0.6314477345943451 0.001588935300707817 0.21402245907485484\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4592, 0.3999]) tensor([0.4584, 0.4058]) tensor([0.5424, 0.4995])\n",
      "R[0]\n",
      "tensor([0.0260], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014960024872329086 0.013656609823796316 0.003997914626499551 0.016578454175963996 0.6330353116989136 0.001410788558423519 0.2111464641690254\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7383, 0.7165]) tensor([0.7167, 0.6945]) tensor([0.8391, 0.8178])\n",
      "R[0]\n",
      "tensor([0.0039], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014583378722425551 0.011590379229230166 0.003756987767977989 0.01552166990051046 0.633574669778347 0.0013765188679099082 0.2167078567892313\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([1.0359, 0.9765]) tensor([1.0187, 0.9686]) tensor([1.0382, 0.9723])\n",
      "R[0]\n",
      "tensor([-0.0155], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013901248859707267 0.011668955457345873 0.00397959826262013 0.015670303423888983 0.6357559725046158 0.001310857228934765 0.22208476190268994\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5274, 0.5592]) tensor([0.5294, 0.5459]) tensor([0.6034, 0.6300])\n",
      "R[0]\n",
      "tensor([0.0005], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014309293285012245 0.013013133311105776 0.003964471277125995 0.01611975861666724 0.6342069389820099 0.0017950843498110772 0.2151914265602827\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2964, 0.3590]) tensor([0.3027, 0.3453]) tensor([0.3619, 0.2317])\n",
      "R[0]\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014527985703665763 0.01335955865815049 0.0040506085080323825 0.015612847964745016 0.6331726682186126 0.0019023927226662636 0.21667374281585217\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5600, 0.5745]) tensor([0.5432, 0.5515]) tensor([0.2837, 0.3523])\n",
      "R[0]\n",
      "tensor([-0.0053], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014949633058626205 0.013779803090525093 0.004008104290078336 0.016788178070914 0.6309174350500106 0.0014030668064951898 0.21677437368035316\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4311, 0.3447]) tensor([0.4310, 0.3595]) tensor([0.4592, 0.3693])\n",
      "R[0]\n",
      "tensor([0.0338], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014610773362684995 0.01272154110228439 0.0034475286725573822 0.016300817399984225 0.6327340635061264 0.0010929544493556024 0.22451936338841916\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.0903, 0.1005]) tensor([0.1344, 0.1487]) tensor([0.0300, 0.0963])\n",
      "R[0]\n",
      "tensor([0.0571], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014073914583772421 0.011390389553300339 0.0038534709985906376 0.015047780363354833 0.6374942021369934 0.0013598309010267257 0.22028209008276461\n",
      "Average (on the epoch) training loss: 0.016117999698384664\n",
      "Episode average V value: 0\n",
      "epoch 13:\n",
      "Learning rate: 2.8242953648100018e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0262, -0.5464]) tensor([ 0.1116, -0.3275]) tensor([ 0.1803, -0.4048])\n",
      "R[0]\n",
      "tensor([0.2291], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014023937688209117 0.013060911175387446 0.0037464207998273197 0.017006015583407134 0.6369335741400719 0.001708110898733139 0.21873274111747742\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1102, -0.2162]) tensor([ 0.1564, -0.1021]) tensor([ 0.2003, -0.0536])\n",
      "R[0]\n",
      "tensor([0.1372], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014549918230623007 0.01238621423161385 0.0029185309820386465 0.015711943885311483 0.6324958894252777 0.0013092648684978486 0.21757100336253643\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([1.0170, 0.9587]) tensor([0.9922, 0.9414]) tensor([1.0054, 0.9564])\n",
      "R[0]\n",
      "tensor([0.0149], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015221895017661155 0.01241679251397727 0.003610215046661324 0.01589926543785259 0.6320476324558258 0.001805708460509777 0.2125859814733267\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7907, 0.7600]) tensor([0.7738, 0.7505]) tensor([0.7188, 0.7253])\n",
      "R[0]\n",
      "tensor([0.0035], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014990929756313562 0.013042215630222927 0.003360709750573733 0.016849577703513204 0.6320108112096786 0.001372100330889225 0.21288473723828794\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2286, -0.0295]) tensor([0.2153, 0.0053]) tensor([ 0.2742, -0.0674])\n",
      "R[0]\n",
      "tensor([0.0139], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015609779243357479 0.01432974341388035 0.0042344094718737325 0.01763039802527055 0.6260608913302421 0.001218275472521782 0.20915649841725825\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9084, 0.9044]) tensor([0.9044, 0.9018]) tensor([0.8679, 0.8691])\n",
      "R[0]\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01518282802309841 0.012232600641538739 0.0039334099964926286 0.01656424665544182 0.6258279184699058 0.0008762737810611724 0.20883053755760192\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4850, 0.4771]) tensor([0.4862, 0.4726]) tensor([0.4890, 0.4855])\n",
      "R[0]\n",
      "tensor([0.0066], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014985788986552506 0.012934473638844793 0.003373500600542684 0.016133400041610003 0.6311387314796447 0.001323227472603321 0.21407758313417435\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4809, 0.5641]) tensor([0.4720, 0.5393]) tensor([0.4703, 0.5793])\n",
      "R[0]\n",
      "tensor([-0.0048], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014645011998247356 0.012442793798676575 0.003118923507139698 0.016059864113107322 0.6308864172697067 0.0011318057030439378 0.21451635292172433\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6562, 0.6587]) tensor([0.6341, 0.6349]) tensor([0.6746, 0.6777])\n",
      "R[0]\n",
      "tensor([0.0024], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014606339707970619 0.012301659633711097 0.0034523047053608025 0.015674603493651375 0.6311677088737487 0.0009846653267741204 0.2160673869997263\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9216, 0.8767]) tensor([0.9001, 0.8542]) tensor([0.9330, 0.9142])\n",
      "R[0]\n",
      "tensor([0.0141], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014274146090261638 0.012124371915037045 0.003108718553388826 0.015697531287325546 0.6330276250839233 0.0009961136505007744 0.21791101664304732\n",
      "Average (on the epoch) training loss: 0.016322684622649103\n",
      "Episode average V value: 0\n",
      "epoch 14:\n",
      "Learning rate: 2.5418658283290016e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3122, 0.2531]) tensor([0.3028, 0.2534]) tensor([0.3342, 0.2679])\n",
      "R[0]\n",
      "tensor([-8.2143e-05], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013398310326971113 0.012002515762025724 0.0044807854748960384 0.01568453655997291 0.6366506033539772 0.000937036469578743 0.2185753322839737\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4786, 0.5005]) tensor([0.4823, 0.4973]) tensor([0.5039, 0.5875])\n",
      "R[0]\n",
      "tensor([0.0106], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014485205699689686 0.014017333623894956 0.004009753251459187 0.016703711793292313 0.6277704532146454 0.0008830966353416443 0.21866257938742636\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7888, 0.6712]) tensor([0.7746, 0.6749]) tensor([0.6193, 0.3802])\n",
      "R[0]\n",
      "tensor([0.0004], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014135227663442492 0.012283511424524477 0.003118257593803719 0.01572995077027008 0.6335972373485566 0.001144607901573181 0.21625850966572763\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3458, 0.2608]) tensor([0.3334, 0.2571]) tensor([0.3546, 0.1905])\n",
      "R[0]\n",
      "tensor([0.0207], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014531661618035287 0.01182131770554406 0.00455410614361972 0.015787183742970227 0.6313116941452026 0.0011804627180099488 0.21426286670565606\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.0175, 0.2285]) tensor([0.0707, 0.2469]) tensor([0.1053, 0.2889])\n",
      "R[0]\n",
      "tensor([0.0104], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013948319805786013 0.011966847351191972 0.004147300550286673 0.015252153075765818 0.6357970143556595 0.001284956932067871 0.2169745134115219\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9411, 0.8506]) tensor([0.9250, 0.8510]) tensor([0.9458, 0.8569])\n",
      "R[0]\n",
      "tensor([-0.0060], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01423758933180943 0.012478709584356693 0.0034607648234723457 0.015503285930491984 0.6370528348684311 0.0013689892888069154 0.2193304594606161\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1320, 0.3025]) tensor([0.1506, 0.2818]) tensor([0.1682, 0.2986])\n",
      "R[0]\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01419730086158961 0.012151842757928534 0.004143554124793809 0.016145605061668903 0.6322279496192932 0.0014049562364816667 0.21879174184799194\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6563, 0.4367]) tensor([0.6251, 0.4367]) tensor([0.4897, 0.4412])\n",
      "R[0]\n",
      "tensor([0.0283], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015069780516438187 0.012916816132426902 0.0036112504172488118 0.01569136780872941 0.6294050475358963 0.00148759738355875 0.21862128928303717\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6529, 0.7331]) tensor([0.6576, 0.7207]) tensor([0.6475, 0.7300])\n",
      "R[0]\n",
      "tensor([-0.0064], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013748400974087418 0.011413880350010003 0.004129132469768592 0.014705657611135393 0.6366725256443023 0.0016143882721662522 0.22216187450289726\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5009, 0.5571]) tensor([0.5054, 0.5451]) tensor([0.6153, 0.6493])\n",
      "R[0]\n",
      "tensor([-0.0078], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014166514338925481 0.01107162070342747 0.0035386351425622706 0.01580896791163832 0.6325009396672249 0.0013881048262119294 0.21976616211235522\n",
      "Average (on the epoch) training loss: 0.015701242026593537\n",
      "Episode average V value: 0\n",
      "epoch 15:\n",
      "Learning rate: 2.2876792454961016e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7548, 0.7026]) tensor([0.7401, 0.6974]) tensor([0.8754, 0.7817])\n",
      "R[0]\n",
      "tensor([-0.0033], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01369666236359626 0.012597231261606794 0.003951498488810103 0.01542572309449315 0.6398187952041626 0.001499851755797863 0.2238489276021719\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1615, -0.2915]) tensor([ 0.1477, -0.2256]) tensor([ 0.2321, -0.3424])\n",
      "R[0]\n",
      "tensor([0.1095], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013753892054315656 0.0129727865812456 0.0038959901477992388 0.01585619029100053 0.6389501042366028 0.0015285902470350266 0.22147155199944973\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4763, 0.4627]) tensor([0.4782, 0.4620]) tensor([0.3098, 0.0295])\n",
      "R[0]\n",
      "tensor([0.0162], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014320360291749238 0.012501711177596008 0.0034907398147879577 0.015350609487388284 0.6337655267715454 0.0013865277096629142 0.2164164870083332\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3327, -0.0218]) tensor([-0.2427,  0.0045]) tensor([-0.3300, -0.2649])\n",
      "R[0]\n",
      "tensor([-0.0132], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01481017626170069 0.013754574106438667 0.004340954966402933 0.016258150345878675 0.6299022496342659 0.0013767585456371308 0.21344734063744544\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8767, 0.8277]) tensor([0.8633, 0.8198]) tensor([0.8569, 0.8190])\n",
      "R[0]\n",
      "tensor([0.0039], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01464307749364525 0.012483318968093954 0.0036999743728883916 0.015188960081432015 0.6344303959012032 0.0013619962111115456 0.21897151026129721\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8709, 0.8639]) tensor([0.8492, 0.8478]) tensor([0.6247, 0.6754])\n",
      "R[0]\n",
      "tensor([0.0015], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013977255506440998 0.012514886743701936 0.0029136640993838226 0.014831036477116867 0.6391187413930893 0.0018441154286265374 0.21607363256812095\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1325, 0.2535]) tensor([0.1461, 0.2357]) tensor([0.1950, 0.2332])\n",
      "R[0]\n",
      "tensor([0.0025], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01471237816940993 0.012527656548205413 0.004398104087293177 0.015986632833257318 0.629740022957325 0.0014899869412183762 0.21610002101957798\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9862, 0.9024]) tensor([0.9612, 0.8927]) tensor([0.9691, 0.8954])\n",
      "R[0]\n",
      "tensor([0.0069], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013883985216729343 0.011621371390319837 0.0036828063366865535 0.015709066696697846 0.6366194853186607 0.0015181240290403366 0.21550962147116662\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0554,  0.1497]) tensor([-0.0201,  0.1377]) tensor([0.1282, 0.3343])\n",
      "R[0]\n",
      "tensor([-0.0006], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014128566818311811 0.012686596515544806 0.0027950560900171697 0.015310814703349024 0.6355660645961762 0.0016991549506783486 0.21463688932359218\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1972, 0.0568]) tensor([0.1944, 0.0822]) tensor([0.2306, 0.1166])\n",
      "R[0]\n",
      "tensor([0.0035], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014658029909711332 0.011965953938124585 0.003940420900160461 0.015336139860097318 0.6341011474132537 0.0016773393452167511 0.21333914232254028\n",
      "Average (on the epoch) training loss: 0.015525332387071104\n",
      "Episode average V value: 0\n",
      "epoch 16:\n",
      "Learning rate: 2.0589113209464913e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8218, 0.8455]) tensor([0.8133, 0.8335]) tensor([0.8709, 0.8830])\n",
      "R[0]\n",
      "tensor([0.0073], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014961523212492465 0.013268760910985293 0.003760749058345027 0.016213512759190054 0.6295388470888138 0.0014177080243825912 0.21241477705538273\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5294, 0.5170]) tensor([0.5118, 0.5043]) tensor([0.6069, 0.6077])\n",
      "R[0]\n",
      "tensor([0.0012], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014874479457736015 0.01328445564408321 0.0037596051926084327 0.016268227362073957 0.6294028269052505 0.001135942243039608 0.21266392610967158\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7297, 0.7591]) tensor([0.7103, 0.7405]) tensor([0.7595, 0.7791])\n",
      "R[0]\n",
      "tensor([0.0006], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014936368264257907 0.013461249674248393 0.0042695684509162675 0.016521754009649156 0.631528345823288 0.0013564270436763764 0.21057108315825462\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8183, 0.7676]) tensor([0.7919, 0.7536]) tensor([0.8879, 0.8279])\n",
      "R[0]\n",
      "tensor([0.0018], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015239426724147052 0.012725951784392236 0.003757379428077911 0.015693950129207223 0.6316007080674172 0.0014658277630805968 0.20868276311457157\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3594, -0.2252]) tensor([-0.2746, -0.1763]) tensor([-0.3094, -0.0796])\n",
      "R[0]\n",
      "tensor([0.0405], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014328993528615684 0.012198042471492953 0.003465764727723581 0.015749982288107277 0.6350562816858292 0.0017349426150321961 0.20726888613402844\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9392, 0.9285]) tensor([0.9261, 0.9157]) tensor([0.8509, 0.8579])\n",
      "R[0]\n",
      "tensor([0.0140], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0149179528336972 0.013837635303949354 0.0037129863006575762 0.016643888399470598 0.6322656403779984 0.0015922208726406098 0.21085049718618393\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0509, -0.1005]) tensor([ 0.0054, -0.0086]) tensor([-0.0583, -0.1682])\n",
      "R[0]\n",
      "tensor([0.1090], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014122243280988187 0.011398464424855774 0.003665346974690692 0.014722404931206256 0.6383397020101548 0.0015170172080397606 0.2140158331543207\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2644, 0.2243]) tensor([0.2569, 0.2278]) tensor([0.3490, 0.2176])\n",
      "R[0]\n",
      "tensor([0.0012], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014721793576143682 0.012878976551772212 0.0032143385939507425 0.016224270003614948 0.632835270524025 0.0017674102038145066 0.21278844030201435\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6924, 0.6970]) tensor([0.6869, 0.6833]) tensor([0.6571, 0.6856])\n",
      "R[0]\n",
      "tensor([0.0066], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014500934186391532 0.01297669632700854 0.0028092711332556065 0.015531524850986899 0.6315304954648018 0.0013020136207342148 0.2105706363618374\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1276, 0.2652]) tensor([0.1715, 0.2765]) tensor([-0.1341, -0.0601])\n",
      "R[0]\n",
      "tensor([0.0562], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01421960394270718 0.011536630757334933 0.0032948248594511825 0.015391048457473517 0.6357197992801666 0.00162129957228899 0.2095363868623972\n",
      "Average (on the epoch) training loss: 0.01589605631909799\n",
      "Episode average V value: 0\n",
      "epoch 17:\n",
      "Learning rate: 1.8530201888518422e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3470, 0.2940]) tensor([0.3487, 0.3075]) tensor([0.2478, 0.0544])\n",
      "R[0]\n",
      "tensor([0.0320], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01509045921638608 0.011954066097525355 0.0038071420833202863 0.015478071350138635 0.6310588955879212 0.0015943510234355926 0.208065113350749\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7726, 0.7799]) tensor([0.7557, 0.7627]) tensor([0.7540, 0.7803])\n",
      "R[0]\n",
      "tensor([0.0009], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014630714364349841 0.012705366023918032 0.003985522608405518 0.015742928041610867 0.6357925221920013 0.0018356298431754113 0.20939199163019656\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5316, -0.3177]) tensor([-0.4290, -0.2359]) tensor([-0.4221, -0.2312])\n",
      "R[0]\n",
      "tensor([-0.0034], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015116349355317651 0.01271869677642826 0.0028192726478737315 0.016289102726615965 0.6294796072244644 0.001450016252696514 0.201956044703722\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7303, 0.8065]) tensor([0.7177, 0.7836]) tensor([0.6408, 0.7449])\n",
      "R[0]\n",
      "tensor([0.0026], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014695958136580884 0.01297803412770736 0.0030039061203315214 0.016409797680098565 0.6313687375187874 0.001637332797050476 0.20614245407283305\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6440, 0.6686]) tensor([0.6281, 0.6487]) tensor([0.7147, 0.7154])\n",
      "R[0]\n",
      "tensor([9.7752e-06], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015136921028606594 0.012551987952159834 0.004428439351249836 0.016580701666884124 0.6291612027287483 0.0017815338745713234 0.2055118595659733\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5785, 0.5473]) tensor([0.5702, 0.5448]) tensor([0.6852, 0.6642])\n",
      "R[0]\n",
      "tensor([0.0024], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015116260899230839 0.012426967684645206 0.004242644894749901 0.016271305699367074 0.6286314687728882 0.0012996191382408143 0.20796711125969886\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5328, 0.4633]) tensor([0.5085, 0.4522]) tensor([0.5232, 0.4524])\n",
      "R[0]\n",
      "tensor([0.0007], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014891152110882103 0.0125740783903384 0.0028115877564960103 0.01593087899219245 0.6281432981491089 0.001166119858622551 0.20370728862285614\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([1.0025, 0.9849]) tensor([0.9914, 0.9800]) tensor([1.0025, 0.9849])\n",
      "R[0]\n",
      "tensor([-0.0169], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015344222414307296 0.01391146594689053 0.00336464981399331 0.01682460639020428 0.6283809781074524 0.0014704195931553841 0.20580184115469455\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8099, 0.8359]) tensor([0.8047, 0.8281]) tensor([0.8491, 0.8440])\n",
      "R[0]\n",
      "tensor([-0.0093], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.015026100436225534 0.012675486158506829 0.003604192782606333 0.01642893771082163 0.630866378068924 0.0018308170586824417 0.20071768568456172\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4653, 0.3962]) tensor([0.4468, 0.3870]) tensor([0.4704, 0.3739])\n",
      "R[0]\n",
      "tensor([-0.0006], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01548499572183937 0.013355304722397704 0.004134890932096823 0.01717973597999662 0.6269455730319023 0.0018355936482548714 0.20083854195475578\n",
      "Average (on the epoch) training loss: 0.016313606623793022\n",
      "Episode average V value: 0\n",
      "epoch 18:\n",
      "Learning rate: 1.667718169966658e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8530, 0.8417]) tensor([0.8349, 0.8252]) tensor([0.8304, 0.8251])\n",
      "R[0]\n",
      "tensor([0.0066], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014999310670420528 0.012876959479108337 0.003259138803010501 0.016343696902506054 0.6283231112957001 0.0015539903789758682 0.20580589154362677\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7794, 0.7604]) tensor([0.7603, 0.7399]) tensor([0.6408, 0.6438])\n",
      "R[0]\n",
      "tensor([0.0105], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014939651838969439 0.01277707114128134 0.0031412530498510023 0.016055712918750943 0.6306417723298072 0.0014953427463769912 0.20883843100070953\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8873, 0.8688]) tensor([0.8733, 0.8574]) tensor([0.7800, 0.7792])\n",
      "R[0]\n",
      "tensor([-0.0200], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014105871208943426 0.012273388568370137 0.003166887942546964 0.01673220416600816 0.6353481290340424 0.0017172536104917526 0.20787716411054136\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2078, 0.3277]) tensor([0.2305, 0.3320]) tensor([0.2427, 0.3385])\n",
      "R[0]\n",
      "tensor([0.0078], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014865515114739537 0.012559069146722323 0.0031467508630657905 0.01582186139980331 0.6317012395262718 0.0017973245829343795 0.20983597606420518\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9489, 0.9545]) tensor([0.9447, 0.9434]) tensor([0.9463, 0.9291])\n",
      "R[0]\n",
      "tensor([0.0004], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014337201579008252 0.012422401138988789 0.004295047221254208 0.016048116849269718 0.6335600080490112 0.0015704187527298928 0.2096650043874979\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2981, 0.2106]) tensor([0.2983, 0.2348]) tensor([0.2118, 0.1637])\n",
      "R[0]\n",
      "tensor([0.0508], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014665354078635574 0.013454330026033858 0.0036487503010393992 0.015877358516678215 0.634431037068367 0.0014087555333971978 0.20745253343880177\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1486,  0.0620]) tensor([-0.1044,  0.0671]) tensor([0.0571, 0.2333])\n",
      "R[0]\n",
      "tensor([-0.0048], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014268174199853092 0.012381353462791594 0.003345901436157874 0.01541599802626297 0.6368033854961396 0.001438634864985943 0.20518877345323563\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4301, 0.5522]) tensor([0.4398, 0.5392]) tensor([0.6042, 0.6910])\n",
      "R[0]\n",
      "tensor([-0.0047], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013927501569502055 0.010913001217057171 0.0030872086448453045 0.014988156397361308 0.6340868878364563 0.0016351403295993805 0.2075116442888975\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5725, 0.6606]) tensor([0.5749, 0.6469]) tensor([0.5680, 0.6193])\n",
      "R[0]\n",
      "tensor([-0.0079], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014271620534360409 0.011312072983586404 0.003555159568812087 0.015750228633638473 0.6333582445383071 0.0014659847691655158 0.20401848170161246\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3471, 0.5510]) tensor([0.3502, 0.5231]) tensor([0.4402, 0.6042])\n",
      "R[0]\n",
      "tensor([-0.0040], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01447335511026904 0.011851281832787209 0.003552226197982236 0.015490452417638153 0.6350169415473939 0.0018828843608498574 0.20686891043186187\n",
      "Average (on the epoch) training loss: 0.015852378622791728\n",
      "Episode average V value: 0\n",
      "epoch 19:\n",
      "Learning rate: 1.5009463529699922e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5548, 0.5255]) tensor([0.5525, 0.5251]) tensor([0.5183, 0.4628])\n",
      "R[0]\n",
      "tensor([0.0055], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013835825404617936 0.012567950649870908 0.003480311128220819 0.01498961861524731 0.6378995316028595 0.0017252931445837021 0.21373987555503846\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7263, 0.7058]) tensor([0.7074, 0.6950]) tensor([0.8315, 0.7869])\n",
      "R[0]\n",
      "tensor([0.0020], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014246781543362886 0.012034088657004758 0.0033511955734284128 0.015516166689340026 0.6367879685163498 0.0017702913209795951 0.20890145154297352\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6996, 0.5708]) tensor([0.6695, 0.5626]) tensor([0.5721, 0.5126])\n",
      "R[0]\n",
      "tensor([0.0005], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014651262856088578 0.013940025873715059 0.0034183887936651445 0.015957849073689433 0.6321949914097786 0.0016785891130566596 0.20454793873429297\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5971, 0.6623]) tensor([0.5838, 0.6415]) tensor([0.6608, 0.7382])\n",
      "R[0]\n",
      "tensor([0.0024], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014729258424602449 0.012463985898255488 0.003417391426051836 0.015554937495384365 0.6323333123922348 0.0016928941458463668 0.2076909022629261\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7399, 0.6665]) tensor([0.7104, 0.6519]) tensor([0.7324, 0.6599])\n",
      "R[0]\n",
      "tensor([-0.0024], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014079192943871022 0.01213003883426427 0.0038912073214378323 0.015922886980464682 0.6365098286867141 0.0016990160420536995 0.21217776930332183\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1099, 0.2125]) tensor([0.1201, 0.2008]) tensor([-0.0729,  0.0145])\n",
      "R[0]\n",
      "tensor([0.0033], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014070182042662055 0.011662918378220638 0.002223982567598796 0.015469833903945983 0.6358545522689819 0.001899144284427166 0.20758440163731576\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8607, 0.7632]) tensor([0.8433, 0.7636]) tensor([0.7896, 0.7166])\n",
      "R[0]\n",
      "tensor([-0.0064], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014348091821186244 0.011610217249202833 0.0029499372333466455 0.015801151451654734 0.6368057968616485 0.0017588867470622062 0.2028440248966217\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5277, 0.3929]) tensor([0.5005, 0.3872]) tensor([0.5148, 0.4386])\n",
      "R[0]\n",
      "tensor([0.0156], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014588828668929637 0.012332018852233886 0.0027083377749095236 0.016086477543693037 0.6344443303346634 0.001731370136141777 0.20656753626465799\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4868, 0.4881]) tensor([0.4692, 0.4728]) tensor([0.5094, 0.5145])\n",
      "R[0]\n",
      "tensor([-0.0020], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01492659038770944 0.013705554027081235 0.003563402806114027 0.016700239442288874 0.6308285059928894 0.0018105312883853913 0.20332392770051957\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2474, 0.1950]) tensor([0.2381, 0.1981]) tensor([0.2647, 0.2424])\n",
      "R[0]\n",
      "tensor([-0.0005], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014089717210270464 0.013065810221924039 0.00340663636913996 0.016023682041093706 0.6361585130691528 0.0018585675060749055 0.20613998217880725\n",
      "Average (on the epoch) training loss: 0.015802284323680214\n",
      "Episode average V value: 0\n",
      "epoch 20:\n",
      "Learning rate: 1.350851717672993e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1217, 0.1574]) tensor([0.1257, 0.1516]) tensor([-0.0348,  0.0050])\n",
      "R[0]\n",
      "tensor([0.0139], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01413204526156187 0.012774906764781917 0.0045021228231235 0.01569440131634474 0.6340397011637687 0.001970743767917156 0.2072145841419697\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6721, 0.6217]) tensor([0.6626, 0.6205]) tensor([0.5136, 0.5595])\n",
      "R[0]\n",
      "tensor([0.0164], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013896104381419718 0.012002931477079983 0.003951289970147627 0.016099138972349464 0.6355040049552918 0.002112909860908985 0.20610730496048926\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0352,  0.0363]) tensor([-0.0182,  0.0465]) tensor([0.0751, 0.2003])\n",
      "R[0]\n",
      "tensor([-0.0040], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014145345710217952 0.013570316372031812 0.003316560889941684 0.015970720020588487 0.6383584640622139 0.002036342702805996 0.2079183354228735\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9940, 0.9483]) tensor([0.9804, 0.9452]) tensor([1.0071, 0.9749])\n",
      "R[0]\n",
      "tensor([-0.0154], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014436097403988242 0.013835112549568294 0.003100677079757588 0.016312435059808193 0.6349475569128991 0.002044987089931965 0.2040686569660902\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 1.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4713, -0.8011]) tensor([-0.3105, -0.5191]) tensor([-0.5173, -0.7270])\n",
      "R[0]\n",
      "tensor([0.3164], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01375087125133723 0.011880346487392671 0.0037664417542964657 0.015159152594627813 0.6379874385595322 0.0019806735143065453 0.20758234605193138\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8450, 0.8168]) tensor([0.8200, 0.8021]) tensor([0.8403, 0.8299])\n",
      "R[0]\n",
      "tensor([0.0009], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013317630192730575 0.01149998291261727 0.003325032242146335 0.014654084116686136 0.6399719579815865 0.0017174727842211724 0.20908671934902667\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1942, 0.3359]) tensor([0.2182, 0.3425]) tensor([0.2678, 0.3751])\n",
      "R[0]\n",
      "tensor([0.0087], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013458205160684883 0.012384603190497728 0.0028652179483906366 0.01516204968164675 0.6402529886960984 0.001872214674949646 0.20749182422459125\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1380, 0.1983]) tensor([0.1417, 0.1868]) tensor([-0.1944, -0.0827])\n",
      "R[0]\n",
      "tensor([0.0086], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01394383531343192 0.013206737033775425 0.0032466269680226107 0.015282292549032718 0.6373230177164078 0.0016094305217266083 0.20531062188744545\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2953, 0.4124]) tensor([0.2954, 0.3931]) tensor([0.2308, 0.3840])\n",
      "R[0]\n",
      "tensor([-1.3635e-06], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013714991683140398 0.013926680976946954 0.003092568144473262 0.01592984746489674 0.6369983547925949 0.0014741855189204216 0.20757800537347793\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8509, 0.8710]) tensor([0.8423, 0.8640]) tensor([0.8536, 0.8734])\n",
      "R[0]\n",
      "tensor([-0.0113], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013771314433775841 0.01165485037862527 0.003016287555370582 0.014767661818768828 0.6384008806943894 0.0016109342724084854 0.2055043739527464\n",
      "Average (on the epoch) training loss: 0.015503178359474987\n",
      "Episode average V value: 0\n",
      "epoch 21:\n",
      "Learning rate: 1.2157665459056937e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2388, 0.1679]) tensor([0.2273, 0.1696]) tensor([0.2331, 0.2253])\n",
      "R[0]\n",
      "tensor([0.0203], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013686711899470539 0.012397361736446328 0.0028189116117664525 0.015458682556403801 0.6381903882622719 0.001572417639195919 0.21068260112404824\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([1.0079, 0.9576]) tensor([0.9883, 0.9427]) tensor([1.0074, 0.9568])\n",
      "R[0]\n",
      "tensor([0.0118], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0133178818305023 0.011960665250662715 0.003495566281668289 0.015498514024773613 0.6406852722167968 0.0015923005267977715 0.20954609642922878\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1261, -0.1174]) tensor([-0.0989, -0.0790]) tensor([-0.1336, -0.1375])\n",
      "R[0]\n",
      "tensor([-0.0018], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013242524241097271 0.013506846123491414 0.003316933964055352 0.01523659901227802 0.6418404309749604 0.0015465490892529487 0.21111066788434982\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5781, 0.6167]) tensor([0.5634, 0.5984]) tensor([0.7367, 0.7709])\n",
      "R[0]\n",
      "tensor([0.0074], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01321680289460346 0.011709817411188851 0.0029196978353193117 0.014272241037804634 0.6413511723279953 0.0015428876504302025 0.21019361051917076\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6232, 0.7233]) tensor([0.6153, 0.7029]) tensor([0.6363, 0.7318])\n",
      "R[0]\n",
      "tensor([0.0004], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013297388844192027 0.01131278977562033 0.0032081609001888864 0.014561877106316387 0.6412945178747177 0.0012756441980600356 0.2134156429320574\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8463, 0.8726]) tensor([0.8233, 0.8504]) tensor([0.6255, 0.7384])\n",
      "R[0]\n",
      "tensor([0.0026], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01301426593074575 0.012010277025343385 0.0034160182293871914 0.014145114954095334 0.647134697318077 0.001755508564412594 0.21058291098475457\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.0667, 0.2845]) tensor([0.0807, 0.2650]) tensor([0.2176, 0.4045])\n",
      "R[0]\n",
      "tensor([-0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013709051377139986 0.012431623613796546 0.0032112683869290775 0.015040244934149086 0.6395673550963402 0.0016172367334365844 0.21141489247977735\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([1.0104, 0.9857]) tensor([1.0023, 0.9755]) tensor([0.9939, 0.9832])\n",
      "R[0]\n",
      "tensor([0.0075], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013842036636080593 0.01222088179581624 0.0026299456534616184 0.01448763519525528 0.6407346706390381 0.0018377019166946412 0.20664548850059508\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5396, 0.3649]) tensor([0.5276, 0.3896]) tensor([0.5138, 0.4402])\n",
      "R[0]\n",
      "tensor([0.0250], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01377930048201233 0.013056726109833107 0.0036563367371054483 0.015065069875214249 0.6377173717021942 0.0018705765381455421 0.20928521366417407\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3365, 0.4897]) tensor([0.3357, 0.4702]) tensor([0.1597, 0.2926])\n",
      "R[0]\n",
      "tensor([-0.0046], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013476414882112294 0.01160342169640353 0.003019601130250521 0.014193739644251763 0.6418664674162865 0.0017582210972905158 0.21123876091837884\n",
      "Average (on the epoch) training loss: 0.014795971834054216\n",
      "Episode average V value: 0\n",
      "epoch 22:\n",
      "Learning rate: 1.0941898913151244e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5694, 0.5879]) tensor([0.5518, 0.5711]) tensor([0.5542, 0.5820])\n",
      "R[0]\n",
      "tensor([-0.0026], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013354573416523635 0.012131101188540924 0.004008184757116396 0.015173086945666001 0.6423766050338745 0.001436386026442051 0.20826283486187458\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7287, 0.7102]) tensor([0.7222, 0.7089]) tensor([0.7278, 0.7081])\n",
      "R[0]\n",
      "tensor([-0.0055], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013403619422577322 0.012184743753939983 0.003452713441765809 0.015090132388519124 0.6403912156820297 0.001458385907113552 0.21051457172632218\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5956, 0.6649]) tensor([0.5813, 0.6433]) tensor([0.5490, 0.6309])\n",
      "R[0]\n",
      "tensor([0.0064], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013507815987337381 0.014080917332532408 0.003198162194179531 0.014817661626497284 0.6406545000076294 0.0014249472320079803 0.20668171074986458\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7520, 0.7201]) tensor([0.7276, 0.7049]) tensor([0.7416, 0.7170])\n",
      "R[0]\n",
      "tensor([-0.0028], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013931060958653688 0.01163961361003021 0.003028456906509746 0.014628938560374082 0.6351378376483917 0.0012565596848726273 0.20641207844018936\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4262, 0.3849]) tensor([0.4264, 0.3997]) tensor([0.5467, 0.3706])\n",
      "R[0]\n",
      "tensor([0.0233], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013200270236004143 0.011429074391970061 0.004007457087174771 0.0144746408383362 0.6438392502069473 0.0015035283043980598 0.21341004697978497\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1850, 0.3518]) tensor([0.1867, 0.3312]) tensor([0.1140, 0.2961])\n",
      "R[0]\n",
      "tensor([0.0004], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01309238722268492 0.011013826157344739 0.003540247011538668 0.014324666008818895 0.6429969815015792 0.0016096884235739708 0.21267236296832562\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0767,  0.0421]) tensor([-0.0551,  0.0550]) tensor([-0.1850, -0.0639])\n",
      "R[0]\n",
      "tensor([-0.0028], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013509076843038202 0.011747522069024853 0.0026736590078817245 0.014660987727576867 0.6437687095403671 0.0015198744460940361 0.21404649344086646\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4301, 0.3668]) tensor([0.4301, 0.3823]) tensor([0.4944, 0.4445])\n",
      "R[0]\n",
      "tensor([0.0242], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013415878338273615 0.012196839000855107 0.0034406890499885778 0.014536600003018976 0.6429196509122849 0.001443242758512497 0.20875666907429696\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5489, 0.5652]) tensor([0.5460, 0.5619]) tensor([0.4719, 0.3936])\n",
      "R[0]\n",
      "tensor([0.0138], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013354874589014798 0.01090002203395852 0.00399867876297867 0.014066518415929749 0.6398608989715576 0.0013699018508195877 0.20775668470561504\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2596, 0.0999]) tensor([0.2703, 0.1477]) tensor([0.2716, 0.1015])\n",
      "R[0]\n",
      "tensor([0.0636], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01388444077828899 0.012380537206532607 0.0034627682900973014 0.01539474849239923 0.6354209147691726 0.0012554243206977845 0.20870054937899113\n",
      "Average (on the epoch) training loss: 0.01471679810071364\n",
      "Episode average V value: 0\n",
      "epoch 23:\n",
      "Learning rate: 9.84770902183612e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0581, -0.1376]) tensor([ 0.0556, -0.0888]) tensor([0.1051, 0.1200])\n",
      "R[0]\n",
      "tensor([0.0033], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013298733818344772 0.011868981441075448 0.0033039524743471702 0.015452567532658577 0.6394016124606132 0.0009517104551196098 0.21560908633470535\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5255, 0.5320]) tensor([0.5091, 0.5210]) tensor([0.5461, 0.5554])\n",
      "R[0]\n",
      "tensor([0.0029], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013742207414936274 0.011579756680119317 0.0031184994249870215 0.013850781752262264 0.6369107233285904 0.0011206974983215333 0.21184149059653282\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7685, 0.7765]) tensor([0.7637, 0.7701]) tensor([0.5895, 0.6397])\n",
      "R[0]\n",
      "tensor([0.0037], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013187986514065414 0.012420746927411528 0.0025694053194920343 0.015307088181376457 0.6416697942018509 0.001360094927251339 0.21307792119681834\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7747, 0.7547]) tensor([0.7498, 0.7334]) tensor([0.7832, 0.7567])\n",
      "R[0]\n",
      "tensor([0.0117], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01387993874307722 0.013398136828516727 0.0032121720046616245 0.015190626071766019 0.6391467493772507 0.001346930779516697 0.20916131198406218\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6095, 0.7400]) tensor([0.6158, 0.7238]) tensor([0.5833, 0.7122])\n",
      "R[0]\n",
      "tensor([-0.0074], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013289939508773387 0.011656537785267573 0.003377786363751511 0.014800206418847665 0.6380882025957108 0.0012978067621588706 0.21159185813367368\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1087,  0.0730]) tensor([-0.0286,  0.1243]) tensor([-0.2943, -0.1154])\n",
      "R[0]\n",
      "tensor([0.1019], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01366083134431392 0.013665487987469532 0.0025719731870358373 0.016155147588346155 0.6386345468759537 0.00135101880133152 0.2045021670460701\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5654, 0.7369]) tensor([0.5594, 0.7135]) tensor([0.7092, 0.8217])\n",
      "R[0]\n",
      "tensor([0.0039], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012945275941863656 0.012671730344358366 0.003767856058855614 0.015881392821669577 0.6399893521666526 0.0010716432854533196 0.2092350817024708\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6935, 0.7026]) tensor([0.6729, 0.6843]) tensor([0.6209, 0.5715])\n",
      "R[0]\n",
      "tensor([-0.0027], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013750840773340314 0.012597612182886223 0.0026771920071369097 0.015287108198506757 0.6356417974829673 0.0010315904319286345 0.2108114828467369\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6945, 0.6641]) tensor([0.6867, 0.6619]) tensor([0.6941, 0.6965])\n",
      "R[0]\n",
      "tensor([0.0135], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013304911109153182 0.010240491637778177 0.003471254280751964 0.013881870199926198 0.64056858253479 0.0011345982179045678 0.21199808333814144\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6823, 0.7558]) tensor([0.6808, 0.7429]) tensor([0.5459, 0.5952])\n",
      "R[0]\n",
      "tensor([-0.0032], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013197443041950464 0.012751547194144223 0.0024988750579304905 0.015753467056900262 0.6379862035512924 0.0009998548477888108 0.20523443534970284\n",
      "Average (on the epoch) training loss: 0.015156025582225993\n",
      "Episode average V value: 0\n",
      "epoch 24:\n",
      "Learning rate: 8.862938119652508e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9412, 0.9116]) tensor([0.9185, 0.8922]) tensor([0.8161, 0.7823])\n",
      "R[0]\n",
      "tensor([0.0147], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013476131534203887 0.012768312212516321 0.003364896242072064 0.015253752022515983 0.637455983042717 0.0008264574259519577 0.20919353938102722\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4753, 0.4353]) tensor([0.4679, 0.4437]) tensor([0.3308, 0.3064])\n",
      "R[0]\n",
      "tensor([0.0307], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013435852704569698 0.01275896765338257 0.0027626764396300133 0.014912269799504429 0.6398955118656159 0.0008150569349527359 0.2103192365169525\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.0405, 0.3440]) tensor([0.0846, 0.3487]) tensor([0.3072, 0.5436])\n",
      "R[0]\n",
      "tensor([-0.0041], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013287753594573588 0.013269815491716144 0.003706567667397394 0.015422638276591898 0.6373476017713546 0.0008262356668710709 0.20670661616325378\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5089, -0.3810]) tensor([-0.4241, -0.2878]) tensor([-0.6306, -0.4263])\n",
      "R[0]\n",
      "tensor([-0.0003], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013400996425189078 0.0125403885091946 0.0027338614700183825 0.014651557668345049 0.6397722636461258 0.0007459517791867256 0.2082264755219221\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5332, 0.6662]) tensor([0.5368, 0.6506]) tensor([0.5274, 0.6533])\n",
      "R[0]\n",
      "tensor([-0.0066], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013539429302327335 0.012991001656992011 0.0031658654040365945 0.015303862237371504 0.6348611701726914 0.0007997962236404419 0.20477462570369243\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9055, 0.9020]) tensor([0.8937, 0.8899]) tensor([0.9201, 0.9104])\n",
      "R[0]\n",
      "tensor([0.0061], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013894355127587915 0.012386075551985414 0.003030938387866627 0.015661119135096668 0.6354362396001816 0.0008367556184530258 0.20856061251461505\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7566, 0.7600]) tensor([0.7475, 0.7510]) tensor([0.7367, 0.7164])\n",
      "R[0]\n",
      "tensor([0.0032], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012998495016247034 0.011984838414486148 0.00337478138404731 0.014345903030363844 0.6428300983905793 0.0008869599103927613 0.2072093462795019\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2965, 0.3874]) tensor([0.2914, 0.3752]) tensor([0.2101, 0.3223])\n",
      "R[0]\n",
      "tensor([-0.0011], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01300692632701248 0.011892518156426377 0.003621414005116094 0.01530354955419898 0.6402403941750526 0.0009148338735103607 0.2066884625554085\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7852, 0.7565]) tensor([0.7734, 0.7548]) tensor([0.7489, 0.7628])\n",
      "R[0]\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01270618347870186 0.011732032722240547 0.0031099669878058194 0.014590050995117053 0.6408304889202118 0.0011773429214954375 0.20616987910866738\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6447, 0.7572]) tensor([0.6343, 0.7378]) tensor([0.5942, 0.7365])\n",
      "R[0]\n",
      "tensor([0.0010], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01303993023559451 0.012254463372999453 0.003508726401207241 0.01477999303280376 0.6387409069538117 0.0010644441470503807 0.20859734024107457\n",
      "Average (on the epoch) training loss: 0.015022469575190916\n",
      "Episode average V value: 0\n",
      "epoch 25:\n",
      "Learning rate: 7.976644307687257e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0428, -0.0750]) tensor([-0.0294, -0.0537]) tensor([ 0.0413, -0.0161])\n",
      "R[0]\n",
      "tensor([0.0426], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012784922391176224 0.01097021554870298 0.0034009687011002823 0.01441718903905712 0.6384305952191353 0.0011905659213662149 0.20739193162322045\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5276, 0.5472]) tensor([0.5088, 0.5335]) tensor([0.6859, 0.6556])\n",
      "R[0]\n",
      "tensor([-2.4065e-05], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013171874944120646 0.01293557665834669 0.003548159303201828 0.015064495193073527 0.6423219495415687 0.0011818718686699868 0.20725048092007636\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8081, 0.7423]) tensor([0.7815, 0.7265]) tensor([0.5810, 0.5613])\n",
      "R[0]\n",
      "tensor([0.0147], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0129410669519566 0.012006856357416837 0.0031601774690188903 0.014884222737280652 0.6384494262933731 0.001092169240117073 0.20490961085259915\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5780, 0.7059]) tensor([0.5672, 0.6840]) tensor([0.4827, 0.6611])\n",
      "R[0]\n",
      "tensor([0.0016], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01355113871395588 0.012733051466479083 0.003060977913703027 0.015378625288140029 0.6337786305546761 0.0012094398140907287 0.20202532276511193\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9210, 0.8817]) tensor([0.9101, 0.8757]) tensor([0.8830, 0.8333])\n",
      "R[0]\n",
      "tensor([0.0122], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013146700786892325 0.01319144713186688 0.002962473212313853 0.015892833630088716 0.6371611691713334 0.0011735336855053901 0.20160992495715618\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6437, 0.6458]) tensor([0.6249, 0.6332]) tensor([0.6319, 0.6497])\n",
      "R[0]\n",
      "tensor([0.0013], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01351454290561378 0.012233049356393166 0.003202208702949065 0.015555305523332208 0.6364282785654068 0.0011797090619802475 0.20112144148349761\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8154, 0.8118]) tensor([0.7917, 0.7949]) tensor([0.9156, 0.8380])\n",
      "R[0]\n",
      "tensor([-0.0011], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01286998703237623 0.011241784783233016 0.003310150127597808 0.014426790714263915 0.6369341346025467 0.0010849602073431015 0.2066458905786276\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6953, 0.7972]) tensor([0.6970, 0.7834]) tensor([0.7717, 0.8284])\n",
      "R[0]\n",
      "tensor([-0.0066], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013216757595073432 0.013028930832049809 0.0024751308638424232 0.01550587155087851 0.6379930908083916 0.001122290201485157 0.2038243675082922\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5372, 0.5344]) tensor([0.5291, 0.5343]) tensor([0.6116, 0.6014])\n",
      "R[0]\n",
      "tensor([0.0187], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013045062988065183 0.011138083462879877 0.003010461276040587 0.014228459761478006 0.6379673323631286 0.0011473353281617165 0.20633888587355614\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5665, 0.7063]) tensor([0.5718, 0.6917]) tensor([0.5178, 0.6714])\n",
      "R[0]\n",
      "tensor([-0.0093], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01258964391099289 0.011941395406087395 0.0035138192585200158 0.01453790673846379 0.6431954521536827 0.0012961985394358635 0.20580959971249105\n",
      "Average (on the epoch) training loss: 0.014989170017605647\n",
      "Episode average V value: 0\n",
      "epoch 26:\n",
      "Learning rate: 7.1789798769185315e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5196, 0.5939]) tensor([0.5074, 0.5763]) tensor([0.5081, 0.5810])\n",
      "R[0]\n",
      "tensor([0.0066], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013104585941880941 0.013395394735242008 0.003607301201969676 0.015400833391351626 0.6382677931785583 0.001255634345114231 0.20566268721222877\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4845, 0.5247]) tensor([0.4830, 0.5230]) tensor([0.4359, 0.4968])\n",
      "R[0]\n",
      "tensor([0.0175], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012882929420098662 0.012535231206245953 0.0030538173341574295 0.014390223812777549 0.6411734733581543 0.0012504029348492623 0.20722050781548024\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([1.0182, 0.9995]) tensor([0.9925, 0.9833]) tensor([0.9886, 0.9942])\n",
      "R[0]\n",
      "tensor([-0.0018], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012469507236965001 0.01182939423211792 0.003871197501890492 0.013965760798193514 0.6431388512849808 0.001056694194674492 0.20765603159368037\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8199, 0.8656]) tensor([0.7996, 0.8456]) tensor([0.6255, 0.6880])\n",
      "R[0]\n",
      "tensor([0.0010], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013344282343052327 0.012801663759542862 0.0034118442643102753 0.014806515769567341 0.6369714363217354 0.0010671835094690323 0.2044912760555744\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8739, 0.9163]) tensor([0.8574, 0.8989]) tensor([0.9321, 0.9442])\n",
      "R[0]\n",
      "tensor([0.0055], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012836774073541165 0.012127734108493314 0.0033998111140517723 0.014950102288974448 0.6375731570720673 0.001212963804602623 0.20345970222353935\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9603, 0.9328]) tensor([0.9397, 0.9191]) tensor([0.9416, 0.9299])\n",
      "R[0]\n",
      "tensor([0.0088], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012963201734703033 0.01038798234891874 0.003248028321653692 0.013071328692138196 0.6402514384388923 0.0013835102766752243 0.20672264619171618\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9947, 0.9727]) tensor([0.9692, 0.9561]) tensor([1.0372, 1.0111])\n",
      "R[0]\n",
      "tensor([-0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012747556843794883 0.013172282499508583 0.0025075815898017027 0.01502773946430534 0.642811919927597 0.0015116231441497804 0.20888257063925267\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4109, 0.3627]) tensor([0.3905, 0.3556]) tensor([0.4097, 0.3734])\n",
      "R[0]\n",
      "tensor([0.0203], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01258617043402046 0.012094286792140337 0.0027805669925255643 0.01431553167058155 0.6424612171649933 0.0011808895617723466 0.2070514393299818\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.8405, 0.8751]) tensor([0.8306, 0.8682]) tensor([0.8086, 0.8607])\n",
      "R[0]\n",
      "tensor([-0.0124], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013414824150502682 0.01340178683916747 0.0028617212044591725 0.015912662100046875 0.6371900994181633 0.001184305302798748 0.20608612436056137\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2918, 0.3957]) tensor([0.2860, 0.3828]) tensor([0.3378, 0.4573])\n",
      "R[0]\n",
      "tensor([-2.2754e-05], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01278569186432287 0.010688406025146834 0.0022568318467292557 0.013388427269645035 0.641270475268364 0.0013068625032901765 0.2079122166186571\n",
      "Average (on the epoch) training loss: 0.014522912525758148\n",
      "Episode average V value: 0\n",
      "epoch 27:\n",
      "Learning rate: 6.461081889226678e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7235, 0.8295]) tensor([0.7131, 0.8108]) tensor([0.7450, 0.8404])\n",
      "R[0]\n",
      "tensor([0.0040], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012893983595073223 0.013010706211120123 0.003210313935647719 0.014953966949600727 0.6429995820522308 0.0013766983300447463 0.20473691406846045\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5102, 0.6661]) tensor([0.5182, 0.6539]) tensor([0.6339, 0.7663])\n",
      "R[0]\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012705971424467862 0.011431335525761824 0.0033630468829469466 0.01378848196612671 0.642151376247406 0.001379180558025837 0.2096598846614361\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5069, 0.4856]) tensor([0.4869, 0.4750]) tensor([0.5091, 0.4794])\n",
      "R[0]\n",
      "tensor([0.0155], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013096994241699576 0.013351288718658906 0.003973783372170146 0.015375934317940846 0.641999861240387 0.0013497047647833824 0.2055972276479006\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7422, 0.7598]) tensor([0.7230, 0.7444]) tensor([0.6985, 0.7702])\n",
      "R[0]\n",
      "tensor([0.0013], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013059524450916796 0.013783133628712675 0.003001126497587393 0.015455827329307795 0.6439853770136833 0.0015587960109114648 0.20492184348404408\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5370, 0.5613]) tensor([0.5357, 0.5627]) tensor([0.5254, 0.5141])\n",
      "R[0]\n",
      "tensor([0.0062], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01290021549584344 0.01308973038430122 0.002520860490643827 0.01443743582093157 0.6428435091376304 0.0016308753043413162 0.2041224652081728\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2031, -0.0807]) tensor([-0.1652, -0.0440]) tensor([-0.1548,  0.0016])\n",
      "R[0]\n",
      "tensor([0.0022], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01348081551026553 0.012097828065008798 0.0027528268653713894 0.013874260504031554 0.642389936208725 0.001459913358092308 0.20658863787353038\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6389, 0.7666]) tensor([0.6284, 0.7457]) tensor([0.5710, 0.6853])\n",
      "R[0]\n",
      "tensor([0.0020], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013529217204544693 0.013567838905757527 0.0034115406978235114 0.01572346988203935 0.6377515932321548 0.0012894686460494995 0.20434498611092566\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3768, 0.4985]) tensor([0.3665, 0.4785]) tensor([0.2816, 0.4344])\n",
      "R[0]\n",
      "tensor([0.0087], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013146670986898243 0.012190838302805786 0.0036957959115334234 0.014751033181790262 0.6369929041862488 0.001387041576206684 0.20578117816150188\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5358, 0.6223]) tensor([0.5229, 0.6047]) tensor([0.3547, 0.3490])\n",
      "R[0]\n",
      "tensor([0.0091], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013195200817659498 0.011540835881081875 0.0034233148236653506 0.013940434474963695 0.6393317003250122 0.0015548125058412552 0.20594172029197216\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7431, 0.8077]) tensor([0.7388, 0.7999]) tensor([0.7374, 0.7759])\n",
      "R[0]\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013508131160866469 0.013108492757819477 0.0031444092944457225 0.014304490830050781 0.6397751693725586 0.0013794111758470536 0.20331577414274216\n",
      "Average (on the epoch) training loss: 0.014660533525678329\n",
      "Episode average V value: 0\n",
      "epoch 28:\n",
      "Learning rate: 5.81497370030401e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.6632, 0.7706]) tensor([0.6442, 0.7470]) tensor([0.6279, 0.7493])\n",
      "R[0]\n",
      "tensor([0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013325051452498883 0.012178543130125036 0.002527923331690545 0.014387883256189525 0.6395898575782776 0.001380286894738674 0.2020542972534895\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2327, 0.1520]) tensor([0.2161, 0.1564]) tensor([0.2494, 0.1858])\n",
      "R[0]\n",
      "tensor([0.0331], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014026436455082149 0.013434358924452681 0.003213560487885843 0.015491099880775436 0.6367592514753342 0.0013887036070227624 0.2018585239648819\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4962, 0.5838]) tensor([0.5015, 0.5803]) tensor([0.6276, 0.6882])\n",
      "R[0]\n",
      "tensor([0.0112], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013909165196120739 0.012286040339997271 0.0027664836205858593 0.015398816282860935 0.6369220943450927 0.001342851735651493 0.20365439476072789\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3006, 0.5107]) tensor([0.3246, 0.5016]) tensor([0.1769, 0.4211])\n",
      "R[0]\n",
      "tensor([0.0222], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01451447497261688 0.012590978805761552 0.0025151135771775443 0.015298759192228317 0.6313952598571777 0.0014564144015312196 0.1977333330512047\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4002, 0.3475]) tensor([0.3993, 0.3619]) tensor([0.4883, 0.4340])\n",
      "R[0]\n",
      "tensor([0.0223], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014130427279509604 0.012198502571729478 0.0029763591426999482 0.015512262600008398 0.6344501764774323 0.0012761261686682701 0.20232996092736721\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0069, -0.2460]) tensor([ 0.0032, -0.2003]) tensor([ 0.0351, -0.4401])\n",
      "R[0]\n",
      "tensor([0.0787], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013684950246009976 0.011818570698145778 0.0033550130533694755 0.015014327042736112 0.6383074442148209 0.0012710826098918915 0.20251672254502773\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.0515, 0.2492]) tensor([0.0636, 0.2359]) tensor([0.0767, 0.2742])\n",
      "R[0]\n",
      "tensor([0.0040], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01361572714149952 0.012292898202758806 0.002720593899995947 0.015188903839327394 0.6382931453585625 0.0012760006189346314 0.20378804428875447\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0247, -0.0901]) tensor([-0.0181, -0.0684]) tensor([-0.1337, -0.4358])\n",
      "R[0]\n",
      "tensor([0.0431], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01365065635368228 0.012073859846226696 0.0031692654990365552 0.014979775495594367 0.6361727118492126 0.0012942379787564278 0.20167309099435807\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3771, 0.4525]) tensor([0.3878, 0.4554]) tensor([0.4588, 0.4898])\n",
      "R[0]\n",
      "tensor([0.0153], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013145735518541188 0.01180494558794453 0.003111110134756018 0.01438185060955584 0.6390164622068405 0.0015849670991301536 0.20508642715215683\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2111, 0.3336]) tensor([0.2262, 0.3440]) tensor([0.4965, 0.6089])\n",
      "R[0]\n",
      "tensor([0.0150], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012830759027041495 0.011475889599387301 0.003419742615369614 0.013984061567112804 0.6402523159980774 0.0016206457987427711 0.2107704416513443\n",
      "Average (on the epoch) training loss: 0.014963773976638913\n",
      "Episode average V value: 0\n",
      "epoch 29:\n",
      "Learning rate: 5.23347633027361e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4756, 0.5819]) tensor([0.4801, 0.5764]) tensor([0.4517, 0.6055])\n",
      "R[0]\n",
      "tensor([0.0052], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013351411276962608 0.012394893345277524 0.0028254482942575124 0.01462070468114689 0.6395040767192841 0.0017258830443024635 0.20305458511412144\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3022, 0.1134]) tensor([0.2786, 0.1253]) tensor([0.4676, 0.2300])\n",
      "R[0]\n",
      "tensor([0.0435], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01332582284975797 0.011772933575120987 0.003364299549763018 0.01462229021335952 0.6397363690137863 0.0015701179951429368 0.2024517338424921\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.5001, 0.5069]) tensor([0.4982, 0.5107]) tensor([0.4643, 0.4946])\n",
      "R[0]\n",
      "tensor([0.0096], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013808717483188957 0.01212527707812842 0.0030236468286857416 0.015429172800388188 0.6359725533127785 0.001339537888765335 0.20306590388715268\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7177, 0.6919]) tensor([0.7043, 0.6863]) tensor([0.8316, 0.8170])\n",
      "R[0]\n",
      "tensor([0.0120], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013887877931352705 0.011276346396029112 0.003717717201132473 0.014705426384229213 0.6345117431282997 0.0015657577589154243 0.2029097315520048\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2302, -0.0248]) tensor([0.2287, 0.0450]) tensor([ 0.2869, -0.0909])\n",
      "R[0]\n",
      "tensor([0.1269], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013085052908863873 0.012146128397231223 0.0036703265664236825 0.014814234630437568 0.6410311365127563 0.00157121342420578 0.2033724519610405\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.9179, 0.9068]) tensor([0.8966, 0.8916]) tensor([0.9474, 0.9147])\n",
      "R[0]\n",
      "tensor([0.0089], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013084969676099718 0.011674644504208118 0.004015290970464776 0.014790715739363804 0.6398385751843453 0.0016574516519904136 0.2051416613459587\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1713, 0.2165]) tensor([0.2008, 0.2438]) tensor([0.1902, 0.2176])\n",
      "R[0]\n",
      "tensor([0.0512], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013347903312183917 0.011876162249420303 0.0029214012021038798 0.014663832765072583 0.6372560868263245 0.0013544279262423515 0.20249784395098686\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.1776, 0.2687]) tensor([0.1783, 0.2644]) tensor([0.2656, 0.2376])\n",
      "R[0]\n",
      "tensor([-0.0015], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01354250182583928 0.011866027173746262 0.0031531816816823266 0.014707175974268466 0.6351010282039642 0.001398761622607708 0.20366507531702518\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.7418, 0.7965]) tensor([0.7214, 0.7760]) tensor([0.8753, 0.9218])\n",
      "R[0]\n",
      "tensor([-0.0008], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013448184107430278 0.0123726078994805 0.003016474410556839 0.014594119117828087 0.6382495413422584 0.0013235767632722854 0.20571744717657567\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4026, 0.4890]) tensor([0.3894, 0.4719]) tensor([0.3999, 0.5548])\n",
      "R[0]\n",
      "tensor([0.0024], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01323165178205818 0.011355592491592688 0.003010865583521081 0.01432755283289589 0.6388390409946442 0.001434791088104248 0.2046554552614689\n",
      "Average (on the epoch) training loss: 0.01472752251389902\n",
      "Episode average V value: 0\n",
      "epoch 30:\n",
      "Learning rate: 4.710128697246249e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:352: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:360: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0715, -0.0456]) tensor([-0.0559, -0.0289]) tensor([-0.0344, -0.0206])\n",
      "R[0]\n",
      "tensor([0.0314], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013430412975139916 0.013985226599426824 0.0037525278655557485 0.015639516222290695 0.6382153388261795 0.0014672747254371643 0.20081318861246109\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.3823, 0.4840]) tensor([0.3717, 0.4662]) tensor([0.2638, 0.3682])\n",
      "R[0]\n",
      "tensor([0.0015], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01378022905997932 0.01232128502386331 0.00326974317789427 0.014171246913727373 0.6369241110682488 0.0015340997949242592 0.20064471866190434\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0746, -0.3602]) tensor([-0.0201, -0.2124]) tensor([-0.0079, -0.2976])\n",
      "R[0]\n",
      "tensor([0.2183], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.013307352867908776 0.011608520866997424 0.00273472814411798 0.014364181587705388 0.6351629717946052 0.0013845626041293145 0.20228234654664992\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1253,  0.0590]) tensor([-0.0680,  0.1128]) tensor([-0.1845, -0.0205])\n",
      "R[0]\n",
      "tensor([0.0506], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"params\")\n",
    "except Exception:\n",
    "    pass\n",
    "dump(vars(parameters), \"params/\" + fname + \".jldump\")\n",
    "#agent.gathering_data=False\n",
    "agent.run(parameters.epochs, parameters.steps_per_epoch)\n",
    "\n",
    "# --- Show results ---\n",
    "basename = \"scores/\" + fname\n",
    "scores = load(basename + \"_scores.jldump\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.setNetwork(fname, nEpoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._in_episode = True\n",
    "agent._mode = 0 # Testing mode with plan_depth=0\n",
    "initState = env.reset(agent._mode)\n",
    "inputDims = env.inputDimensions()\n",
    "\n",
    "for i in range(len(inputDims)):\n",
    "    if inputDims[i][0] > 1:\n",
    "        agent._state[i][1:] = initState[i][1:]\n",
    "agent._Vs_on_last_episode = []\n",
    "is_terminal = False\n",
    "reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for i in range(100):\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    plt.figure()\n",
    "    plt.imshow(np.flip(_obs.squeeze()))\n",
    "    plt.show()\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "    V, action, reward, _ = agent._step()\n",
    "    print(action)\n",
    "    agent._Vs_on_last_episode.append(V)\n",
    "    is_terminal = env.inTerminalState()\n",
    "    if is_terminal: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "obs = env.observe()\n",
    "_obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "_obs = np.flip(_obs.squeeze())\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "im = ax.imshow(np.zeros(_obs.shape))\n",
    "\n",
    "def init():\n",
    "    plt.cla()\n",
    "    im = ax.imshow(_obs)\n",
    "    return [im]\n",
    "\n",
    "def animate(i, *args, **kwargs):\n",
    "    plt.cla()\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    _obs = np.flip(_obs.squeeze())\n",
    "    im = ax.imshow(_obs)\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "        V, action, reward, _ = agent._step()\n",
    "        agent._Vs_on_last_episode.append(V)\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init, \n",
    "     frames=100, blit=False, repeat=True)\n",
    "ani.save('behavior.gif', writer=\"ffmpeg\", fps = 15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
