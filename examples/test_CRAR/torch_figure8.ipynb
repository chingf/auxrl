{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import hash, dump, load\n",
    "import os\n",
    "\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.CRAR_torch import CRAR\n",
    "from figure8_env import MyEnv as figure8_env\n",
    "import deer.experiment.base_controllers as bc\n",
    "\n",
    "from deer.policies import EpsilonGreedyPolicy, FixedFigure8Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure8_give_rewards = True #False\n",
    "nn_yaml = 'network_noconv.yaml'\n",
    "higher_dim_obs = True #False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    steps_per_epoch = 5000\n",
    "    epochs = 50\n",
    "    steps_per_test = 1000\n",
    "    period_btw_summary_perfs = 1\n",
    "    \n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    frame_skip = 2\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    update_rule = 'rmsprop'\n",
    "    learning_rate = 5 * 1E-4 # 1E-4\n",
    "    learning_rate_decay = 0.9\n",
    "    discount = 0.9\n",
    "    discount_inc = 1\n",
    "    discount_max = 0.99\n",
    "    rms_decay = 0.9\n",
    "    rms_epsilon = 0.0001\n",
    "    momentum = 0\n",
    "    clip_norm = 1.0\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 1.0\n",
    "    epsilon_decay = 10000\n",
    "    update_frequency = 1\n",
    "    replay_memory_size = 1000000 #replacing with 200000 will works just fine (in case you dont have 18gb of memory)\n",
    "    batch_size = 32\n",
    "    freeze_interval = 1000\n",
    "    deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters hash is: 62977be8e45d8a56a5537c11dfd5d2fd8dda69e0\n",
      "The parameters are: <__main__.Defaults object at 0x2b64dc33d2b0>\n",
      "end gathering data\n"
     ]
    }
   ],
   "source": [
    "parameters = Defaults()\n",
    "if parameters.deterministic:\n",
    "    rng = np.random.RandomState(123456)\n",
    "else:\n",
    "    rng = np.random.RandomState()\n",
    "\n",
    "# --- Instantiate environment ---\n",
    "env = figure8_env(\n",
    "    give_rewards=figure8_give_rewards,\n",
    "    higher_dim_obs=higher_dim_obs,\n",
    "    )\n",
    "\n",
    "# --- Instantiate learning_algo ---\n",
    "learning_algo = CRAR(\n",
    "    env,\n",
    "    parameters.rms_decay,\n",
    "    parameters.rms_epsilon,\n",
    "    parameters.momentum,\n",
    "    parameters.clip_norm,\n",
    "    parameters.freeze_interval,\n",
    "    parameters.batch_size,\n",
    "    parameters.update_rule,\n",
    "    rng,\n",
    "    high_int_dim=False,\n",
    "    internal_dim=2, lr=parameters.learning_rate,\n",
    "    nn_yaml=nn_yaml\n",
    "    )\n",
    "\n",
    "if figure8_give_rewards:\n",
    "    train_policy = None\n",
    "    test_policy = EpsilonGreedyPolicy(learning_algo, env.nActions(), rng, 0.)\n",
    "else:\n",
    "    train_policy = FixedFigure8Policy.FixedFigure8Policy(learning_algo, env.nActions(), rng)\n",
    "    test_policy = FixedFigure8Policy.FixedFigure8Policy(learning_algo, env.nActions(), rng)\n",
    "\n",
    "# --- Instantiate agent ---\n",
    "agent = NeuralAgent(\n",
    "    env,\n",
    "    learning_algo,\n",
    "    parameters.replay_memory_size,\n",
    "    1,\n",
    "    parameters.batch_size,\n",
    "    rng,\n",
    "    train_policy=train_policy,\n",
    "    test_policy=test_policy)\n",
    "\n",
    "# --- Create unique filename for FindBestController ---\n",
    "h = hash(vars(parameters), hash_name=\"sha1\")\n",
    "fname = \"test_\" + h\n",
    "print(\"The parameters hash is: {}\".format(h))\n",
    "print(\"The parameters are: {}\".format(parameters))\n",
    "\n",
    "# As for the discount factor and the learning rate, one can update periodically the parameter of the epsilon-greedy\n",
    "# policy implemented by the agent. This controllers has a bit more capabilities, as it allows one to choose more\n",
    "# precisely when to update epsilon: after every X action, episode or epoch. This parameter can also be reset every\n",
    "# episode or epoch (or never, hence the resetEvery='none').\n",
    "agent.attach(bc.EpsilonController(\n",
    "    initial_e=parameters.epsilon_start,\n",
    "    e_decays=parameters.epsilon_decay,\n",
    "    e_min=parameters.epsilon_min,\n",
    "    evaluate_on='action',\n",
    "    periodicity=1,\n",
    "    reset_every='none'))\n",
    "\n",
    "agent.run(10, 500)\n",
    "print(\"end gathering data\")\n",
    "\n",
    "# --- Bind controllers to the agent ---\n",
    "# Before every training epoch (periodicity=1), we want to print a summary of the agent's epsilon, discount and \n",
    "# learning rate as well as the training epoch number.\n",
    "agent.attach(bc.VerboseController(\n",
    "    evaluate_on='epoch', \n",
    "    periodicity=1))\n",
    "\n",
    "# Every epoch end, one has the possibility to modify the learning rate using a LearningRateController. Here we \n",
    "# wish to update the learning rate after every training epoch (periodicity=1), according to the parameters given.\n",
    "agent.attach(bc.LearningRateController(\n",
    "    initial_learning_rate=parameters.learning_rate, \n",
    "    learning_rate_decay=parameters.learning_rate_decay,\n",
    "    periodicity=1))\n",
    "\n",
    "# Same for the discount factor.\n",
    "agent.attach(bc.DiscountFactorController(\n",
    "    initial_discount_factor=parameters.discount, \n",
    "    discount_factor_growth=parameters.discount_inc, \n",
    "    discount_factor_max=parameters.discount_max,\n",
    "    periodicity=1))\n",
    "\n",
    "# During training epochs, we want to train the agent after every [parameters.update_frequency] action it takes.\n",
    "# Plus, we also want to display after each training episode (!= than after every training) the average bellman\n",
    "# residual and the average of the V values obtained during the last episode, hence the two last arguments.\n",
    "agent.attach(bc.TrainerController(\n",
    "    evaluate_on='action', \n",
    "    periodicity=parameters.update_frequency, \n",
    "    show_episode_avg_V_value=True, \n",
    "    show_avg_Bellman_residual=True))\n",
    "\n",
    "# We wish to discover, among all versions of our neural network (i.e., after every training epoch), which one \n",
    "# has the highest validation score.\n",
    "# To achieve this goal, one can use the FindBestController along with an InterleavedTestEpochControllers. It is \n",
    "# important that the validationID is the same than the id argument of the InterleavedTestEpochController.\n",
    "# The FindBestController will dump on disk the validation scores for each and every network, as well as the \n",
    "# structure of the neural network having the best validation score. These dumps can then used to plot the evolution \n",
    "# of the validation and test scores (see below) or simply recover the resulting neural network for your \n",
    "# application.\n",
    "agent.attach(bc.FindBestController(\n",
    "    validationID=figure8_env.VALIDATION_MODE,\n",
    "    testID=None,\n",
    "    unique_fname=fname))\n",
    "\n",
    "# All previous controllers control the agent during the epochs it goes through. However, we want to interleave a \n",
    "# \"validation epoch\" between each training epoch. For each validation epoch, we want also to display the sum of all \n",
    "# rewards obtained, hence the showScore=True. Finally, we want to call the summarizePerformance method of ALE_env \n",
    "# every [parameters.period_btw_summary_perfs] *validation* epochs.\n",
    "agent.attach(bc.InterleavedTestEpochController(\n",
    "    id=figure8_env.VALIDATION_MODE, \n",
    "    epoch_length=parameters.steps_per_test,\n",
    "    periodicity=1,\n",
    "    show_score=True,\n",
    "    summarize_every=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m dump(\u001b[38;5;28mvars\u001b[39m(parameters), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jldump\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m agent\u001b[38;5;241m.\u001b[39mgathering_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# --- Show results ---\u001b[39;00m\n\u001b[1;32m     10\u001b[0m basename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fname\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/agent.py:270\u001b[0m, in \u001b[0;36mNeuralAgent.run\u001b[0;34m(self, n_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03mThis function encapsulates the inference and the learning.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03mIf the agent is in train mode (mode = -1):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m    maximum number of steps for a given epoch\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_non_train(n_epochs, epoch_length)\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/agent.py:295\u001b[0m, in \u001b[0;36mNeuralAgent._run_train\u001b[0;34m(self, n_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_loss_averages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m nbr_steps_left \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# run new episodes until the number of steps left for the epoch has reached 0\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m     nbr_steps_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runEpisode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbr_steps_left\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_controllers: c\u001b[38;5;241m.\u001b[39monEpochEnd(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/agent.py:374\u001b[0m, in \u001b[0;36mNeuralAgent._runEpisode\u001b[0;34m(self, maxSteps)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_addSample(obs, action, reward, \u001b[38;5;28;01mTrue\u001b[39;00m)      \u001b[38;5;66;03m# If the episode ends because max number of steps is reached, mark the transition as terminal\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_controllers: \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monActionTaken\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_terminal:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/experiment/base_controllers.py:407\u001b[0m, in \u001b[0;36mTrainerController.onActionTaken\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_action:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/experiment/base_controllers.py:411\u001b[0m, in \u001b[0;36mTrainerController._update\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent):\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_periodicity \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_periodicity \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 411\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/agent.py:193\u001b[0m, in \u001b[0;36mNeuralAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m     loss, loss_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learning_algo\u001b[38;5;241m.\u001b[39mtrain(observations, actions, rewards, terminals)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     states, actions, rewards, next_states, terminals, rndValidIndices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandomBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exp_priority\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     loss, loss_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learning_algo\u001b[38;5;241m.\u001b[39mtrain(states, actions, rewards, next_states, terminals)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_loss_averages\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/agent.py:613\u001b[0m, in \u001b[0;36mDataSet.randomBatch\u001b[0;34m(self, batch_size, use_priority)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mslice\u001b[39m)):\n\u001b[0;32m--> 613\u001b[0m         states[\u001b[38;5;28minput\u001b[39m][i][\u001b[38;5;241m-\u001b[39mj\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mslice\u001b[39m[\u001b[38;5;241m-\u001b[39mj\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    614\u001b[0m  \u001b[38;5;66;03m# If transition leads to terminal, we don't care about next state\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rndValidIndices[i] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_elems \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m terminals[i]:\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"params\")\n",
    "except Exception:\n",
    "    pass\n",
    "dump(vars(parameters), \"params/\" + fname + \".jldump\")\n",
    "agent.gathering_data=False\n",
    "agent.run(parameters.epochs, parameters.steps_per_epoch)\n",
    "\n",
    "# --- Show results ---\n",
    "basename = \"scores/\" + fname\n",
    "scores = load(basename + \"_scores.jldump\")\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._learning_algo.crar.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._in_episode = True\n",
    "agent._mode = 0 # Testing mode with plan_depth=0\n",
    "initState = env.reset(agent._mode)\n",
    "inputDims = env.inputDimensions()\n",
    "\n",
    "for i in range(len(inputDims)):\n",
    "    if inputDims[i][0] > 1:\n",
    "        agent._state[i][1:] = initState[i][1:]\n",
    "agent._Vs_on_last_episode = []\n",
    "is_terminal = False\n",
    "reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/ctn/users/cf2794/Code/deer/deer/learning_algos/CRAR_torch.py:395: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  state = torch.as_tensor(state, device=self.device).float()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (7x6 and 1x200)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     agent\u001b[38;5;241m.\u001b[39m_state[i][\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39m_state[i][\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     10\u001b[0m     agent\u001b[38;5;241m.\u001b[39m_state[i][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m obs[i]\n\u001b[0;32m---> 11\u001b[0m V, action, reward \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m agent\u001b[38;5;241m.\u001b[39m_Vs_on_last_episode\u001b[38;5;241m.\u001b[39mappend(V)\n\u001b[1;32m     13\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39minTerminalState()\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/agent.py:398\u001b[0m, in \u001b[0;36mNeuralAgent._step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    This method is called at each time step and performs one action in the environment.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;03m        Reward obtained for the transition\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     action, V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chooseAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msticky_action):\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/agent.py:416\u001b[0m, in \u001b[0;36mNeuralAgent._chooseAction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chooseAction\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;66;03m# Act according to the test policy if not in training mode\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m         action, V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset\u001b[38;5;241m.\u001b[39mn_elems \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replay_start_size:\n\u001b[1;32m    419\u001b[0m             \u001b[38;5;66;03m# follow the train policy\u001b[39;00m\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/policies/EpsilonGreedyPolicy.py:21\u001b[0m, in \u001b[0;36mEpsilonGreedyPolicy.action\u001b[0;34m(self, state, mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     action, V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandomAction()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     action, V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbestAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action, V\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/base_classes/policy.py:30\u001b[0m, in \u001b[0;36mPolicy.bestAction\u001b[0;34m(self, state, mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbestAction\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124;03m\"\"\" Returns the best Action for the given state. This is an additional encapsulation for q-network.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     action,V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchooseBestAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action, V\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/learning_algos/CRAR_torch.py:396\u001b[0m, in \u001b[0;36mCRAR.chooseBestAction\u001b[0;34m(self, state, mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    395\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(state, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m--> 396\u001b[0m     xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m q_vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqValues(xs, d\u001b[38;5;241m=\u001b[39mdepths[mode])\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39margmax(q_vals), torch\u001b[38;5;241m.\u001b[39mmax(q_vals)\n",
      "File \u001b[0;32m~/.conda/envs/auxrl/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/learning_algos/NN_CRAR_torch.py:123\u001b[0m, in \u001b[0;36mNN.encoder_model.<locals>.Encoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    121\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs(x)\n\u001b[1;32m    122\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 123\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/auxrl/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/auxrl/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/auxrl/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/auxrl/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (7x6 and 1x200)"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for i in range(100):\n",
    "    obs = env.observe()\n",
    "    plt.figure()\n",
    "    plt.imshow(np.flip(obs[0].squeeze()))\n",
    "    plt.show()\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "    V, action, reward = agent._step()\n",
    "    agent._Vs_on_last_episode.append(V)\n",
    "    is_terminal = env.inTerminalState()\n",
    "    if is_terminal: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(obs[0]==10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
