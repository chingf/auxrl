{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import hash, dump, load\n",
    "import os\n",
    "\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.CRAR_torch import CRAR\n",
    "from figure8_env import MyEnv as figure8_env\n",
    "import deer.experiment.base_controllers as bc\n",
    "\n",
    "from deer.policies import EpsilonGreedyPolicy, FixedFigure8Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure8_give_rewards = True\n",
    "nn_yaml = 'network_noconv.yaml'\n",
    "higher_dim_obs = False\n",
    "internal_dim = 10\n",
    "fname = 'mb_only'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Setup Parameters (copied for convenience)\n",
    "    # ----------------------\n",
    "    figure8_give_rewards = figure8_give_rewards\n",
    "    nn_yaml = nn_yaml\n",
    "    higher_dim_obs = higher_dim_obs\n",
    "    internal_dim = internal_dim\n",
    "    fname = fname\n",
    "    \n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    steps_per_epoch = 5000\n",
    "    epochs = 50\n",
    "    steps_per_test = 1000\n",
    "    period_btw_summary_perfs = 1\n",
    "\n",
    "    # ----------------------\n",
    "    # Temporal Processing Parameters\n",
    "    # ----------------------\n",
    "    nstep = 15 #20\n",
    "    nstep_decay = 0.8\n",
    "    encoder_type = 'variational'\n",
    "    \n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    frame_skip = 2\n",
    "    show_rewards = False\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    learning_rate = 1*1E-4\n",
    "    learning_rate_decay = 1.0\n",
    "    discount = 0.9\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 1.0\n",
    "    epsilon_decay = 1000\n",
    "    update_frequency = 1\n",
    "    replay_memory_size = 100000 #50000\n",
    "    batch_size = 64\n",
    "    freeze_interval = 1000\n",
    "    deterministic = False\n",
    "    \n",
    "    # ----------------------\n",
    "    # Learning algo parameters\n",
    "    # ----------------------\n",
    "    # T, entropy_neighbor, entropy_random, volume, gamma, R, Q, variational\n",
    "    #loss_weights = [5E-3, 1E-3, 5E-3, 5E-3, 5E-3, 5E-3, 1.]\n",
    "    #loss_weights = [0, 0, 0, 0, 0, 0, 1.]\n",
    "    #loss_weights = [0., 0., 0., 0., 0., 0., 1., 2E-4]\n",
    "    loss_weights = [5E-3, 1E-3, 5E-3, 5E-3, 5E-3, 5E-3, 0., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = Defaults()\n",
    "with open(f'params/{fname}.p', 'wb') as f:\n",
    "    pickle.dump(parameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end gathering data\n"
     ]
    }
   ],
   "source": [
    "if parameters.deterministic:\n",
    "    rng = np.random.RandomState(123456)\n",
    "else:\n",
    "    rng = np.random.RandomState()\n",
    "\n",
    "# --- Instantiate environment ---\n",
    "env = figure8_env(\n",
    "    give_rewards=figure8_give_rewards,\n",
    "    intern_dim=internal_dim,\n",
    "    higher_dim_obs=higher_dim_obs,\n",
    "    show_rewards=parameters.show_rewards,\n",
    "    nstep=parameters.nstep, nstep_decay=parameters.nstep_decay\n",
    "    )\n",
    "\n",
    "# --- Instantiate learning_algo ---\n",
    "learning_algo = CRAR(\n",
    "    env,\n",
    "    parameters.freeze_interval,\n",
    "    parameters.batch_size,\n",
    "    rng,\n",
    "    high_int_dim=False,\n",
    "    internal_dim=internal_dim, lr=parameters.learning_rate,\n",
    "    nn_yaml=nn_yaml, double_Q=True,\n",
    "    loss_weights=parameters.loss_weights,\n",
    "    nstep=parameters.nstep, nstep_decay=parameters.nstep_decay,\n",
    "    encoder_type=parameters.encoder_type\n",
    "    )\n",
    "\n",
    "if figure8_give_rewards:\n",
    "    train_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.2,\n",
    "        consider_valid_transitions=False\n",
    "        )\n",
    "    test_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.\n",
    "        )\n",
    "else:\n",
    "    train_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng, epsilon=0.2,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "    test_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "\n",
    "# --- Instantiate agent ---\n",
    "agent = NeuralAgent(\n",
    "    env, learning_algo,\n",
    "    parameters.replay_memory_size,\n",
    "    1, parameters.batch_size, rng,\n",
    "    train_policy=train_policy, test_policy=test_policy)\n",
    "#agent.setNetwork('test', nEpoch=7)\n",
    "\n",
    "agent.run(10, 500)\n",
    "print(\"end gathering data\")\n",
    "\n",
    "# --- Bind controllers to the agent ---\n",
    "# Before every training epoch (periodicity=1), we want to print a summary of the agent's epsilon, discount and \n",
    "# learning rate as well as the training epoch number.\n",
    "agent.attach(bc.VerboseController(\n",
    "    evaluate_on='epoch', \n",
    "    periodicity=1))\n",
    "\n",
    "# Learning rate may follow a scheduler\n",
    "agent.attach(bc.LearningRateController(\n",
    "    initial_learning_rate=parameters.learning_rate, \n",
    "    learning_rate_decay=parameters.learning_rate_decay,\n",
    "    periodicity=1))\n",
    "\n",
    "# During training epochs, we want to train the agent after every [parameters.update_frequency] action it takes.\n",
    "# Plus, we also want to display after each training episode (!= than after every training) the average bellman\n",
    "# residual and the average of the V values obtained during the last episode, hence the two last arguments.\n",
    "agent.attach(bc.TrainerController(\n",
    "    evaluate_on='action', \n",
    "    periodicity=parameters.update_frequency, \n",
    "    show_episode_avg_V_value=True, \n",
    "    show_avg_Bellman_residual=True))\n",
    "\n",
    "# We wish to discover, among all versions of our neural network (i.e., after every training epoch), which one \n",
    "# has the highest validation score.\n",
    "# To achieve this goal, one can use the FindBestController along with an InterleavedTestEpochControllers. It is \n",
    "# important that the validationID is the same than the id argument of the InterleavedTestEpochController.\n",
    "# The FindBestController will dump on disk the validation scores for each and every network, as well as the \n",
    "# structure of the neural network having the best validation score. These dumps can then used to plot the evolution \n",
    "# of the validation and test scores (see below) or simply recover the resulting neural network for your \n",
    "# application.\n",
    "agent.attach(bc.FindBestController(\n",
    "    validationID=figure8_env.VALIDATION_MODE,\n",
    "    testID=None,\n",
    "    unique_fname=fname))\n",
    "\n",
    "# All previous controllers control the agent during the epochs it goes through. However, we want to interleave a \n",
    "# \"validation epoch\" between each training epoch. For each validation epoch, we want also to display the sum of all \n",
    "# rewards obtained, hence the showScore=True. Finally, we want to call the summarizePerformance method of ALE_env \n",
    "# every [parameters.period_btw_summary_perfs] *validation* epochs.\n",
    "agent.attach(bc.InterleavedTestEpochController(\n",
    "    id=4, #figure8_env.VALIDATION_MODE, \n",
    "    epoch_length=parameters.steps_per_test,\n",
    "    periodicity=1,\n",
    "    show_score=True,\n",
    "    summarize_every=1,\n",
    "    unique_fname=fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2564,  0.3370, -0.2887, -0.2132, -0.8375, -1.1611,  1.5461,  0.8724,\n",
      "        -0.1277, -1.7907]) tensor([ 0.1974,  0.4150, -0.1651, -0.3140, -0.8839, -1.0156,  1.8061,  0.8656,\n",
      "        -0.2465, -1.8082]) tensor([ 0.3286,  0.1901,  0.3193, -0.6153, -0.9397,  0.6058,  1.0818,  0.2714,\n",
      "        -0.4723,  1.3920])\n",
      "R[0]\n",
      "tensor([-0.0028], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 1.0914389392137527; R = 0.0028216145677179157;                 Gamma = 0.23573839682713152; Q = 0.026096030319109557;\n",
      "Entropy Neighbor = 2.4993908123242647e-05;                 Entropy Random = 2.3971439615575464e-05;                 Volume = 1.3348172419667244; VAE = 1.187779640700668\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.8749,  0.1830, -0.8391,  0.3862,  0.1028,  1.2806,  0.5252,  0.4654,\n",
      "        -0.2283,  0.2625]) tensor([-0.6602,  0.0095, -0.6149,  0.2212,  0.0949,  1.2932,  0.6125,  0.3123,\n",
      "        -0.4711,  0.4676]) tensor([-0.4603,  0.7412,  0.6616,  0.7787,  0.5086,  0.8122, -0.3940, -0.0421,\n",
      "         0.6157,  0.8816])\n",
      "R[0]\n",
      "tensor([-0.0066], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.5171234261989593; R = 0.0012894279974566417;                 Gamma = 0.024337706320453435; Q = 0.022851733464747667;\n",
      "Entropy Neighbor = 0.0001427098062049481;                 Entropy Random = 0.00014012494827329646;                 Volume = 0.3886709160208702; VAE = 2.818181548833847\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6336, -0.1955,  0.1047,  0.0447, -0.3325,  0.2768,  0.4039,  0.4262,\n",
      "         0.0369,  0.9790]) tensor([ 0.4687, -0.0152, -0.1030,  0.2004, -0.2629,  0.2717,  0.3527,  0.7150,\n",
      "         0.1386,  0.5160]) tensor([ 0.4205, -0.6243, -0.1038, -0.7049,  0.8551, -0.1747,  0.7594, -0.3129,\n",
      "        -0.2088,  0.0839])\n",
      "R[0]\n",
      "tensor([3.0527e-05], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.3319283538758755; R = 0.001413125360097183;                 Gamma = 0.02037549174763262; Q = 0.014097645094618201;\n",
      "Entropy Neighbor = 0.00040498187560297083;                 Entropy Random = 0.00037575661433220375;                 Volume = 0.15451254814118148; VAE = 4.0205201816558835\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3685, -0.0883,  0.4537, -0.3712, -0.2809, -0.5011,  0.0427,  0.5845,\n",
      "        -0.9504,  0.9162]) tensor([ 0.0706,  0.1482,  0.1888, -0.1480, -0.3413, -0.4701,  0.2305,  0.6629,\n",
      "        -0.3101,  0.4354]) tensor([ 0.2226, -0.4376,  0.1857, -0.5100,  0.1690, -0.3693, -0.2898, -0.7749,\n",
      "         0.0492,  0.3474])\n",
      "R[0]\n",
      "tensor([0.0004], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.24205768644809722; R = 0.0014115264605572976;                 Gamma = 0.017318230532051528; Q = 0.01360689689591527;\n",
      "Entropy Neighbor = 0.0008046178341319319;                 Entropy Random = 0.0007876791538437829;                 Volume = 0.06818388822302222; VAE = 5.002462763309479\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6396,  0.1330, -0.0167, -0.1293, -0.1689, -0.0422,  0.9663, -0.4403,\n",
      "        -0.3706, -0.3847]) tensor([ 0.4022,  0.1030, -0.1200, -0.2360, -0.2955,  0.0128,  0.2282, -0.0515,\n",
      "        -0.0302, -0.1743]) tensor([ 0.3940, -0.3128, -0.1283,  0.4696, -0.2194, -0.0094, -0.3043, -0.2594,\n",
      "         0.2328, -0.7058])\n",
      "R[0]\n",
      "tensor([0.0007], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.19279821208119394; R = 0.0012874289179127345;                 Gamma = 0.016783013531879987; Q = 0.01223883238248527;\n",
      "Entropy Neighbor = 0.0013712331501301378;                 Entropy Random = 0.0013654142042505554;                 Volume = 0.03084833287447691; VAE = 5.882877096176148\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1692, -0.0294, -0.5313,  0.5875,  0.3272,  0.1501,  0.0551,  0.1810,\n",
      "        -0.0263, -0.5726]) tensor([-0.0343, -0.1192, -0.0794,  0.3249,  0.1056,  0.1521, -0.1416, -0.1439,\n",
      "        -0.0674,  0.0284]) tensor([-0.5374,  0.1616,  0.3219,  0.2473,  0.2939,  0.0558,  0.4034, -0.0628,\n",
      "         0.1630,  0.1033])\n",
      "R[0]\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.16008657971024512; R = 0.001162273079789884;                 Gamma = 0.015150284513598308; Q = 0.011468873835168779;\n",
      "Entropy Neighbor = 0.002214818122796714;                 Entropy Random = 0.0021647276129806415;                 Volume = 0.016373310517519713; VAE = 6.661659445762634\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5590,  0.0965,  0.2042,  0.3699, -0.4133,  0.1331, -0.1586, -0.2940,\n",
      "        -0.8496,  0.4785]) tensor([ 0.3055,  0.2589, -0.1528,  0.4309, -0.3506,  0.0570, -0.1560, -0.1460,\n",
      "        -0.1549,  0.1001]) tensor([ 0.2885, -0.1496, -0.4193, -0.6302, -0.0878, -0.1073, -0.0277,  0.4479,\n",
      "        -0.7682,  0.0581])\n",
      "R[0]\n",
      "tensor([-0.0028], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.13482998651266098; R = 0.0011287643450536393;                 Gamma = 0.014675024179188768; Q = 0.010643711840268225;\n",
      "Entropy Neighbor = 0.0032249244276899842;                 Entropy Random = 0.0031744798158761115;                 Volume = 0.007100279040634632; VAE = 7.4257057714462285\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2068, -0.1612, -0.0006, -0.0430, -0.1657, -0.0314, -0.0563, -0.3924,\n",
      "        -0.5016, -0.4086]) tensor([ 0.0085, -0.1404, -0.2926, -0.0485, -0.2716,  0.0270, -0.1409, -0.2779,\n",
      "        -0.0726, -0.1535]) tensor([-0.3782, -0.0601,  0.0060, -0.0419,  0.3512,  0.2237, -0.1531, -0.2663,\n",
      "        -0.0717, -0.7956])\n",
      "R[0]\n",
      "tensor([0.0055], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.11395628325641155; R = 0.001316591265304055;                 Gamma = 0.013708456088003003; Q = 0.010533309569116682;\n",
      "Entropy Neighbor = 0.004578799387440086;                 Entropy Random = 0.004519374046940357;                 Volume = 0.0035115456283092497; VAE = 8.124338548660278\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5501,  0.0512, -0.0300,  0.3716, -0.8443, -0.0521,  0.1641,  0.1280,\n",
      "         0.0243,  0.0100]) tensor([ 0.5116,  0.1497, -0.2042,  0.2581, -0.5386, -0.3161,  0.0556,  0.1883,\n",
      "        -0.0212, -0.0024]) tensor([-0.2880, -0.0337, -0.4027, -0.2179, -0.2317, -0.2711, -0.1921, -0.2182,\n",
      "         0.3308, -0.5357])\n",
      "R[0]\n",
      "tensor([-0.0015], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.09650818668305874; R = 0.0008157105647979677;                 Gamma = 0.01268681587302126; Q = 0.009439029959030449;\n",
      "Entropy Neighbor = 0.006402674402110279;                 Entropy Random = 0.005689873752649874;                 Volume = 0.0020521143078804016; VAE = 8.838014167785644\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2276, -0.1457,  0.3744,  0.1850,  0.1153, -0.1292, -0.2888, -0.1324,\n",
      "         0.1620, -0.5678]) tensor([ 0.0363, -0.1102, -0.0225,  0.1126, -0.0107,  0.0183, -0.0050,  0.0206,\n",
      "        -0.0252, -0.0468]) tensor([-0.4602, -0.1056, -0.2014, -0.0714, -0.1340,  0.1355, -0.3377, -0.0099,\n",
      "        -0.0021, -0.2953])\n",
      "R[0]\n",
      "tensor([0.0001], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.08292375074326992; R = 0.0012237513208909832;                 Gamma = 0.012485205339093227; Q = 0.009805194333195686;\n",
      "Entropy Neighbor = 0.008444456067867577;                 Entropy Random = 0.005364344335626811;                 Volume = 0.0009417972303926945; VAE = 9.616408264160157\n",
      "Average (on the epoch) training loss: 0.014078125769365579\n",
      "Episode average V value: 0\n",
      "epoch 1:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"params\")\n",
    "except Exception:\n",
    "    pass\n",
    "dump(vars(parameters), \"params/\" + fname + \".jldump\")\n",
    "#agent.gathering_data=False\n",
    "#agent.setNetwork('test', nEpoch=7)\n",
    "agent.run(parameters.epochs, parameters.steps_per_epoch)\n",
    "\n",
    "# --- Show results ---\n",
    "basename = \"scores/\" + fname\n",
    "scores = load(basename + \"_scores.jldump\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.setNetwork(fname, nEpoch=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._in_episode = True\n",
    "agent._mode = 0 # Testing mode with plan_depth=0\n",
    "initState = env.reset(agent._mode)\n",
    "inputDims = env.inputDimensions()\n",
    "\n",
    "for i in range(len(inputDims)):\n",
    "    if inputDims[i][0] > 1:\n",
    "        agent._state[i][1:] = initState[i][1:]\n",
    "agent._Vs_on_last_episode = []\n",
    "is_terminal = False\n",
    "reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for i in range(100):\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    plt.figure()\n",
    "    plt.imshow(np.flip(_obs.squeeze()))\n",
    "    plt.show()\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "    V, action, reward, _ = agent._step()\n",
    "    print(action)\n",
    "    agent._Vs_on_last_episode.append(V)\n",
    "    is_terminal = env.inTerminalState()\n",
    "    if is_terminal: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "obs = env.observe()\n",
    "_obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "_obs = np.flip(_obs.squeeze())\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "im = ax.imshow(np.zeros(_obs.shape))\n",
    "\n",
    "def init():\n",
    "    plt.cla()\n",
    "    im = ax.imshow(_obs)\n",
    "    return [im]\n",
    "\n",
    "def animate(i, *args, **kwargs):\n",
    "    plt.cla()\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    _obs = np.flip(_obs.squeeze())\n",
    "    im = ax.imshow(_obs)\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "        V, action, reward, _ = agent._step()\n",
    "        agent._Vs_on_last_episode.append(V)\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init, \n",
    "     frames=100, blit=False, repeat=True)\n",
    "ani.save(f'figs/{fname}/behavior.gif', writer=\"ffmpeg\", fps = 15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
