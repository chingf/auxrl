{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import hash, dump, load\n",
    "import os\n",
    "\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.CRAR_torch import CRAR\n",
    "from figure8_env import MyEnv as figure8_env\n",
    "import deer.experiment.base_controllers as bc\n",
    "\n",
    "from deer.policies import EpsilonGreedyPolicy, FixedFigure8Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure8_give_rewards = True\n",
    "nn_yaml = 'network_noconv.yaml'\n",
    "higher_dim_obs = False\n",
    "internal_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    steps_per_epoch = 5000\n",
    "    epochs = 50\n",
    "    steps_per_test = 1000\n",
    "    period_btw_summary_perfs = 1\n",
    "\n",
    "    # ----------------------\n",
    "    # Temporal Processing Parameters\n",
    "    # ----------------------\n",
    "    nstep = 20\n",
    "    nstep_decay = 0.8\n",
    "    recurrent = False\n",
    "    \n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    frame_skip = 2\n",
    "    show_rewards = False\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    update_rule = 'rmsprop'\n",
    "    learning_rate = 1 * 1E-4\n",
    "    learning_rate_decay = 0.9\n",
    "    discount = 0.9\n",
    "    discount_inc = 1\n",
    "    discount_max = 0.99\n",
    "    rms_decay = 0.9\n",
    "    rms_epsilon = 0.0001\n",
    "    momentum = 0\n",
    "    clip_norm = 1.0\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 1.0\n",
    "    epsilon_decay = 1000\n",
    "    update_frequency = 1\n",
    "    replay_memory_size = 50000 #replacing with 200000 will works just fine (in case you dont have 18gb of memory)\n",
    "    batch_size = 64 #32\n",
    "    freeze_interval = 1000\n",
    "    deterministic = False\n",
    "    \n",
    "    # ----------------------\n",
    "    # Learning algo parameters\n",
    "    # ----------------------\n",
    "    #loss_weights = [5E-3, 1E-3, 5E-3, 5E-3, 5E-3, 5E-3, 1.]\n",
    "    #loss_weights = [0, 0, 0, 0, 0, 0, 1.]\n",
    "    loss_weights = [5E-3, 5E-4, 1E-3, 1E-3, 1E-3, 1E-3, 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters hash is: 62977be8e45d8a56a5537c11dfd5d2fd8dda69e0\n",
      "The parameters are: <__main__.Defaults object at 0x28f3b8f70>\n",
      "end gathering data\n"
     ]
    }
   ],
   "source": [
    "parameters = Defaults()\n",
    "if parameters.deterministic:\n",
    "    rng = np.random.RandomState(123456)\n",
    "else:\n",
    "    rng = np.random.RandomState()\n",
    "\n",
    "# --- Instantiate environment ---\n",
    "env = figure8_env(\n",
    "    give_rewards=figure8_give_rewards,\n",
    "    intern_dim=internal_dim,\n",
    "    higher_dim_obs=higher_dim_obs,\n",
    "    show_rewards=parameters.show_rewards,\n",
    "    nstep=parameters.nstep, nstep_decay=parameters.nstep_decay\n",
    "    )\n",
    "\n",
    "# --- Instantiate learning_algo ---\n",
    "learning_algo = CRAR(\n",
    "    env,\n",
    "    parameters.rms_decay,\n",
    "    parameters.rms_epsilon,\n",
    "    parameters.momentum,\n",
    "    parameters.clip_norm,\n",
    "    parameters.freeze_interval,\n",
    "    parameters.batch_size,\n",
    "    parameters.update_rule,\n",
    "    rng,\n",
    "    high_int_dim=False,\n",
    "    internal_dim=internal_dim, lr=parameters.learning_rate,\n",
    "    nn_yaml=nn_yaml, double_Q=True,\n",
    "    loss_weights=parameters.loss_weights,\n",
    "    nstep=parameters.nstep, nstep_decay=parameters.nstep_decay,\n",
    "    recurrent=parameters.recurrent\n",
    "    )\n",
    "\n",
    "if figure8_give_rewards:\n",
    "    train_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.2,\n",
    "        consider_valid_transitions=False\n",
    "        )\n",
    "    test_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.\n",
    "        )\n",
    "else:\n",
    "    train_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng, epsilon=0.2,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "    test_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "\n",
    "# --- Instantiate agent ---\n",
    "agent = NeuralAgent(\n",
    "    env,\n",
    "    learning_algo,\n",
    "    parameters.replay_memory_size,\n",
    "    1,\n",
    "    parameters.batch_size,\n",
    "    rng,\n",
    "    train_policy=train_policy,\n",
    "    test_policy=test_policy)\n",
    "\n",
    "# --- Create unique filename for FindBestController ---\n",
    "h = hash(vars(parameters), hash_name=\"sha1\")\n",
    "fname = \"test_\" + h\n",
    "print(\"The parameters hash is: {}\".format(h))\n",
    "print(\"The parameters are: {}\".format(parameters))\n",
    "\n",
    "# As for the discount factor and the learning rate, one can update periodically the parameter of the epsilon-greedy\n",
    "# policy implemented by the agent. This controllers has a bit more capabilities, as it allows one to choose more\n",
    "# precisely when to update epsilon: after every X action, episode or epoch. This parameter can also be reset every\n",
    "# episode or epoch (or never, hence the resetEvery='none').\n",
    "agent.attach(bc.EpsilonController(\n",
    "    initial_e=parameters.epsilon_start,\n",
    "    e_decays=parameters.epsilon_decay,\n",
    "    e_min=parameters.epsilon_min,\n",
    "    evaluate_on='episode',\n",
    "    periodicity=1,\n",
    "    reset_every='none'))\n",
    "\n",
    "agent.run(10, 500)\n",
    "print(\"end gathering data\")\n",
    "\n",
    "# --- Bind controllers to the agent ---\n",
    "# Before every training epoch (periodicity=1), we want to print a summary of the agent's epsilon, discount and \n",
    "# learning rate as well as the training epoch number.\n",
    "agent.attach(bc.VerboseController(\n",
    "    evaluate_on='epoch', \n",
    "    periodicity=1))\n",
    "\n",
    "# Every epoch end, one has the possibility to modify the learning rate using a LearningRateController. Here we \n",
    "# wish to update the learning rate after every training epoch (periodicity=1), according to the parameters given.\n",
    "agent.attach(bc.LearningRateController(\n",
    "    initial_learning_rate=parameters.learning_rate, \n",
    "    learning_rate_decay=parameters.learning_rate_decay,\n",
    "    periodicity=1))\n",
    "\n",
    "# Same for the discount factor.\n",
    "agent.attach(bc.DiscountFactorController(\n",
    "    initial_discount_factor=parameters.discount, \n",
    "    discount_factor_growth=parameters.discount_inc, \n",
    "    discount_factor_max=parameters.discount_max,\n",
    "    periodicity=1))\n",
    "\n",
    "# During training epochs, we want to train the agent after every [parameters.update_frequency] action it takes.\n",
    "# Plus, we also want to display after each training episode (!= than after every training) the average bellman\n",
    "# residual and the average of the V values obtained during the last episode, hence the two last arguments.\n",
    "agent.attach(bc.TrainerController(\n",
    "    evaluate_on='action', \n",
    "    periodicity=parameters.update_frequency, \n",
    "    show_episode_avg_V_value=True, \n",
    "    show_avg_Bellman_residual=True))\n",
    "\n",
    "# We wish to discover, among all versions of our neural network (i.e., after every training epoch), which one \n",
    "# has the highest validation score.\n",
    "# To achieve this goal, one can use the FindBestController along with an InterleavedTestEpochControllers. It is \n",
    "# important that the validationID is the same than the id argument of the InterleavedTestEpochController.\n",
    "# The FindBestController will dump on disk the validation scores for each and every network, as well as the \n",
    "# structure of the neural network having the best validation score. These dumps can then used to plot the evolution \n",
    "# of the validation and test scores (see below) or simply recover the resulting neural network for your \n",
    "# application.\n",
    "agent.attach(bc.FindBestController(\n",
    "    validationID=figure8_env.VALIDATION_MODE,\n",
    "    testID=None,\n",
    "    unique_fname=fname))\n",
    "\n",
    "# All previous controllers control the agent during the epochs it goes through. However, we want to interleave a \n",
    "# \"validation epoch\" between each training epoch. For each validation epoch, we want also to display the sum of all \n",
    "# rewards obtained, hence the showScore=True. Finally, we want to call the summarizePerformance method of ALE_env \n",
    "# every [parameters.period_btw_summary_perfs] *validation* epochs.\n",
    "agent.attach(bc.InterleavedTestEpochController(\n",
    "    id=figure8_env.VALIDATION_MODE, \n",
    "    epoch_length=parameters.steps_per_test,\n",
    "    periodicity=1,\n",
    "    show_score=True,\n",
    "    summarize_every=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0996,  0.0964, -0.1624]) tensor([-0.2874,  0.0526,  0.1991]) tensor([-0.0961,  0.0991, -0.1758])\n",
      "R[0]\n",
      "tensor([0.0118], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012901472488418221 0.007999631728569512 0.11310826613381506 0.01263221628195606 0.6901085186004638 0.0001275363713502884 0.20870905995368957\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0265,  0.3438, -0.5351]) tensor([ 0.0433,  0.3432, -0.5494]) tensor([ 0.0254,  0.3229, -0.5310])\n",
      "R[0]\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.009411494599655271 0.0078237061916152 0.029773729108972476 0.012326748033752664 0.6022542955875396 0.002802106134593487 0.1512109296619892\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2158,  0.6744, -0.2962]) tensor([-0.2131,  0.6769, -0.2922]) tensor([-0.2659,  0.5538, -0.2296])\n",
      "R[0]\n",
      "tensor([0.0165], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.008924061880912632 0.007028427020210074 0.024922231168719007 0.007130646944278851 0.6343701566457749 0.005630140826106071 0.1585765474587679\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4587, -0.5323, -0.3374]) tensor([-0.4553, -0.5097, -0.3203]) tensor([-0.4664, -0.4598, -0.3871])\n",
      "R[0]\n",
      "tensor([-0.0056], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.011459648940246552 0.0068707478417200036 0.02331398404727224 0.004549146933772136 0.6229290294647217 0.010066092781722545 0.15772821286320687\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1594,  0.0532, -0.6340]) tensor([-0.1642,  0.0423, -0.6393]) tensor([-0.1077,  0.1977, -0.7020])\n",
      "R[0]\n",
      "tensor([-0.0034], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012389060341287405 0.006976357250503497 0.020009238545491827 0.004952238440397196 0.6077808855772019 0.017402726761996747 0.14249573431909085\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1278,  0.1444, -0.7359]) tensor([-0.1318,  0.1377, -0.7383]) tensor([-0.0810,  0.2115, -0.8838])\n",
      "R[0]\n",
      "tensor([-0.0016], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.011580998648423702 0.00602491741128324 0.020589887983340303 0.0038553233296843245 0.6110412460565567 0.017459204360842705 0.1524287771731615\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0532,  0.3396, -0.9314]) tensor([-0.0459,  0.3114, -0.9242]) tensor([-0.0414,  0.3596, -1.0256])\n",
      "R[0]\n",
      "tensor([0.0073], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.011353556845337152 0.006494034883129643 0.018862148860178424 0.004216978763230145 0.6084733186364174 0.011853145476430654 0.14613115309178829\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4213, -0.3647, -0.3847]) tensor([-0.4134, -0.3663, -0.3831]) tensor([-0.4019, -0.4141, -0.3968])\n",
      "R[0]\n",
      "tensor([0.0111], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.011108865004964172 0.005737663159132353 0.018438241778698284 0.003609356256376486 0.606323594212532 0.010318470604717732 0.1501032825857401\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3775, -0.1024, -0.6496]) tensor([-0.3686, -0.0914, -0.6292]) tensor([-0.2855, -0.0134, -0.8000])\n",
      "R[0]\n",
      "tensor([-0.0186], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012082814838737249 0.00633369358125492 0.01713305447023595 0.004537284748628735 0.5891934077143669 0.00863741073384881 0.13750089228153228\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2531,  0.1156, -0.8647]) tensor([-0.2446,  0.1026, -0.8458]) tensor([-0.2439,  0.0652, -0.7522])\n",
      "R[0]\n",
      "tensor([-0.0123], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.011765609190333634 0.006038198269408894 0.015790555082028732 0.004341378853365313 0.5864601758718491 0.007953959934413434 0.14089459851384162\n",
      "Average (on the epoch) training loss: 0.006215131858544191\n",
      "Episode average V value: 0\n",
      "epoch 1:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:336: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  ax.scatter(\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:374: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:382: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2781,  0.0198, -0.6974]) tensor([-0.2689,  0.0327, -0.6997]) tensor([-0.2854, -0.0008, -0.6784])\n",
      "R[0]\n",
      "tensor([0.0052], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012427992668002845 0.006476919812062988 0.016147484902729048 0.004919407941051759 0.5714043018221855 0.004975842606276273 0.13135291451215744\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0223,  0.4537, -1.1545]) tensor([ 0.0203,  0.4560, -1.1413]) tensor([ 0.0514,  0.3688, -1.1257])\n",
      "R[0]\n",
      "tensor([0.0190], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012601699494756759 0.006530315133655677 0.01513425464744796 0.004981928060646169 0.5647564767599106 0.0046478999070823195 0.13355037645995618\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6889, -0.6548, -0.0411]) tensor([-0.6766, -0.6640, -0.0164]) tensor([-0.6694, -0.6591,  0.0077])\n",
      "R[0]\n",
      "tensor([-0.0097], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01284451373759657 0.005564608504602802 0.015095691794966115 0.005088307056576013 0.5579439818263053 0.004854904238134622 0.125545765042305\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0952,  0.3502, -0.9299]) tensor([-0.1120,  0.3171, -0.8747]) tensor([-0.1099,  0.3299, -0.9832])\n",
      "R[0]\n",
      "tensor([-0.0056], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01236533604003489 0.005867163333037752 0.014592882544355234 0.004911686632782221 0.555759526193142 0.004205017413944006 0.1303670925423503\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3972, -0.0492, -0.6037]) tensor([-0.3912, -0.0387, -0.5895]) tensor([-0.3325,  0.0721, -0.7197])\n",
      "R[0]\n",
      "tensor([-0.0030], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012523086606524885 0.005471465340888244 0.014289914753346239 0.0051972279633628205 0.5469539915919304 0.0035035026706755163 0.1242266922518611\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5108, -0.2386, -0.4433]) tensor([-0.4980, -0.2117, -0.4520]) tensor([-0.5855, -0.3977, -0.2792])\n",
      "R[0]\n",
      "tensor([-0.0110], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012507672269828617 0.005298613731662044 0.014410652869119077 0.004744052133290097 0.5477927834391594 0.003496411394327879 0.12650039479136466\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5086, -0.1976, -0.4982]) tensor([-0.4944, -0.2158, -0.4468]) tensor([-0.5766, -0.3447, -0.3529])\n",
      "R[0]\n",
      "tensor([-0.0003], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.011993957974947988 0.004715337161047501 0.01384689721907489 0.004881432446883991 0.5479096418619156 0.0037745952904224395 0.12209110152721404\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2906,  0.7459, -0.5179]) tensor([-0.2668,  0.6166, -0.5019]) tensor([-0.2621,  0.7145, -0.5861])\n",
      "R[0]\n",
      "tensor([-0.0122], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0120124115915969 0.004915118254342815 0.012308419375796803 0.004777933832490817 0.5485694867968559 0.0038761633150279523 0.11959273695200681\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3804, -0.0113, -0.6633]) tensor([-0.3781, -0.0054, -0.6589]) tensor([-0.3902, -0.0575, -0.6176])\n",
      "R[0]\n",
      "tensor([-5.3478e-05], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012168046356178821 0.00494195623206906 0.01170071885950165 0.004981495724292472 0.5468821058869362 0.003036972478032112 0.115497639849782\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2496,  0.2596, -0.9428]) tensor([-0.2420,  0.2816, -0.9245]) tensor([-0.2367,  0.3459, -0.9979])\n",
      "R[0]\n",
      "tensor([0.0055], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012087381851859392 0.005188052824771148 0.01250921539562114 0.0049922462975373495 0.5476881121397018 0.0032813497111201287 0.11212282674014569\n",
      "Average (on the epoch) training loss: 0.004947571808891371\n",
      "Episode average V value: 0\n",
      "epoch 2:\n",
      "Learning rate: 9e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 1.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.9999000099990001 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:336: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  ax.scatter(\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:374: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:382: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.4823, 0.2831, 0.3623]) tensor([0.3901, 0.2787, 0.3308]) tensor([0.3918, 0.3288, 0.3725])\n",
      "R[0]\n",
      "tensor([0.0209], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.010941057404503227 0.004694636211410397 0.012510578647619695 0.004768031530780718 0.5547410075068474 0.002462654672563076 0.11338715158402919\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0903, -0.2446, -0.2437]) tensor([-0.1465, -0.1785, -0.2656]) tensor([-0.3972, -0.3083, -0.3287])\n",
      "R[0]\n",
      "tensor([-0.0056], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.011534982113167643 0.005375332160445396 0.012246023335668724 0.004718369754147716 0.554740243434906 0.0027592805959284305 0.1132975427955389\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2713,  0.4221, -1.0465]) tensor([-0.2584,  0.4417, -1.0360]) tensor([-0.3073,  0.3709, -0.9794])\n",
      "R[0]\n",
      "tensor([0.0105], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.011111364071257412 0.005136591020986089 0.01228502501908224 0.004517965852282941 0.5566407755017281 0.002346303429454565 0.10900759471207858\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6113, -0.3952, -0.2881]) tensor([-0.6058, -0.3802, -0.2870]) tensor([-0.6037, -0.3617, -0.3402])\n",
      "R[0]\n",
      "tensor([-0.0013], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.011158308392390608 0.005503989509845269 0.011293635111287585 0.00461729375156574 0.5599400556087494 0.0029737865924835205 0.10745848586410284\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3129,  0.6765, -0.4201]) tensor([ 0.2222,  0.6047, -0.4795]) tensor([ 0.3080,  0.7230, -0.4718])\n",
      "R[0]\n",
      "tensor([0.1123], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.010930038995109498 0.005309486492580618 0.011103656819614116 0.004354876407189295 0.5584606712460518 0.0020582352951169012 0.10736025112122297\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6349, -0.4907, -0.1668]) tensor([-0.6318, -0.4696, -0.1655]) tensor([-0.6533, -0.5226, -0.1033])\n",
      "R[0]\n",
      "tensor([0.0021], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.010918798085302115 0.005269409088985412 0.010907066318570286 0.00461481053661555 0.5597106683850288 0.0019691722467541696 0.10494237247109413\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6113, -0.5010, -0.0509]) tensor([-0.6122, -0.4795, -0.0695]) tensor([-0.5843, -0.3797, -0.2212])\n",
      "R[0]\n",
      "tensor([0.0021], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.010712187673896551 0.004789422203080903 0.009693711991203599 0.004221894880291075 0.5597573854327201 0.0013068509995937346 0.10041014152020215\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6814, -0.5387, -0.1305]) tensor([-0.6621, -0.5502, -0.0902]) tensor([-0.6778, -0.5801, -0.0438])\n",
      "R[0]\n",
      "tensor([0.0011], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.010714348610490561 0.005338495621545008 0.010084032266000577 0.0046003168560564515 0.5609214992523194 0.0014000522419810296 0.10072692848742008\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0374,  0.8293, -0.8699]) tensor([-0.0472,  0.8378, -0.8584]) tensor([ 0.1001,  1.0010, -0.5476])\n",
      "R[0]\n",
      "tensor([0.0089], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01025479184370488 0.004674218447471503 0.009250749908249417 0.004361945454031229 0.5649994220733643 0.0006499970443546772 0.09738782767951489\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4982, -0.0825, -0.5758]) tensor([-0.4893, -0.0605, -0.5729]) tensor([-0.5431, -0.1196, -0.5735])\n",
      "R[0]\n",
      "tensor([-0.0051], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01068529052939266 0.005147087313409429 0.008979451122817408 0.0044647309674182905 0.5620909171104431 0.0007998600117862225 0.0989722381234169\n",
      "Average (on the epoch) training loss: 0.0045240235990379\n",
      "Episode average V value: 0\n",
      "epoch 3:\n",
      "Learning rate: 8.1e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:336: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  ax.scatter(\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:374: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:382: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4901,  0.0731, -0.6948]) tensor([-0.4809,  0.0796, -0.6872]) tensor([-0.4742,  0.0598, -0.6669])\n",
      "R[0]\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.010761789122596383 0.004859067757919547 0.010505617394286673 0.004628690513782203 0.5597462466359139 0.0004631447046995163 0.09633740218728781\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 1.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6799, -0.6227, -0.0167]) tensor([-0.6561, -0.6008, -0.0198]) tensor([-0.6744, -0.5970, -0.0637])\n",
      "R[0]\n",
      "tensor([-0.0117], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.010922032311558723 0.005139996723257354 0.010364915469035623 0.004645782719133421 0.5589279238581657 0.0008355867192149163 0.09690562590956688\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1460, -0.2724,  0.6360]) tensor([ 0.0883, -0.2513,  0.6263]) tensor([ 0.2129, -0.2131,  0.6305])\n",
      "R[0]\n",
      "tensor([-0.0256], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.011167066255584359 0.005116551003433415 0.010180190404847963 0.004717833598144352 0.5560234721899032 0.00048619306460022924 0.09924962405860424\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2850, -0.6770,  0.5811]) tensor([-0.3368, -0.6364,  0.5310]) tensor([-0.5292, -0.4984,  0.3564])\n",
      "R[0]\n",
      "tensor([-0.0123], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"params\")\n",
    "except Exception:\n",
    "    pass\n",
    "dump(vars(parameters), \"params/\" + fname + \".jldump\")\n",
    "#agent.gathering_data=False\n",
    "agent.run(parameters.epochs, parameters.steps_per_epoch)\n",
    "\n",
    "# --- Show results ---\n",
    "basename = \"scores/\" + fname\n",
    "scores = load(basename + \"_scores.jldump\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.setNetwork(fname, nEpoch=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._in_episode = True\n",
    "agent._mode = 0 # Testing mode with plan_depth=0\n",
    "initState = env.reset(agent._mode)\n",
    "inputDims = env.inputDimensions()\n",
    "\n",
    "for i in range(len(inputDims)):\n",
    "    if inputDims[i][0] > 1:\n",
    "        agent._state[i][1:] = initState[i][1:]\n",
    "agent._Vs_on_last_episode = []\n",
    "is_terminal = False\n",
    "reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for i in range(100):\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    plt.figure()\n",
    "    plt.imshow(np.flip(_obs.squeeze()))\n",
    "    plt.show()\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "    V, action, reward, _ = agent._step()\n",
    "    print(action)\n",
    "    agent._Vs_on_last_episode.append(V)\n",
    "    is_terminal = env.inTerminalState()\n",
    "    if is_terminal: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "obs = env.observe()\n",
    "_obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "_obs = np.flip(_obs.squeeze())\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "im = ax.imshow(np.zeros(_obs.shape))\n",
    "\n",
    "def init():\n",
    "    plt.cla()\n",
    "    im = ax.imshow(_obs)\n",
    "    return [im]\n",
    "\n",
    "def animate(i, *args, **kwargs):\n",
    "    plt.cla()\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    _obs = np.flip(_obs.squeeze())\n",
    "    im = ax.imshow(_obs)\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "        V, action, reward, _ = agent._step()\n",
    "        agent._Vs_on_last_episode.append(V)\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init, \n",
    "     frames=100, blit=False, repeat=True)\n",
    "ani.save('behavior.gif', writer=\"ffmpeg\", fps = 15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
