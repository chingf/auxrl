{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import hash, dump, load\n",
    "import os\n",
    "\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.CRAR_torch import CRAR\n",
    "from figure8_env import MyEnv as figure8_env\n",
    "import deer.experiment.base_controllers as bc\n",
    "\n",
    "from deer.policies import EpsilonGreedyPolicy, FixedFigure8Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure8_give_rewards = True\n",
    "nn_yaml = 'network_rnn.yaml'\n",
    "#nn_yaml = 'network_noconv.yaml'\n",
    "higher_dim_obs = False\n",
    "internal_dim = 3\n",
    "nstep = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    steps_per_epoch = 5000\n",
    "    epochs = 50\n",
    "    steps_per_test = 1000\n",
    "    period_btw_summary_perfs = 1\n",
    "    \n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    frame_skip = 2\n",
    "    show_rewards = False\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    update_rule = 'rmsprop'\n",
    "    learning_rate = 5 * 1E-4 # 1E-4\n",
    "    learning_rate_decay = 0.9\n",
    "    discount = 0.9\n",
    "    discount_inc = 1\n",
    "    discount_max = 0.99\n",
    "    rms_decay = 0.9\n",
    "    rms_epsilon = 0.0001\n",
    "    momentum = 0\n",
    "    clip_norm = 1.0\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 1.0\n",
    "    epsilon_decay = 10000\n",
    "    update_frequency = 1\n",
    "    replay_memory_size = 1000000 #replacing with 200000 will works just fine (in case you dont have 18gb of memory)\n",
    "    batch_size = 32\n",
    "    freeze_interval = 1000\n",
    "    deterministic = False\n",
    "    \n",
    "    # ----------------------\n",
    "    # Learning algo parameters\n",
    "    # ----------------------\n",
    "    loss_weights = [1E-2, 1E-3, 1E-3, 1E-3, 1E-3, 1E-3, 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters hash is: 62977be8e45d8a56a5537c11dfd5d2fd8dda69e0\n",
      "The parameters are: <__main__.Defaults object at 0x147e1fa00>\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "end gathering data\n"
     ]
    }
   ],
   "source": [
    "parameters = Defaults()\n",
    "if parameters.deterministic:\n",
    "    rng = np.random.RandomState(123456)\n",
    "else:\n",
    "    rng = np.random.RandomState()\n",
    "\n",
    "# --- Instantiate environment ---\n",
    "env = figure8_env(\n",
    "    give_rewards=figure8_give_rewards,\n",
    "    intern_dim=internal_dim,\n",
    "    higher_dim_obs=higher_dim_obs,\n",
    "    show_rewards=parameters.show_rewards\n",
    "    )\n",
    "\n",
    "# --- Instantiate learning_algo ---\n",
    "learning_algo = CRAR(\n",
    "    env,\n",
    "    parameters.rms_decay,\n",
    "    parameters.rms_epsilon,\n",
    "    parameters.momentum,\n",
    "    parameters.clip_norm,\n",
    "    parameters.freeze_interval,\n",
    "    parameters.batch_size,\n",
    "    parameters.update_rule,\n",
    "    rng,\n",
    "    high_int_dim=False,\n",
    "    internal_dim=internal_dim, lr=parameters.learning_rate,\n",
    "    nn_yaml=nn_yaml, double_Q=True,\n",
    "    loss_weights=parameters.loss_weights,\n",
    "    nstep=nstep\n",
    "    )\n",
    "\n",
    "if figure8_give_rewards:\n",
    "    train_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.2,\n",
    "        consider_valid_transitions=False\n",
    "        )\n",
    "    test_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.\n",
    "        )\n",
    "else:\n",
    "    train_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng, epsilon=0.2,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "    test_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "\n",
    "# --- Instantiate agent ---\n",
    "agent = NeuralAgent(\n",
    "    env,\n",
    "    learning_algo,\n",
    "    parameters.replay_memory_size,\n",
    "    1,\n",
    "    parameters.batch_size,\n",
    "    rng,\n",
    "    train_policy=train_policy,\n",
    "    test_policy=test_policy)\n",
    "\n",
    "# --- Create unique filename for FindBestController ---\n",
    "h = hash(vars(parameters), hash_name=\"sha1\")\n",
    "fname = \"test_\" + h\n",
    "print(\"The parameters hash is: {}\".format(h))\n",
    "print(\"The parameters are: {}\".format(parameters))\n",
    "\n",
    "# As for the discount factor and the learning rate, one can update periodically the parameter of the epsilon-greedy\n",
    "# policy implemented by the agent. This controllers has a bit more capabilities, as it allows one to choose more\n",
    "# precisely when to update epsilon: after every X action, episode or epoch. This parameter can also be reset every\n",
    "# episode or epoch (or never, hence the resetEvery='none').\n",
    "agent.attach(bc.EpsilonController(\n",
    "    initial_e=parameters.epsilon_start,\n",
    "    e_decays=parameters.epsilon_decay,\n",
    "    e_min=parameters.epsilon_min,\n",
    "    evaluate_on='episode',\n",
    "    periodicity=1,\n",
    "    reset_every='none'))\n",
    "\n",
    "agent.run(10, 500)\n",
    "print(\"end gathering data\")\n",
    "\n",
    "# --- Bind controllers to the agent ---\n",
    "# Before every training epoch (periodicity=1), we want to print a summary of the agent's epsilon, discount and \n",
    "# learning rate as well as the training epoch number.\n",
    "agent.attach(bc.VerboseController(\n",
    "    evaluate_on='epoch', \n",
    "    periodicity=1))\n",
    "\n",
    "# Every epoch end, one has the possibility to modify the learning rate using a LearningRateController. Here we \n",
    "# wish to update the learning rate after every training epoch (periodicity=1), according to the parameters given.\n",
    "agent.attach(bc.LearningRateController(\n",
    "    initial_learning_rate=parameters.learning_rate, \n",
    "    learning_rate_decay=parameters.learning_rate_decay,\n",
    "    periodicity=1))\n",
    "\n",
    "# Same for the discount factor.\n",
    "agent.attach(bc.DiscountFactorController(\n",
    "    initial_discount_factor=parameters.discount, \n",
    "    discount_factor_growth=parameters.discount_inc, \n",
    "    discount_factor_max=parameters.discount_max,\n",
    "    periodicity=1))\n",
    "\n",
    "# During training epochs, we want to train the agent after every [parameters.update_frequency] action it takes.\n",
    "# Plus, we also want to display after each training episode (!= than after every training) the average bellman\n",
    "# residual and the average of the V values obtained during the last episode, hence the two last arguments.\n",
    "agent.attach(bc.TrainerController(\n",
    "    evaluate_on='action', \n",
    "    periodicity=parameters.update_frequency, \n",
    "    show_episode_avg_V_value=True, \n",
    "    show_avg_Bellman_residual=True))\n",
    "\n",
    "# We wish to discover, among all versions of our neural network (i.e., after every training epoch), which one \n",
    "# has the highest validation score.\n",
    "# To achieve this goal, one can use the FindBestController along with an InterleavedTestEpochControllers. It is \n",
    "# important that the validationID is the same than the id argument of the InterleavedTestEpochController.\n",
    "# The FindBestController will dump on disk the validation scores for each and every network, as well as the \n",
    "# structure of the neural network having the best validation score. These dumps can then used to plot the evolution \n",
    "# of the validation and test scores (see below) or simply recover the resulting neural network for your \n",
    "# application.\n",
    "agent.attach(bc.FindBestController(\n",
    "    validationID=figure8_env.VALIDATION_MODE,\n",
    "    testID=None,\n",
    "    unique_fname=fname))\n",
    "\n",
    "# All previous controllers control the agent during the epochs it goes through. However, we want to interleave a \n",
    "# \"validation epoch\" between each training epoch. For each validation epoch, we want also to display the sum of all \n",
    "# rewards obtained, hence the showScore=True. Finally, we want to call the summarizePerformance method of ALE_env \n",
    "# every [parameters.period_btw_summary_perfs] *validation* epochs.\n",
    "agent.attach(bc.InterleavedTestEpochController(\n",
    "    id=figure8_env.VALIDATION_MODE, \n",
    "    epoch_length=parameters.steps_per_test,\n",
    "    periodicity=1,\n",
    "    show_score=True,\n",
    "    summarize_every=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent._dataset.n_elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent._mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.gathering_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4825, -0.0980, -0.0048]) tensor([ 0.1886, -0.0058, -0.1392]) tensor([ 0.4828, -0.0944, -0.0029])\n",
      "R[0]\n",
      "tensor([0.1812], grad_fn=<SelectBackward0>)\n",
      "> \u001b[0;32m/Users/chingfang/Code/deer/deer/agent.py\u001b[0m(848)\u001b[0;36maddSample\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    846 \u001b[0;31m                \u001b[0;32mif\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    847 \u001b[0;31m                    \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 848 \u001b[0;31m                \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    849 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hiddens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    850 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> pdb.set_trace = lambda: None\n",
      "ipdb> c\n",
      "(tensor([[[ 4.8251e-01, -9.7965e-02, -4.8438e-03],\n",
      "         [ 5.1031e-01, -1.1737e-01, -1.9709e-02],\n",
      "         [ 5.1249e-01, -1.2306e-01, -2.5969e-02],\n",
      "         [ 4.6557e-01, -8.9939e-02,  1.1008e-02],\n",
      "         [ 4.9002e-01, -1.0402e-01, -1.0703e-02],\n",
      "         [ 4.8998e-01, -1.0826e-01, -1.0628e-02],\n",
      "         [ 4.9735e-01, -1.2378e-01, -1.5329e-02],\n",
      "         [ 4.8728e-01, -1.0293e-01, -5.9378e-03],\n",
      "         [ 4.8098e-01, -1.0380e-01,  7.3440e-04],\n",
      "         [ 4.9109e-01, -1.0446e-01, -9.7953e-03],\n",
      "         [ 4.8437e-01, -8.9121e-02, -6.9651e-04],\n",
      "         [ 4.8178e-01, -1.1422e-01, -3.7225e-03],\n",
      "         [ 5.1504e-01, -1.2320e-01, -3.1323e-02],\n",
      "         [ 5.1149e-01, -1.1606e-01, -2.3320e-02],\n",
      "         [ 5.1622e-01, -1.2450e-01, -2.8754e-02],\n",
      "         [ 4.8541e-01, -1.1812e-01, -8.2127e-03],\n",
      "         [ 4.9621e-01, -1.1493e-01, -1.2247e-02],\n",
      "         [ 5.0289e-01, -1.2335e-01, -2.1089e-02],\n",
      "         [ 4.6889e-01, -8.5577e-02,  1.3578e-02],\n",
      "         [ 5.1406e-01, -1.2391e-01, -2.5754e-02],\n",
      "         [ 4.8583e-01, -9.2163e-02, -1.8527e-04],\n",
      "         [ 5.0412e-01, -1.2754e-01, -2.1367e-02],\n",
      "         [ 5.0374e-01, -1.2574e-01, -1.9810e-02],\n",
      "         [ 4.9044e-01, -1.0625e-01, -1.1462e-02],\n",
      "         [ 4.9405e-01, -1.0199e-01, -1.0277e-02],\n",
      "         [ 4.9471e-01, -1.0245e-01, -1.0349e-02],\n",
      "         [ 4.8426e-01, -1.1396e-01, -2.8956e-03],\n",
      "         [ 4.8672e-01, -1.0552e-01, -5.1862e-03],\n",
      "         [ 4.7667e-01, -8.9211e-02,  8.3072e-03],\n",
      "         [ 4.9703e-01, -1.2354e-01, -2.1269e-02],\n",
      "         [ 4.9679e-01, -1.2022e-01, -1.3650e-02],\n",
      "         [ 5.1344e-01, -1.2405e-01, -2.5651e-02]]], grad_fn=<StackBackward0>), tensor([[[ 1.2145e+00, -2.1548e-01, -9.9347e-03],\n",
      "         [ 1.2261e+00, -2.5998e-01, -4.3608e-02],\n",
      "         [ 1.2448e+00, -2.7766e-01, -5.7215e-02],\n",
      "         [ 1.2041e+00, -1.9472e-01,  2.1756e-02],\n",
      "         [ 1.2115e+00, -2.2486e-01, -2.2499e-02],\n",
      "         [ 1.2112e+00, -2.3451e-01, -2.2357e-02],\n",
      "         [ 1.2269e+00, -2.6853e-01, -3.2852e-02],\n",
      "         [ 1.2253e+00, -2.2242e-01, -1.2390e-02],\n",
      "         [ 1.2130e+00, -2.1976e-01,  1.5244e-03],\n",
      "         [ 1.2164e+00, -2.2552e-01, -2.0599e-02],\n",
      "         [ 1.2037e+00, -1.8975e-01, -1.4533e-03],\n",
      "         [ 1.2213e+00, -2.4450e-01, -7.7259e-03],\n",
      "         [ 1.2325e+00, -2.7836e-01, -6.9224e-02],\n",
      "         [ 1.2114e+00, -2.5877e-01, -5.1575e-02],\n",
      "         [ 1.2412e+00, -2.8095e-01, -6.3563e-02],\n",
      "         [ 1.2388e+00, -2.5404e-01, -1.7049e-02],\n",
      "         [ 1.2239e+00, -2.4769e-01, -2.6211e-02],\n",
      "         [ 1.2223e+00, -2.6894e-01, -4.5937e-02],\n",
      "         [ 1.2271e+00, -1.8457e-01,  2.6822e-02],\n",
      "         [ 1.2558e+00, -2.7986e-01, -5.6735e-02],\n",
      "         [ 1.2135e+00, -1.9651e-01, -3.8670e-04],\n",
      "         [ 1.2301e+00, -2.7892e-01, -4.6567e-02],\n",
      "         [ 1.2255e+00, -2.7399e-01, -4.3189e-02],\n",
      "         [ 1.2131e+00, -2.3005e-01, -2.4101e-02],\n",
      "         [ 1.2136e+00, -2.2570e-01, -2.1612e-02],\n",
      "         [ 1.2178e+00, -2.2683e-01, -2.1763e-02],\n",
      "         [ 1.2293e+00, -2.4310e-01, -6.0169e-03],\n",
      "         [ 1.2225e+00, -2.2820e-01, -1.0827e-02],\n",
      "         [ 1.2220e+00, -1.8894e-01,  1.7071e-02],\n",
      "         [ 1.2256e+00, -2.7034e-01, -4.5589e-02],\n",
      "         [ 1.2249e+00, -2.5997e-01, -2.9242e-02],\n",
      "         [ 1.2517e+00, -2.8012e-01, -5.6514e-02]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5207, -0.1338, -0.0321],\n",
      "         [ 0.4862, -0.1144, -0.0114],\n",
      "         [ 0.4856, -0.0948, -0.0085],\n",
      "         [ 0.5082, -0.1268, -0.0244],\n",
      "         [ 0.5074, -0.1231, -0.0315],\n",
      "         [ 0.5080, -0.1202, -0.0314],\n",
      "         [ 0.4987, -0.1136, -0.0169],\n",
      "         [ 0.5217, -0.1355, -0.0334],\n",
      "         [ 0.4882, -0.1060, -0.0117],\n",
      "         [ 0.5081, -0.1274, -0.0239],\n",
      "         [ 0.5178, -0.1343, -0.0317],\n",
      "         [ 0.5021, -0.1325, -0.0199],\n",
      "         [ 0.5151, -0.1332, -0.0319],\n",
      "         [ 0.4863, -0.1035, -0.0084],\n",
      "         [ 0.4874, -0.0957, -0.0026],\n",
      "         [ 0.4948, -0.1086, -0.0156],\n",
      "         [ 0.5074, -0.1147, -0.0198],\n",
      "         [ 0.4848, -0.1009, -0.0080],\n",
      "         [ 0.4875, -0.1050, -0.0091],\n",
      "         [ 0.5078, -0.1249, -0.0315],\n",
      "         [ 0.5009, -0.1128, -0.0138],\n",
      "         [ 0.4866, -0.1031, -0.0042],\n",
      "         [ 0.5212, -0.1351, -0.0333],\n",
      "         [ 0.5167, -0.1329, -0.0306],\n",
      "         [ 0.4851, -0.1021, -0.0079],\n",
      "         [ 0.5180, -0.1300, -0.0341],\n",
      "         [ 0.4852, -0.0964, -0.0113],\n",
      "         [ 0.5000, -0.1282, -0.0175],\n",
      "         [ 0.5225, -0.1345, -0.0351],\n",
      "         [ 0.4634, -0.0882,  0.0073],\n",
      "         [ 0.5073, -0.1229, -0.0306],\n",
      "         [ 0.4837, -0.1081, -0.0140]]], grad_fn=<StackBackward0>), tensor([[[ 1.2471, -0.3036, -0.0724],\n",
      "         [ 1.2109, -0.2448, -0.0241],\n",
      "         [ 1.2114, -0.2081, -0.0175],\n",
      "         [ 1.2216, -0.2763, -0.0539],\n",
      "         [ 1.1990, -0.2733, -0.0692],\n",
      "         [ 1.2026, -0.2666, -0.0689],\n",
      "         [ 1.2039, -0.2444, -0.0366],\n",
      "         [ 1.2546, -0.3084, -0.0754],\n",
      "         [ 1.2107, -0.2301, -0.0244],\n",
      "         [ 1.2214, -0.2776, -0.0528],\n",
      "         [ 1.2566, -0.3037, -0.0706],\n",
      "         [ 1.2284, -0.2886, -0.0433],\n",
      "         [ 1.2385, -0.3007, -0.0711],\n",
      "         [ 1.2038, -0.2239, -0.0176],\n",
      "         [ 1.2062, -0.2044, -0.0054],\n",
      "         [ 1.2110, -0.2420, -0.0330],\n",
      "         [ 1.2023, -0.2509, -0.0434],\n",
      "         [ 1.2059, -0.2219, -0.0164],\n",
      "         [ 1.2110, -0.2275, -0.0190],\n",
      "         [ 1.2015, -0.2778, -0.0692],\n",
      "         [ 1.2160, -0.2410, -0.0300],\n",
      "         [ 1.2017, -0.2168, -0.0089],\n",
      "         [ 1.2513, -0.3074, -0.0752],\n",
      "         [ 1.2478, -0.2997, -0.0683],\n",
      "         [ 1.2081, -0.2248, -0.0163],\n",
      "         [ 1.2228, -0.2943, -0.0767],\n",
      "         [ 1.2115, -0.2126, -0.0232],\n",
      "         [ 1.2194, -0.2795, -0.0381],\n",
      "         [ 1.2491, -0.3058, -0.0789],\n",
      "         [ 1.2006, -0.1913,  0.0144],\n",
      "         [ 1.1992, -0.2726, -0.0673],\n",
      "         [ 1.2066, -0.2410, -0.0288]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5075, -0.1396, -0.0259],\n",
      "         [ 0.4969, -0.1181, -0.0183],\n",
      "         [ 0.4911, -0.1143, -0.0149],\n",
      "         [ 0.4924, -0.1285, -0.0193],\n",
      "         [ 0.5293, -0.1436, -0.0432],\n",
      "         [ 0.5071, -0.1366, -0.0256],\n",
      "         [ 0.5181, -0.1374, -0.0315],\n",
      "         [ 0.4860, -0.1020, -0.0097],\n",
      "         [ 0.5078, -0.1384, -0.0250],\n",
      "         [ 0.4920, -0.1286, -0.0194],\n",
      "         [ 0.4918, -0.1168, -0.0168],\n",
      "         [ 0.5206, -0.1397, -0.0339],\n",
      "         [ 0.5108, -0.1422, -0.0281],\n",
      "         [ 0.5201, -0.1428, -0.0369],\n",
      "         [ 0.5234, -0.1302, -0.0357],\n",
      "         [ 0.4878, -0.1292, -0.0117],\n",
      "         [ 0.5219, -0.1219, -0.0322],\n",
      "         [ 0.5217, -0.1190, -0.0326],\n",
      "         [ 0.4604, -0.0920,  0.0064],\n",
      "         [ 0.5304, -0.1432, -0.0381],\n",
      "         [ 0.5034, -0.1310, -0.0196],\n",
      "         [ 0.5065, -0.1383, -0.0257],\n",
      "         [ 0.5124, -0.1440, -0.0315],\n",
      "         [ 0.5217, -0.1409, -0.0342],\n",
      "         [ 0.5206, -0.1309, -0.0404],\n",
      "         [ 0.5281, -0.1434, -0.0364],\n",
      "         [ 0.5238, -0.1393, -0.0397],\n",
      "         [ 0.5316, -0.1462, -0.0412],\n",
      "         [ 0.4958, -0.1042, -0.0200],\n",
      "         [ 0.4858, -0.1184,  0.0011],\n",
      "         [ 0.5089, -0.1421, -0.0264],\n",
      "         [ 0.5316, -0.1447, -0.0390]]], grad_fn=<StackBackward0>), tensor([[[ 1.2240, -0.3059, -0.0572],\n",
      "         [ 1.2128, -0.2644, -0.0387],\n",
      "         [ 1.2076, -0.2487, -0.0315],\n",
      "         [ 1.2160, -0.2778, -0.0412],\n",
      "         [ 1.2436, -0.3292, -0.0995],\n",
      "         [ 1.2216, -0.2985, -0.0566],\n",
      "         [ 1.2310, -0.3093, -0.0712],\n",
      "         [ 1.2006, -0.2244, -0.0201],\n",
      "         [ 1.2267, -0.3027, -0.0553],\n",
      "         [ 1.2136, -0.2778, -0.0414],\n",
      "         [ 1.2071, -0.2544, -0.0356],\n",
      "         [ 1.2503, -0.3163, -0.0766],\n",
      "         [ 1.2425, -0.3128, -0.0621],\n",
      "         [ 1.2487, -0.3251, -0.0833],\n",
      "         [ 1.1979, -0.2899, -0.0817],\n",
      "         [ 1.2547, -0.2839, -0.0245],\n",
      "         [ 1.1929, -0.2692, -0.0735],\n",
      "         [ 1.1909, -0.2623, -0.0744],\n",
      "         [ 1.1959, -0.1998,  0.0125],\n",
      "         [ 1.2555, -0.3274, -0.0884],\n",
      "         [ 1.2069, -0.2850, -0.0432],\n",
      "         [ 1.2268, -0.3045, -0.0565],\n",
      "         [ 1.2550, -0.3182, -0.0697],\n",
      "         [ 1.2570, -0.3194, -0.0772],\n",
      "         [ 1.1963, -0.2947, -0.0922],\n",
      "         [ 1.2434, -0.3272, -0.0844],\n",
      "         [ 1.2137, -0.3173, -0.0913],\n",
      "         [ 1.2651, -0.3362, -0.0956],\n",
      "         [ 1.2040, -0.2316, -0.0424],\n",
      "         [ 1.2428, -0.2553,  0.0023],\n",
      "         [ 1.2325, -0.3120, -0.0583],\n",
      "         [ 1.2613, -0.3314, -0.0905]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 4.5739e-01, -8.9655e-02,  9.2837e-03],\n",
      "         [ 4.8616e-01, -1.1411e-01, -4.3527e-03],\n",
      "         [ 5.3144e-01, -1.5103e-01, -5.4260e-02],\n",
      "         [ 5.2939e-01, -1.3524e-01, -4.9201e-02],\n",
      "         [ 4.9766e-01, -1.4200e-01, -2.2558e-02],\n",
      "         [ 4.9575e-01, -1.0663e-01, -2.1650e-02],\n",
      "         [ 5.2319e-01, -1.5260e-01, -3.9602e-02],\n",
      "         [ 5.3915e-01, -1.5535e-01, -4.7483e-02],\n",
      "         [ 4.5985e-01, -8.7823e-02,  1.5843e-02],\n",
      "         [ 5.1706e-01, -1.4997e-01, -3.5684e-02],\n",
      "         [ 5.4006e-01, -1.5490e-01, -4.5426e-02],\n",
      "         [ 5.1237e-01, -1.4965e-01, -3.7524e-02],\n",
      "         [ 5.4050e-01, -1.5618e-01, -4.7956e-02],\n",
      "         [ 5.3941e-01, -1.5478e-01, -4.8141e-02],\n",
      "         [ 4.8111e-01, -9.5439e-02, -1.7222e-05],\n",
      "         [ 4.8712e-01, -1.3444e-01, -1.2600e-02],\n",
      "         [ 5.3832e-01, -1.5517e-01, -4.6723e-02],\n",
      "         [ 4.9731e-01, -1.2200e-01, -2.6194e-02],\n",
      "         [ 5.3821e-01, -1.5399e-01, -4.8570e-02],\n",
      "         [ 5.3916e-01, -1.5553e-01, -4.7964e-02],\n",
      "         [ 5.0539e-01, -1.3519e-01, -3.0542e-02],\n",
      "         [ 5.2319e-01, -1.5260e-01, -3.9602e-02],\n",
      "         [ 4.8686e-01, -1.1864e-01, -2.8466e-03],\n",
      "         [ 4.9598e-01, -1.1017e-01, -1.8582e-02],\n",
      "         [ 4.8546e-01, -1.2302e-01, -2.9265e-02],\n",
      "         [ 4.6057e-01, -9.9635e-02, -1.2307e-03],\n",
      "         [ 5.2846e-01, -1.5002e-01, -3.9780e-02],\n",
      "         [ 5.2631e-01, -1.4858e-01, -3.9338e-02],\n",
      "         [ 5.2319e-01, -1.5260e-01, -3.9602e-02],\n",
      "         [ 5.4008e-01, -1.5420e-01, -4.5316e-02],\n",
      "         [ 5.4000e-01, -1.5671e-01, -4.7847e-02],\n",
      "         [ 4.9470e-01, -1.1002e-01, -6.9616e-03]]], grad_fn=<StackBackward0>), tensor([[[ 1.1916e+00, -1.9415e-01,  1.8032e-02],\n",
      "         [ 1.2271e+00, -2.4527e-01, -9.1565e-03],\n",
      "         [ 1.1937e+00, -3.4806e-01, -1.2783e-01],\n",
      "         [ 1.1881e+00, -3.0757e-01, -1.1550e-01],\n",
      "         [ 1.2265e+00, -3.0912e-01, -4.8604e-02],\n",
      "         [ 1.1980e+00, -2.3717e-01, -4.5896e-02],\n",
      "         [ 1.2468e+00, -3.4013e-01, -9.0522e-02],\n",
      "         [ 1.2652e+00, -3.6015e-01, -1.1302e-01],\n",
      "         [ 1.1945e+00, -1.8788e-01,  3.0863e-02],\n",
      "         [ 1.2251e+00, -3.3326e-01, -8.1235e-02],\n",
      "         [ 1.2661e+00, -3.5775e-01, -1.0821e-01],\n",
      "         [ 1.2566e+00, -3.3539e-01, -8.3022e-02],\n",
      "         [ 1.2738e+00, -3.6259e-01, -1.1414e-01],\n",
      "         [ 1.2650e+00, -3.5882e-01, -1.1459e-01],\n",
      "         [ 1.1880e+00, -2.0123e-01, -3.6227e-05],\n",
      "         [ 1.2480e+00, -2.9637e-01, -2.6488e-02],\n",
      "         [ 1.2615e+00, -3.5956e-01, -1.1120e-01],\n",
      "         [ 1.2140e+00, -2.7579e-01, -5.5589e-02],\n",
      "         [ 1.2650e+00, -3.5668e-01, -1.1394e-01],\n",
      "         [ 1.2648e+00, -3.6073e-01, -1.1417e-01],\n",
      "         [ 1.2090e+00, -2.9753e-01, -6.7583e-02],\n",
      "         [ 1.2468e+00, -3.4013e-01, -9.0522e-02],\n",
      "         [ 1.2342e+00, -2.5553e-01, -5.9916e-03],\n",
      "         [ 1.1988e+00, -2.4471e-01, -3.9441e-02],\n",
      "         [ 1.1985e+00, -2.7858e-01, -6.0793e-02],\n",
      "         [ 1.1964e+00, -2.1772e-01, -2.3929e-03],\n",
      "         [ 1.2649e+00, -3.4252e-01, -9.1506e-02],\n",
      "         [ 1.2510e+00, -3.3852e-01, -9.0495e-02],\n",
      "         [ 1.2468e+00, -3.4013e-01, -9.0522e-02],\n",
      "         [ 1.2663e+00, -3.5592e-01, -1.0793e-01],\n",
      "         [ 1.2706e+00, -3.6389e-01, -1.1390e-01],\n",
      "         [ 1.2021e+00, -2.3516e-01, -1.4893e-02]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.4578, -0.1056,  0.0036],\n",
      "         [ 0.4882, -0.1092, -0.0151],\n",
      "         [ 0.5496, -0.1653, -0.0546],\n",
      "         [ 0.5001, -0.1292, -0.0284],\n",
      "         [ 0.4939, -0.1358, -0.0082],\n",
      "         [ 0.5273, -0.1489, -0.0451],\n",
      "         [ 0.4985, -0.1121, -0.0220],\n",
      "         [ 0.4995, -0.1198, -0.0233],\n",
      "         [ 0.5346, -0.1578, -0.0378],\n",
      "         [ 0.4947, -0.1277, -0.0170],\n",
      "         [ 0.5012, -0.1348, -0.0274],\n",
      "         [ 0.4954, -0.1284, -0.0211],\n",
      "         [ 0.5295, -0.1621, -0.0482],\n",
      "         [ 0.4877, -0.1045, -0.0146],\n",
      "         [ 0.5298, -0.1584, -0.0464],\n",
      "         [ 0.5309, -0.1526, -0.0436],\n",
      "         [ 0.5312, -0.1643, -0.0491],\n",
      "         [ 0.5434, -0.1483, -0.0505],\n",
      "         [ 0.5459, -0.1605, -0.0521],\n",
      "         [ 0.5090, -0.1433, -0.0360],\n",
      "         [ 0.4940, -0.1305, -0.0342],\n",
      "         [ 0.5106, -0.1502, -0.0347],\n",
      "         [ 0.4978, -0.1101, -0.0242],\n",
      "         [ 0.5001, -0.1297, -0.0300],\n",
      "         [ 0.5000, -0.1465, -0.0278],\n",
      "         [ 0.5480, -0.1638, -0.0543],\n",
      "         [ 0.4878, -0.1112, -0.0059],\n",
      "         [ 0.4988, -0.1155, -0.0114],\n",
      "         [ 0.5365, -0.1550, -0.0517],\n",
      "         [ 0.5293, -0.1424, -0.0330],\n",
      "         [ 0.5379, -0.1486, -0.0558],\n",
      "         [ 0.4558, -0.0939,  0.0113]]], grad_fn=<StackBackward0>), tensor([[[ 1.1974, -0.2306,  0.0069],\n",
      "         [ 1.1963, -0.2409, -0.0315],\n",
      "         [ 1.2831, -0.3874, -0.1337],\n",
      "         [ 1.2037, -0.2841, -0.0613],\n",
      "         [ 1.2561, -0.2957, -0.0174],\n",
      "         [ 1.2195, -0.3404, -0.1055],\n",
      "         [ 1.1928, -0.2491, -0.0470],\n",
      "         [ 1.2042, -0.2683, -0.0500],\n",
      "         [ 1.2349, -0.3501, -0.0892],\n",
      "         [ 1.1973, -0.2732, -0.0368],\n",
      "         [ 1.2234, -0.3073, -0.0587],\n",
      "         [ 1.1945, -0.2753, -0.0457],\n",
      "         [ 1.2175, -0.3659, -0.1136],\n",
      "         [ 1.1945, -0.2297, -0.0304],\n",
      "         [ 1.2346, -0.3656, -0.1087],\n",
      "         [ 1.2423, -0.3498, -0.1020],\n",
      "         [ 1.2267, -0.3721, -0.1159],\n",
      "         [ 1.1908, -0.3373, -0.1228],\n",
      "         [ 1.2528, -0.3726, -0.1276],\n",
      "         [ 1.2159, -0.3174, -0.0802],\n",
      "         [ 1.1906, -0.2907, -0.0738],\n",
      "         [ 1.2286, -0.3342, -0.0775],\n",
      "         [ 1.1927, -0.2451, -0.0518],\n",
      "         [ 1.2122, -0.2948, -0.0642],\n",
      "         [ 1.2220, -0.3195, -0.0603],\n",
      "         [ 1.2729, -0.3830, -0.1328],\n",
      "         [ 1.1990, -0.2361, -0.0125],\n",
      "         [ 1.1967, -0.2475, -0.0247],\n",
      "         [ 1.2239, -0.3587, -0.1232],\n",
      "         [ 1.2099, -0.3116, -0.0779],\n",
      "         [ 1.1819, -0.3426, -0.1351],\n",
      "         [ 1.1892, -0.2029,  0.0219]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5384, -0.1632, -0.0551],\n",
      "         [ 0.5451, -0.1645, -0.0582],\n",
      "         [ 0.5419, -0.1611, -0.0577],\n",
      "         [ 0.5554, -0.1718, -0.0617],\n",
      "         [ 0.4894, -0.1511, -0.0275],\n",
      "         [ 0.4998, -0.1347, -0.0413],\n",
      "         [ 0.5141, -0.1593, -0.0457],\n",
      "         [ 0.5557, -0.1728, -0.0617],\n",
      "         [ 0.5013, -0.1302, -0.0288],\n",
      "         [ 0.5031, -0.1220, -0.0134],\n",
      "         [ 0.5022, -0.1198, -0.0295],\n",
      "         [ 0.5585, -0.1742, -0.0615],\n",
      "         [ 0.4975, -0.1366, -0.0248],\n",
      "         [ 0.5013, -0.1177, -0.0280],\n",
      "         [ 0.5547, -0.1712, -0.0598],\n",
      "         [ 0.5004, -0.1462, -0.0429],\n",
      "         [ 0.5128, -0.1584, -0.0363],\n",
      "         [ 0.5365, -0.1602, -0.0505],\n",
      "         [ 0.4835, -0.1200, -0.0050],\n",
      "         [ 0.5407, -0.1647, -0.0549],\n",
      "         [ 0.5015, -0.1207, -0.0195],\n",
      "         [ 0.4930, -0.1139, -0.0103],\n",
      "         [ 0.5025, -0.1227, -0.0288],\n",
      "         [ 0.5015, -0.1484, -0.0461],\n",
      "         [ 0.5492, -0.1560, -0.0529],\n",
      "         [ 0.5380, -0.1614, -0.0524],\n",
      "         [ 0.4519, -0.0969,  0.0079],\n",
      "         [ 0.5441, -0.1684, -0.0549],\n",
      "         [ 0.5091, -0.1485, -0.0272],\n",
      "         [ 0.5463, -0.1452, -0.0453],\n",
      "         [ 0.4941, -0.1122, -0.0124],\n",
      "         [ 0.5036, -0.1403, -0.0302]]], grad_fn=<StackBackward0>), tensor([[[ 1.2585, -0.3812, -0.1311],\n",
      "         [ 1.2486, -0.3848, -0.1407],\n",
      "         [ 1.2293, -0.3756, -0.1393],\n",
      "         [ 1.2666, -0.4045, -0.1558],\n",
      "         [ 1.2634, -0.3372, -0.0580],\n",
      "         [ 1.1977, -0.3019, -0.0900],\n",
      "         [ 1.2414, -0.3590, -0.1024],\n",
      "         [ 1.2687, -0.4074, -0.1557],\n",
      "         [ 1.1889, -0.2861, -0.0630],\n",
      "         [ 1.1938, -0.2617, -0.0294],\n",
      "         [ 1.1938, -0.2687, -0.0639],\n",
      "         [ 1.2879, -0.4115, -0.1551],\n",
      "         [ 1.1980, -0.2940, -0.0539],\n",
      "         [ 1.1904, -0.2635, -0.0605],\n",
      "         [ 1.2655, -0.4027, -0.1507],\n",
      "         [ 1.2004, -0.3306, -0.0937],\n",
      "         [ 1.2329, -0.3541, -0.0815],\n",
      "         [ 1.2405, -0.3707, -0.1202],\n",
      "         [ 1.2135, -0.2564, -0.0105],\n",
      "         [ 1.2743, -0.3853, -0.1306],\n",
      "         [ 1.1755, -0.2596, -0.0427],\n",
      "         [ 1.1672, -0.2377, -0.0225],\n",
      "         [ 1.1968, -0.2757, -0.0624],\n",
      "         [ 1.2011, -0.3361, -0.1008],\n",
      "         [ 1.1890, -0.3574, -0.1318],\n",
      "         [ 1.2468, -0.3743, -0.1249],\n",
      "         [ 1.1778, -0.2100,  0.0151],\n",
      "         [ 1.2534, -0.3842, -0.1325],\n",
      "         [ 1.2056, -0.3260, -0.0611],\n",
      "         [ 1.1825, -0.3283, -0.1124],\n",
      "         [ 1.1714, -0.2343, -0.0271],\n",
      "         [ 1.2023, -0.3110, -0.0663]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.4821, -0.1191, -0.0014],\n",
      "         [ 0.5541, -0.1725, -0.0731],\n",
      "         [ 0.5443, -0.1746, -0.0617],\n",
      "         [ 0.5514, -0.1642, -0.0655],\n",
      "         [ 0.4926, -0.1287, -0.0248],\n",
      "         [ 0.4970, -0.1287, -0.0143],\n",
      "         [ 0.4514, -0.1092,  0.0009],\n",
      "         [ 0.5118, -0.1597, -0.0469],\n",
      "         [ 0.5541, -0.1465, -0.0514],\n",
      "         [ 0.5445, -0.1672, -0.0568],\n",
      "         [ 0.4911, -0.1409, -0.0435],\n",
      "         [ 0.4920, -0.1189, -0.0196],\n",
      "         [ 0.5488, -0.1469, -0.0506],\n",
      "         [ 0.5095, -0.1288, -0.0362],\n",
      "         [ 0.4905, -0.1366, -0.0428],\n",
      "         [ 0.4931, -0.1176, -0.0224],\n",
      "         [ 0.4923, -0.1223, -0.0208],\n",
      "         [ 0.4925, -0.1199, -0.0206],\n",
      "         [ 0.4914, -0.1194, -0.0249],\n",
      "         [ 0.4756, -0.0944,  0.0016],\n",
      "         [ 0.5433, -0.1714, -0.0610],\n",
      "         [ 0.5592, -0.1736, -0.0612],\n",
      "         [ 0.5079, -0.1302, -0.0357],\n",
      "         [ 0.5480, -0.1739, -0.0632],\n",
      "         [ 0.4525, -0.0964,  0.0146],\n",
      "         [ 0.4997, -0.1521, -0.0249],\n",
      "         [ 0.5078, -0.1260, -0.0339],\n",
      "         [ 0.5401, -0.1698, -0.0600],\n",
      "         [ 0.5107, -0.1547, -0.0434],\n",
      "         [ 0.4516, -0.1113,  0.0021],\n",
      "         [ 0.5523, -0.1647, -0.0666],\n",
      "         [ 0.5550, -0.1493, -0.0527]]], grad_fn=<StackBackward0>), tensor([[[ 1.2033, -0.2507, -0.0030],\n",
      "         [ 1.1934, -0.4081, -0.1859],\n",
      "         [ 1.2131, -0.4020, -0.1526],\n",
      "         [ 1.1869, -0.3844, -0.1661],\n",
      "         [ 1.2014, -0.2886, -0.0523],\n",
      "         [ 1.1822, -0.2703, -0.0312],\n",
      "         [ 1.1885, -0.2387,  0.0018],\n",
      "         [ 1.2181, -0.3591, -0.1059],\n",
      "         [ 1.1823, -0.3308, -0.1305],\n",
      "         [ 1.2518, -0.3903, -0.1381],\n",
      "         [ 1.2020, -0.3241, -0.0918],\n",
      "         [ 1.1938, -0.2633, -0.0414],\n",
      "         [ 1.1642, -0.3327, -0.1281],\n",
      "         [ 1.1976, -0.2913, -0.0799],\n",
      "         [ 1.1989, -0.3132, -0.0903],\n",
      "         [ 1.1962, -0.2608, -0.0473],\n",
      "         [ 1.1944, -0.2717, -0.0439],\n",
      "         [ 1.1943, -0.2658, -0.0435],\n",
      "         [ 1.1938, -0.2663, -0.0526],\n",
      "         [ 1.1696, -0.1957,  0.0033],\n",
      "         [ 1.2522, -0.4037, -0.1480],\n",
      "         [ 1.2423, -0.4070, -0.1589],\n",
      "         [ 1.1899, -0.2949, -0.0787],\n",
      "         [ 1.2799, -0.4116, -0.1535],\n",
      "         [ 1.1753, -0.2056,  0.0280],\n",
      "         [ 1.2126, -0.3300, -0.0543],\n",
      "         [ 1.1906, -0.2841, -0.0748],\n",
      "         [ 1.2251, -0.3979, -0.1458],\n",
      "         [ 1.2162, -0.3462, -0.0979],\n",
      "         [ 1.1909, -0.2436,  0.0040],\n",
      "         [ 1.1911, -0.3862, -0.1688],\n",
      "         [ 1.1849, -0.3379, -0.1338]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.4835, -0.1376,  0.0019],\n",
      "         [ 0.4843, -0.1415,  0.0010],\n",
      "         [ 0.4563, -0.1174,  0.0098],\n",
      "         [ 0.5568, -0.1793, -0.0703],\n",
      "         [ 0.5561, -0.1792, -0.0657],\n",
      "         [ 0.4516, -0.1161, -0.0014],\n",
      "         [ 0.5591, -0.1823, -0.0666],\n",
      "         [ 0.5005, -0.1603, -0.0301],\n",
      "         [ 0.4960, -0.1276, -0.0252],\n",
      "         [ 0.5587, -0.1727, -0.0742],\n",
      "         [ 0.4991, -0.1382, -0.0308],\n",
      "         [ 0.5550, -0.1780, -0.0700],\n",
      "         [ 0.4962, -0.1243, -0.0263],\n",
      "         [ 0.5750, -0.1893, -0.0736],\n",
      "         [ 0.4957, -0.1457, -0.0240],\n",
      "         [ 0.5728, -0.1874, -0.0720],\n",
      "         [ 0.4840, -0.1397,  0.0019],\n",
      "         [ 0.5605, -0.1593, -0.0557],\n",
      "         [ 0.4956, -0.1231, -0.0236],\n",
      "         [ 0.5294, -0.1670, -0.0495],\n",
      "         [ 0.4957, -0.1532, -0.0519],\n",
      "         [ 0.5240, -0.1553, -0.0415],\n",
      "         [ 0.4975, -0.1336, -0.0298],\n",
      "         [ 0.5113, -0.1620, -0.0581],\n",
      "         [ 0.4816, -0.1543, -0.0226],\n",
      "         [ 0.5172, -0.1669, -0.0431],\n",
      "         [ 0.5193, -0.1350, -0.0392],\n",
      "         [ 0.5485, -0.1759, -0.0629],\n",
      "         [ 0.4526, -0.1029,  0.0197],\n",
      "         [ 0.5181, -0.1393, -0.0370],\n",
      "         [ 0.5603, -0.1560, -0.0558],\n",
      "         [ 0.5102, -0.1599, -0.0534]]], grad_fn=<StackBackward0>), tensor([[[ 1.2389, -0.2936,  0.0041],\n",
      "         [ 1.2433, -0.3030,  0.0022],\n",
      "         [ 1.2140, -0.2548,  0.0189],\n",
      "         [ 1.2705, -0.4281, -0.1753],\n",
      "         [ 1.2427, -0.4151, -0.1658],\n",
      "         [ 1.1959, -0.2549, -0.0027],\n",
      "         [ 1.2604, -0.4241, -0.1680],\n",
      "         [ 1.2204, -0.3507, -0.0657],\n",
      "         [ 1.1955, -0.2844, -0.0539],\n",
      "         [ 1.1868, -0.4081, -0.1930],\n",
      "         [ 1.2031, -0.3107, -0.0659],\n",
      "         [ 1.2603, -0.4245, -0.1745],\n",
      "         [ 1.1933, -0.2765, -0.0562],\n",
      "         [ 1.2984, -0.4542, -0.1965],\n",
      "         [ 1.1872, -0.3133, -0.0524],\n",
      "         [ 1.2871, -0.4483, -0.1923],\n",
      "         [ 1.2424, -0.2985,  0.0040],\n",
      "         [ 1.1820, -0.3633, -0.1449],\n",
      "         [ 1.1940, -0.2733, -0.0504],\n",
      "         [ 1.2247, -0.3749, -0.1178],\n",
      "         [ 1.2090, -0.3563, -0.1108],\n",
      "         [ 1.1923, -0.3426, -0.0986],\n",
      "         [ 1.1945, -0.2990, -0.0638],\n",
      "         [ 1.2035, -0.3698, -0.1312],\n",
      "         [ 1.2500, -0.3424, -0.0474],\n",
      "         [ 1.2327, -0.3744, -0.0982],\n",
      "         [ 1.1958, -0.3056, -0.0889],\n",
      "         [ 1.2414, -0.4137, -0.1558],\n",
      "         [ 1.2055, -0.2210,  0.0377],\n",
      "         [ 1.1932, -0.3160, -0.0839],\n",
      "         [ 1.1793, -0.3544, -0.1450],\n",
      "         [ 1.2056, -0.3642, -0.1204]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5548, -0.1827, -0.0738],\n",
      "         [ 0.5271, -0.1434, -0.0454],\n",
      "         [ 0.5766, -0.1878, -0.0731],\n",
      "         [ 0.5665, -0.1824, -0.0793],\n",
      "         [ 0.5493, -0.1737, -0.0602],\n",
      "         [ 0.5565, -0.1768, -0.0673],\n",
      "         [ 0.5530, -0.1594, -0.0444],\n",
      "         [ 0.5669, -0.1853, -0.0821],\n",
      "         [ 0.5204, -0.1667, -0.0440],\n",
      "         [ 0.4859, -0.1679, -0.0363],\n",
      "         [ 0.5292, -0.1575, -0.0515],\n",
      "         [ 0.5652, -0.1598, -0.0575],\n",
      "         [ 0.5292, -0.1575, -0.0515],\n",
      "         [ 0.5655, -0.1596, -0.0577],\n",
      "         [ 0.5660, -0.1874, -0.0718],\n",
      "         [ 0.5010, -0.1396, -0.0304],\n",
      "         [ 0.5607, -0.1759, -0.0448],\n",
      "         [ 0.5316, -0.1645, -0.0535],\n",
      "         [ 0.4808, -0.1305, -0.0038],\n",
      "         [ 0.5645, -0.1826, -0.0535],\n",
      "         [ 0.5000, -0.1317, -0.0281],\n",
      "         [ 0.4526, -0.1002,  0.0121],\n",
      "         [ 0.5106, -0.1371, -0.0375],\n",
      "         [ 0.5167, -0.1705, -0.0635],\n",
      "         [ 0.5313, -0.1678, -0.0524],\n",
      "         [ 0.5007, -0.1418, -0.0385],\n",
      "         [ 0.5567, -0.1764, -0.0568],\n",
      "         [ 0.5604, -0.1835, -0.0699],\n",
      "         [ 0.5500, -0.1648, -0.0539],\n",
      "         [ 0.5273, -0.1483, -0.0520],\n",
      "         [ 0.5625, -0.1846, -0.0707],\n",
      "         [ 0.5815, -0.1944, -0.0782]]], grad_fn=<StackBackward0>), tensor([[[ 1.2530, -0.4363, -0.1863],\n",
      "         [ 1.1914, -0.3272, -0.1055],\n",
      "         [ 1.2618, -0.4464, -0.2003],\n",
      "         [ 1.1973, -0.4351, -0.2113],\n",
      "         [ 1.2292, -0.4081, -0.1515],\n",
      "         [ 1.2099, -0.4081, -0.1731],\n",
      "         [ 1.1820, -0.3526, -0.1144],\n",
      "         [ 1.1980, -0.4442, -0.2191],\n",
      "         [ 1.2260, -0.3720, -0.1013],\n",
      "         [ 1.2852, -0.3768, -0.0762],\n",
      "         [ 1.2162, -0.3674, -0.1198],\n",
      "         [ 1.1771, -0.3640, -0.1529],\n",
      "         [ 1.2162, -0.3674, -0.1198],\n",
      "         [ 1.1786, -0.3635, -0.1532],\n",
      "         [ 1.2636, -0.4394, -0.1851],\n",
      "         [ 1.1994, -0.3137, -0.0659],\n",
      "         [ 1.2235, -0.3949, -0.1156],\n",
      "         [ 1.2154, -0.3681, -0.1285],\n",
      "         [ 1.2081, -0.2746, -0.0079],\n",
      "         [ 1.2304, -0.4141, -0.1385],\n",
      "         [ 1.1950, -0.2937, -0.0607],\n",
      "         [ 1.1714, -0.2137,  0.0233],\n",
      "         [ 1.1684, -0.3005, -0.0858],\n",
      "         [ 1.2089, -0.3914, -0.1456],\n",
      "         [ 1.2147, -0.3761, -0.1258],\n",
      "         [ 1.2038, -0.3220, -0.0831],\n",
      "         [ 1.2281, -0.4116, -0.1442],\n",
      "         [ 1.2776, -0.4367, -0.1764],\n",
      "         [ 1.1941, -0.3795, -0.1365],\n",
      "         [ 1.2013, -0.3431, -0.1207],\n",
      "         [ 1.2427, -0.4310, -0.1822],\n",
      "         [ 1.2983, -0.4685, -0.2143]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5364, -0.1566, -0.0602],\n",
      "         [ 0.5692, -0.1810, -0.0519],\n",
      "         [ 0.4564, -0.1323, -0.0192],\n",
      "         [ 0.4994, -0.1599, -0.0295],\n",
      "         [ 0.5188, -0.1747, -0.0548],\n",
      "         [ 0.5366, -0.1512, -0.0488],\n",
      "         [ 0.5017, -0.1563, -0.0586],\n",
      "         [ 0.4768, -0.1332,  0.0022],\n",
      "         [ 0.5395, -0.1864, -0.0564],\n",
      "         [ 0.5626, -0.1796, -0.0619],\n",
      "         [ 0.5673, -0.1786, -0.0773],\n",
      "         [ 0.5049, -0.1486, -0.0457],\n",
      "         [ 0.5395, -0.1864, -0.0564],\n",
      "         [ 0.5098, -0.1755, -0.0450],\n",
      "         [ 0.4988, -0.1636, -0.0328],\n",
      "         [ 0.5063, -0.1686, -0.0337],\n",
      "         [ 0.5811, -0.1959, -0.0773],\n",
      "         [ 0.4809, -0.1361, -0.0048],\n",
      "         [ 0.4887, -0.1626, -0.0118],\n",
      "         [ 0.5702, -0.1795, -0.0777],\n",
      "         [ 0.5052, -0.1406, -0.0334],\n",
      "         [ 0.5884, -0.2004, -0.0834],\n",
      "         [ 0.4824, -0.1734, -0.0411],\n",
      "         [ 0.5030, -0.1679, -0.0346],\n",
      "         [ 0.5041, -0.1647, -0.0634],\n",
      "         [ 0.5612, -0.1879, -0.0776],\n",
      "         [ 0.4972, -0.1498, -0.0296],\n",
      "         [ 0.4789, -0.1353,  0.0048],\n",
      "         [ 0.5599, -0.1804, -0.0693],\n",
      "         [ 0.5830, -0.1930, -0.0759],\n",
      "         [ 0.4827, -0.1701, -0.0390],\n",
      "         [ 0.4997, -0.1457, -0.0162]]], grad_fn=<StackBackward0>), tensor([[[ 1.1994, -0.3659, -0.1439],\n",
      "         [ 1.2191, -0.4083, -0.1376],\n",
      "         [ 1.2028, -0.2950, -0.0369],\n",
      "         [ 1.1908, -0.3461, -0.0651],\n",
      "         [ 1.2163, -0.3960, -0.1272],\n",
      "         [ 1.1951, -0.3473, -0.1167],\n",
      "         [ 1.2017, -0.3638, -0.1281],\n",
      "         [ 1.1952, -0.2782,  0.0047],\n",
      "         [ 1.2430, -0.4248, -0.1377],\n",
      "         [ 1.2282, -0.4209, -0.1602],\n",
      "         [ 1.1781, -0.4236, -0.2102],\n",
      "         [ 1.2079, -0.3398, -0.0999],\n",
      "         [ 1.2430, -0.4248, -0.1377],\n",
      "         [ 1.2526, -0.3895, -0.0995],\n",
      "         [ 1.1967, -0.3573, -0.0724],\n",
      "         [ 1.2308, -0.3689, -0.0744],\n",
      "         [ 1.2500, -0.4683, -0.2172],\n",
      "         [ 1.2112, -0.2862, -0.0101],\n",
      "         [ 1.2685, -0.3513, -0.0250],\n",
      "         [ 1.1945, -0.4267, -0.2110],\n",
      "         [ 1.2033, -0.3159, -0.0731],\n",
      "         [ 1.3032, -0.4864, -0.2343],\n",
      "         [ 1.2632, -0.3892, -0.0864],\n",
      "         [ 1.2224, -0.3685, -0.0763],\n",
      "         [ 1.2138, -0.3871, -0.1388],\n",
      "         [ 1.2607, -0.4520, -0.1996],\n",
      "         [ 1.1778, -0.3220, -0.0653],\n",
      "         [ 1.2283, -0.2847,  0.0101],\n",
      "         [ 1.2279, -0.4282, -0.1788],\n",
      "         [ 1.2630, -0.4602, -0.2129],\n",
      "         [ 1.2683, -0.3807, -0.0820],\n",
      "         [ 1.1821, -0.3069, -0.0358]]], grad_fn=<StackBackward0>))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.5671, -0.1807, -0.0664],\n",
      "         [ 0.5778, -0.1861, -0.0509],\n",
      "         [ 0.5407, -0.1805, -0.0657],\n",
      "         [ 0.5762, -0.1948, -0.0804],\n",
      "         [ 0.5795, -0.1925, -0.0740],\n",
      "         [ 0.5085, -0.1682, -0.0666],\n",
      "         [ 0.5069, -0.1648, -0.0615],\n",
      "         [ 0.5910, -0.2022, -0.0852],\n",
      "         [ 0.5453, -0.1921, -0.0616],\n",
      "         [ 0.5096, -0.1441, -0.0384],\n",
      "         [ 0.5218, -0.1464, -0.0319],\n",
      "         [ 0.5278, -0.1764, -0.0536],\n",
      "         [ 0.5088, -0.1458, -0.0375],\n",
      "         [ 0.5273, -0.1850, -0.0646],\n",
      "         [ 0.5745, -0.1980, -0.0847],\n",
      "         [ 0.4838, -0.1768, -0.0438],\n",
      "         [ 0.5375, -0.1823, -0.0506],\n",
      "         [ 0.5671, -0.1885, -0.0768],\n",
      "         [ 0.4548, -0.1308, -0.0010],\n",
      "         [ 0.5838, -0.1937, -0.0786],\n",
      "         [ 0.5687, -0.1936, -0.0838],\n",
      "         [ 0.5204, -0.1604, -0.0342],\n",
      "         [ 0.5723, -0.1931, -0.0755],\n",
      "         [ 0.5410, -0.1816, -0.0614],\n",
      "         [ 0.4533, -0.1101,  0.0156],\n",
      "         [ 0.4531, -0.1319, -0.0098],\n",
      "         [ 0.5772, -0.1974, -0.0836],\n",
      "         [ 0.5788, -0.1934, -0.0894],\n",
      "         [ 0.5772, -0.1907, -0.0840],\n",
      "         [ 0.5085, -0.1552, -0.0482],\n",
      "         [ 0.5670, -0.1893, -0.0774],\n",
      "         [ 0.5699, -0.1891, -0.0802]]], grad_fn=<StackBackward0>), tensor([[[ 1.2154, -0.4236, -0.1759],\n",
      "         [ 1.2379, -0.4223, -0.1380],\n",
      "         [ 1.2222, -0.4107, -0.1625],\n",
      "         [ 1.2405, -0.4617, -0.2175],\n",
      "         [ 1.2190, -0.4563, -0.2122],\n",
      "         [ 1.2116, -0.3959, -0.1479],\n",
      "         [ 1.2116, -0.3870, -0.1364],\n",
      "         [ 1.2860, -0.4906, -0.2449],\n",
      "         [ 1.2536, -0.4408, -0.1528],\n",
      "         [ 1.1983, -0.3246, -0.0853],\n",
      "         [ 1.1623, -0.3145, -0.0754],\n",
      "         [ 1.2120, -0.3995, -0.1266],\n",
      "         [ 1.1949, -0.3286, -0.0833],\n",
      "         [ 1.2392, -0.4244, -0.1522],\n",
      "         [ 1.2333, -0.4728, -0.2296],\n",
      "         [ 1.2837, -0.3971, -0.0921],\n",
      "         [ 1.2083, -0.4100, -0.1254],\n",
      "         [ 1.2524, -0.4511, -0.2013],\n",
      "         [ 1.1990, -0.2879, -0.0020],\n",
      "         [ 1.2042, -0.4606, -0.2205],\n",
      "         [ 1.2443, -0.4697, -0.2221],\n",
      "         [ 1.1931, -0.3507, -0.0804],\n",
      "         [ 1.2218, -0.4549, -0.2040],\n",
      "         [ 1.2243, -0.4119, -0.1521],\n",
      "         [ 1.1792, -0.2351,  0.0301],\n",
      "         [ 1.1954, -0.2928, -0.0189],\n",
      "         [ 1.2507, -0.4714, -0.2265],\n",
      "         [ 1.1993, -0.4677, -0.2503],\n",
      "         [ 1.1980, -0.4592, -0.2344],\n",
      "         [ 1.2064, -0.3569, -0.1068],\n",
      "         [ 1.2510, -0.4536, -0.2030],\n",
      "         [ 1.2415, -0.4548, -0.2125]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5770, -0.1931, -0.0699],\n",
      "         [ 0.5729, -0.1928, -0.0817],\n",
      "         [ 0.5570, -0.1822, -0.0693],\n",
      "         [ 0.5466, -0.1957, -0.0634],\n",
      "         [ 0.5453, -0.1919, -0.0688],\n",
      "         [ 0.4789, -0.1818, -0.0502],\n",
      "         [ 0.5389, -0.1773, -0.0545],\n",
      "         [ 0.5714, -0.1953, -0.0849],\n",
      "         [ 0.5743, -0.1961, -0.0857],\n",
      "         [ 0.5793, -0.2001, -0.0904],\n",
      "         [ 0.5739, -0.1867, -0.0704],\n",
      "         [ 0.5934, -0.2034, -0.0862],\n",
      "         [ 0.5551, -0.1694, -0.0626],\n",
      "         [ 0.5752, -0.1907, -0.0800],\n",
      "         [ 0.5756, -0.1788, -0.0514],\n",
      "         [ 0.5836, -0.1916, -0.0575],\n",
      "         [ 0.5877, -0.2035, -0.0885],\n",
      "         [ 0.5947, -0.2070, -0.0893],\n",
      "         [ 0.4497, -0.0914,  0.0172],\n",
      "         [ 0.5696, -0.1888, -0.0769],\n",
      "         [ 0.5874, -0.1833, -0.0717],\n",
      "         [ 0.5014, -0.1513, -0.0352],\n",
      "         [ 0.4793, -0.1820, -0.0482],\n",
      "         [ 0.5339, -0.1844, -0.0673],\n",
      "         [ 0.5991, -0.2088, -0.0914],\n",
      "         [ 0.5015, -0.1543, -0.0330],\n",
      "         [ 0.5693, -0.1899, -0.0754],\n",
      "         [ 0.5782, -0.1990, -0.0894],\n",
      "         [ 0.5898, -0.1867, -0.0735],\n",
      "         [ 0.5152, -0.1597, -0.0548],\n",
      "         [ 0.5156, -0.1527, -0.0443],\n",
      "         [ 0.6012, -0.2093, -0.0919]]], grad_fn=<StackBackward0>), tensor([[[ 1.2316, -0.4579, -0.1904],\n",
      "         [ 1.2510, -0.4641, -0.2187],\n",
      "         [ 1.2212, -0.4365, -0.1756],\n",
      "         [ 1.2383, -0.4488, -0.1594],\n",
      "         [ 1.2318, -0.4410, -0.1726],\n",
      "         [ 1.2513, -0.4085, -0.1056],\n",
      "         [ 1.1930, -0.3965, -0.1367],\n",
      "         [ 1.2477, -0.4733, -0.2276],\n",
      "         [ 1.2646, -0.4763, -0.2297],\n",
      "         [ 1.2612, -0.4895, -0.2466],\n",
      "         [ 1.2136, -0.4398, -0.1917],\n",
      "         [ 1.2572, -0.4909, -0.2538],\n",
      "         [ 1.2018, -0.3975, -0.1584],\n",
      "         [ 1.2006, -0.4487, -0.2216],\n",
      "         [ 1.1919, -0.4014, -0.1424],\n",
      "         [ 1.2208, -0.4360, -0.1600],\n",
      "         [ 1.2640, -0.4889, -0.2460],\n",
      "         [ 1.2740, -0.5043, -0.2631],\n",
      "         [ 1.1506, -0.1927,  0.0333],\n",
      "         [ 1.2090, -0.4516, -0.2089],\n",
      "         [ 1.1929, -0.4272, -0.2053],\n",
      "         [ 1.1722, -0.3251, -0.0788],\n",
      "         [ 1.2572, -0.4091, -0.1014],\n",
      "         [ 1.2003, -0.4216, -0.1626],\n",
      "         [ 1.2974, -0.5107, -0.2697],\n",
      "         [ 1.1753, -0.3318, -0.0739],\n",
      "         [ 1.2343, -0.4533, -0.2016],\n",
      "         [ 1.2921, -0.4866, -0.2396],\n",
      "         [ 1.2032, -0.4375, -0.2107],\n",
      "         [ 1.2102, -0.3693, -0.1235],\n",
      "         [ 1.1974, -0.3463, -0.1002],\n",
      "         [ 1.3100, -0.5127, -0.2709]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5993, -0.2080, -0.0894],\n",
      "         [ 0.5398, -0.1917, -0.0638],\n",
      "         [ 0.4801, -0.1536,  0.0019],\n",
      "         [ 0.6041, -0.2127, -0.0952],\n",
      "         [ 0.5654, -0.1813, -0.0743],\n",
      "         [ 0.5417, -0.1914, -0.0687],\n",
      "         [ 0.5665, -0.1819, -0.0742],\n",
      "         [ 0.5922, -0.2041, -0.0683],\n",
      "         [ 0.5094, -0.1733, -0.0426],\n",
      "         [ 0.5628, -0.1731, -0.0655],\n",
      "         [ 0.5549, -0.2042, -0.0700],\n",
      "         [ 0.5751, -0.1928, -0.0807],\n",
      "         [ 0.5935, -0.1891, -0.0736],\n",
      "         [ 0.5381, -0.1960, -0.0884],\n",
      "         [ 0.4835, -0.1532, -0.0118],\n",
      "         [ 0.5304, -0.1922, -0.0686],\n",
      "         [ 0.5360, -0.1766, -0.0614],\n",
      "         [ 0.5877, -0.1934, -0.0537],\n",
      "         [ 0.5919, -0.2052, -0.0963],\n",
      "         [ 0.4828, -0.1632, -0.0009],\n",
      "         [ 0.4846, -0.1658, -0.0111],\n",
      "         [ 0.5837, -0.2077, -0.0935],\n",
      "         [ 0.4847, -0.1696, -0.0010],\n",
      "         [ 0.5389, -0.1944, -0.0855],\n",
      "         [ 0.6056, -0.2105, -0.0910],\n",
      "         [ 0.5770, -0.1990, -0.0863],\n",
      "         [ 0.6066, -0.2131, -0.0957],\n",
      "         [ 0.6010, -0.2109, -0.0931],\n",
      "         [ 0.5439, -0.1963, -0.0755],\n",
      "         [ 0.5809, -0.1973, -0.0757],\n",
      "         [ 0.5523, -0.1998, -0.0722],\n",
      "         [ 0.5693, -0.1908, -0.0717]]], grad_fn=<StackBackward0>), tensor([[[ 1.2602, -0.5042, -0.2698],\n",
      "         [ 1.2100, -0.4400, -0.1572],\n",
      "         [ 1.2322, -0.3229,  0.0040],\n",
      "         [ 1.2930, -0.5219, -0.2877],\n",
      "         [ 1.2040, -0.4315, -0.1940],\n",
      "         [ 1.2165, -0.4408, -0.1693],\n",
      "         [ 1.2097, -0.4333, -0.1938],\n",
      "         [ 1.2219, -0.4728, -0.1954],\n",
      "         [ 1.1955, -0.3792, -0.0964],\n",
      "         [ 1.2025, -0.4074, -0.1702],\n",
      "         [ 1.2532, -0.4730, -0.1797],\n",
      "         [ 1.2033, -0.4626, -0.2249],\n",
      "         [ 1.1962, -0.4434, -0.2162],\n",
      "         [ 1.2130, -0.4614, -0.2180],\n",
      "         [ 1.2217, -0.3227, -0.0251],\n",
      "         [ 1.2158, -0.4420, -0.1660],\n",
      "         [ 1.1888, -0.3992, -0.1507],\n",
      "         [ 1.2202, -0.4401, -0.1527],\n",
      "         [ 1.2951, -0.5063, -0.2700],\n",
      "         [ 1.2496, -0.3463, -0.0020],\n",
      "         [ 1.2385, -0.3529, -0.0236],\n",
      "         [ 1.2123, -0.5021, -0.2670],\n",
      "         [ 1.2639, -0.3617, -0.0021],\n",
      "         [ 1.2200, -0.4564, -0.2104],\n",
      "         [ 1.2906, -0.5124, -0.2748],\n",
      "         [ 1.2413, -0.4821, -0.2364],\n",
      "         [ 1.3084, -0.5240, -0.2892],\n",
      "         [ 1.2763, -0.5154, -0.2811],\n",
      "         [ 1.2192, -0.4550, -0.1865],\n",
      "         [ 1.2151, -0.4686, -0.2140],\n",
      "         [ 1.2335, -0.4609, -0.1855],\n",
      "         [ 1.2222, -0.4564, -0.1950]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5981, -0.2074, -0.0962],\n",
      "         [ 0.5850, -0.2026, -0.0909],\n",
      "         [ 0.4819, -0.1842, -0.0514],\n",
      "         [ 0.6061, -0.2151, -0.0972],\n",
      "         [ 0.5264, -0.1750, -0.0715],\n",
      "         [ 0.5468, -0.1761, -0.0530],\n",
      "         [ 0.5881, -0.1880, -0.0591],\n",
      "         [ 0.5743, -0.1954, -0.0791],\n",
      "         [ 0.6076, -0.2161, -0.0985],\n",
      "         [ 0.5982, -0.2076, -0.1001],\n",
      "         [ 0.5423, -0.2036, -0.0867],\n",
      "         [ 0.5957, -0.2119, -0.0986],\n",
      "         [ 0.5492, -0.2000, -0.0733],\n",
      "         [ 0.5968, -0.2117, -0.0984],\n",
      "         [ 0.5434, -0.1887, -0.0702],\n",
      "         [ 0.5245, -0.1806, -0.0802],\n",
      "         [ 0.5321, -0.1806, -0.0676],\n",
      "         [ 0.5142, -0.1844, -0.0432],\n",
      "         [ 0.4934, -0.1832, -0.0175],\n",
      "         [ 0.5944, -0.2068, -0.0972],\n",
      "         [ 0.5707, -0.1863, -0.0857],\n",
      "         [ 0.5370, -0.1959, -0.0606],\n",
      "         [ 0.4612, -0.1211,  0.0032],\n",
      "         [ 0.6120, -0.2171, -0.0996],\n",
      "         [ 0.5548, -0.2000, -0.0656],\n",
      "         [ 0.4578, -0.1262, -0.0104],\n",
      "         [ 0.5820, -0.1918, -0.0809],\n",
      "         [ 0.5919, -0.2081, -0.0998],\n",
      "         [ 0.5168, -0.1865, -0.0458],\n",
      "         [ 0.5688, -0.1899, -0.0805],\n",
      "         [ 0.5597, -0.2095, -0.0726],\n",
      "         [ 0.6099, -0.2163, -0.0983]]], grad_fn=<StackBackward0>), tensor([[[ 1.2115, -0.5093, -0.2908],\n",
      "         [ 1.2522, -0.4934, -0.2546],\n",
      "         [ 1.2429, -0.4113, -0.1102],\n",
      "         [ 1.2728, -0.5278, -0.3013],\n",
      "         [ 1.2070, -0.4112, -0.1673],\n",
      "         [ 1.1782, -0.3880, -0.1340],\n",
      "         [ 1.1861, -0.4262, -0.1723],\n",
      "         [ 1.2343, -0.4763, -0.2115],\n",
      "         [ 1.2797, -0.5312, -0.3053],\n",
      "         [ 1.2060, -0.5099, -0.3033],\n",
      "         [ 1.2529, -0.4764, -0.2134],\n",
      "         [ 1.2416, -0.5166, -0.2890],\n",
      "         [ 1.2199, -0.4640, -0.1851],\n",
      "         [ 1.2479, -0.5164, -0.2883],\n",
      "         [ 1.1886, -0.4319, -0.1769],\n",
      "         [ 1.2165, -0.4312, -0.1875],\n",
      "         [ 1.2137, -0.4219, -0.1588],\n",
      "         [ 1.2096, -0.4068, -0.0987],\n",
      "         [ 1.2693, -0.3948, -0.0377],\n",
      "         [ 1.2761, -0.5099, -0.2790],\n",
      "         [ 1.2144, -0.4525, -0.2287],\n",
      "         [ 1.2119, -0.4449, -0.1492],\n",
      "         [ 1.1703, -0.2609,  0.0063],\n",
      "         [ 1.3052, -0.5357, -0.3088],\n",
      "         [ 1.2184, -0.4571, -0.1717],\n",
      "         [ 1.1658, -0.2774, -0.0203],\n",
      "         [ 1.1844, -0.4532, -0.2346],\n",
      "         [ 1.3016, -0.5150, -0.2801],\n",
      "         [ 1.2238, -0.4130, -0.1048],\n",
      "         [ 1.2100, -0.4614, -0.2146],\n",
      "         [ 1.2515, -0.4874, -0.1901],\n",
      "         [ 1.2934, -0.5322, -0.3046]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5898, -0.2067, -0.0947],\n",
      "         [ 0.5841, -0.2062, -0.0883],\n",
      "         [ 0.5450, -0.2033, -0.0612],\n",
      "         [ 0.6152, -0.2197, -0.1020],\n",
      "         [ 0.5966, -0.2120, -0.1040],\n",
      "         [ 0.6104, -0.2181, -0.0991],\n",
      "         [ 0.5373, -0.1890, -0.0548],\n",
      "         [ 0.4688, -0.1415,  0.0048],\n",
      "         [ 0.5553, -0.2039, -0.0787],\n",
      "         [ 0.5491, -0.1992, -0.0873],\n",
      "         [ 0.5910, -0.2076, -0.0958],\n",
      "         [ 0.5777, -0.1924, -0.0902],\n",
      "         [ 0.4624, -0.1356, -0.0124],\n",
      "         [ 0.4895, -0.1678, -0.0089],\n",
      "         [ 0.5996, -0.2135, -0.0976],\n",
      "         [ 0.5880, -0.2091, -0.0969],\n",
      "         [ 0.5285, -0.1881, -0.0849],\n",
      "         [ 0.4706, -0.1343, -0.0139],\n",
      "         [ 0.5271, -0.1864, -0.0895],\n",
      "         [ 0.6120, -0.2163, -0.0955],\n",
      "         [ 0.5981, -0.2144, -0.0965],\n",
      "         [ 0.5905, -0.2069, -0.0950],\n",
      "         [ 0.5313, -0.1904, -0.0934],\n",
      "         [ 0.4709, -0.1614, -0.0538],\n",
      "         [ 0.6102, -0.2181, -0.0971],\n",
      "         [ 0.4640, -0.1259,  0.0019],\n",
      "         [ 0.6047, -0.2142, -0.0989],\n",
      "         [ 0.6071, -0.2110, -0.0859],\n",
      "         [ 0.6031, -0.2036, -0.0829],\n",
      "         [ 0.6137, -0.2186, -0.0993],\n",
      "         [ 0.5756, -0.1887, -0.0841],\n",
      "         [ 0.6037, -0.1970, -0.0777]]], grad_fn=<StackBackward0>), tensor([[[ 1.2480, -0.5054, -0.2712],\n",
      "         [ 1.2356, -0.5043, -0.2517],\n",
      "         [ 1.2287, -0.4619, -0.1526],\n",
      "         [ 1.2924, -0.5424, -0.3242],\n",
      "         [ 1.2956, -0.5266, -0.2986],\n",
      "         [ 1.2674, -0.5358, -0.3144],\n",
      "         [ 1.1903, -0.4217, -0.1363],\n",
      "         [ 1.2132, -0.3105,  0.0095],\n",
      "         [ 1.2153, -0.4751, -0.2035],\n",
      "         [ 1.2081, -0.4685, -0.2249],\n",
      "         [ 1.2544, -0.5087, -0.2746],\n",
      "         [ 1.2115, -0.4708, -0.2482],\n",
      "         [ 1.1741, -0.3004, -0.0245],\n",
      "         [ 1.2327, -0.3549, -0.0194],\n",
      "         [ 1.2237, -0.5186, -0.2936],\n",
      "         [ 1.2351, -0.5124, -0.2780],\n",
      "         [ 1.2161, -0.4520, -0.2014],\n",
      "         [ 1.1841, -0.2941, -0.0277],\n",
      "         [ 1.1981, -0.4466, -0.2128],\n",
      "         [ 1.2662, -0.5279, -0.3028],\n",
      "         [ 1.2247, -0.5223, -0.2897],\n",
      "         [ 1.2519, -0.5060, -0.2722],\n",
      "         [ 1.2221, -0.4593, -0.2221],\n",
      "         [ 1.2116, -0.3721, -0.1069],\n",
      "         [ 1.2540, -0.5332, -0.3086],\n",
      "         [ 1.1786, -0.2737,  0.0039],\n",
      "         [ 1.2499, -0.5221, -0.2974],\n",
      "         [ 1.2116, -0.5104, -0.2661],\n",
      "         [ 1.1932, -0.4869, -0.2562],\n",
      "         [ 1.2759, -0.5368, -0.3152],\n",
      "         [ 1.2073, -0.4588, -0.2306],\n",
      "         [ 1.1905, -0.4646, -0.2398]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5478, -0.2115, -0.0933],\n",
      "         [ 0.6177, -0.2233, -0.1048],\n",
      "         [ 0.5398, -0.1815, -0.0690],\n",
      "         [ 0.6083, -0.2102, -0.0987],\n",
      "         [ 0.5967, -0.2075, -0.0869],\n",
      "         [ 0.5419, -0.1872, -0.0758],\n",
      "         [ 0.6058, -0.2073, -0.0873],\n",
      "         [ 0.5694, -0.2194, -0.0837],\n",
      "         [ 0.5568, -0.1837, -0.0556],\n",
      "         [ 0.6211, -0.2242, -0.1061],\n",
      "         [ 0.5486, -0.2104, -0.0734],\n",
      "         [ 0.5651, -0.2081, -0.0872],\n",
      "         [ 0.5628, -0.1907, -0.0535],\n",
      "         [ 0.5375, -0.1820, -0.0703],\n",
      "         [ 0.5998, -0.2136, -0.0997],\n",
      "         [ 0.5592, -0.2095, -0.1005],\n",
      "         [ 0.6043, -0.2165, -0.1068],\n",
      "         [ 0.5865, -0.1996, -0.0937],\n",
      "         [ 0.6098, -0.2138, -0.1036],\n",
      "         [ 0.5360, -0.1956, -0.0969],\n",
      "         [ 0.5413, -0.2086, -0.0834],\n",
      "         [ 0.5923, -0.2063, -0.0895],\n",
      "         [ 0.5365, -0.1868, -0.0834],\n",
      "         [ 0.5371, -0.1804, -0.0672],\n",
      "         [ 0.5886, -0.1946, -0.0851],\n",
      "         [ 0.5423, -0.1905, -0.0781],\n",
      "         [ 0.6096, -0.2119, -0.0632],\n",
      "         [ 0.4843, -0.1621, -0.0328],\n",
      "         [ 0.5908, -0.2055, -0.0902],\n",
      "         [ 0.5245, -0.1982, -0.0565],\n",
      "         [ 0.6115, -0.2056, -0.0844],\n",
      "         [ 0.5979, -0.1912, -0.0782]]], grad_fn=<StackBackward0>), tensor([[[ 1.2508, -0.4971, -0.2351],\n",
      "         [ 1.2778, -0.5521, -0.3410],\n",
      "         [ 1.2166, -0.4235, -0.1665],\n",
      "         [ 1.2124, -0.5167, -0.3124],\n",
      "         [ 1.2002, -0.4992, -0.2654],\n",
      "         [ 1.2162, -0.4394, -0.1836],\n",
      "         [ 1.1812, -0.4980, -0.2768],\n",
      "         [ 1.2417, -0.5152, -0.2285],\n",
      "         [ 1.1680, -0.4053, -0.1469],\n",
      "         [ 1.2967, -0.5559, -0.3455],\n",
      "         [ 1.2362, -0.4846, -0.1849],\n",
      "         [ 1.2255, -0.4860, -0.2371],\n",
      "         [ 1.2044, -0.4238, -0.1415],\n",
      "         [ 1.2020, -0.4247, -0.1697],\n",
      "         [ 1.2387, -0.5260, -0.2998],\n",
      "         [ 1.2178, -0.4985, -0.2664],\n",
      "         [ 1.2673, -0.5384, -0.3219],\n",
      "         [ 1.2241, -0.4929, -0.2649],\n",
      "         [ 1.2127, -0.5277, -0.3294],\n",
      "         [ 1.2208, -0.4735, -0.2345],\n",
      "         [ 1.2219, -0.4862, -0.2095],\n",
      "         [ 1.2079, -0.5006, -0.2674],\n",
      "         [ 1.2139, -0.4442, -0.2011],\n",
      "         [ 1.2017, -0.4195, -0.1620],\n",
      "         [ 1.2129, -0.4707, -0.2409],\n",
      "         [ 1.2174, -0.4491, -0.1893],\n",
      "         [ 1.2349, -0.4921, -0.1939],\n",
      "         [ 1.2357, -0.3666, -0.0661],\n",
      "         [ 1.1992, -0.4981, -0.2698],\n",
      "         [ 1.2031, -0.4435, -0.1336],\n",
      "         [ 1.1975, -0.4898, -0.2673],\n",
      "         [ 1.1527, -0.4493, -0.2459]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6175, -0.2215, -0.1046],\n",
      "         [ 0.6087, -0.2080, -0.0634],\n",
      "         [ 0.5957, -0.2128, -0.0929],\n",
      "         [ 0.5919, -0.2025, -0.0994],\n",
      "         [ 0.5398, -0.1848, -0.0683],\n",
      "         [ 0.5454, -0.1906, -0.0788],\n",
      "         [ 0.5888, -0.1997, -0.0940],\n",
      "         [ 0.6017, -0.2177, -0.1076],\n",
      "         [ 0.5980, -0.2181, -0.1054],\n",
      "         [ 0.5408, -0.1944, -0.0900],\n",
      "         [ 0.5949, -0.2182, -0.1064],\n",
      "         [ 0.6110, -0.2218, -0.0926],\n",
      "         [ 0.5411, -0.1883, -0.0697],\n",
      "         [ 0.6041, -0.2177, -0.0991],\n",
      "         [ 0.6156, -0.2096, -0.0871],\n",
      "         [ 0.6082, -0.2231, -0.1071],\n",
      "         [ 0.5697, -0.2012, -0.0671],\n",
      "         [ 0.6011, -0.2107, -0.0872],\n",
      "         [ 0.5985, -0.2132, -0.1025],\n",
      "         [ 0.5360, -0.1949, -0.0907],\n",
      "         [ 0.5575, -0.1978, -0.0764],\n",
      "         [ 0.6213, -0.2264, -0.1051],\n",
      "         [ 0.5917, -0.1975, -0.0823],\n",
      "         [ 0.5941, -0.2076, -0.0951],\n",
      "         [ 0.6018, -0.2170, -0.1067],\n",
      "         [ 0.5698, -0.2183, -0.0960],\n",
      "         [ 0.5581, -0.2008, -0.0757],\n",
      "         [ 0.6100, -0.2216, -0.1034],\n",
      "         [ 0.5066, -0.2000, -0.0354],\n",
      "         [ 0.5902, -0.1970, -0.0808],\n",
      "         [ 0.5405, -0.1920, -0.0606],\n",
      "         [ 0.5976, -0.2167, -0.1048]]], grad_fn=<StackBackward0>), tensor([[[ 1.2641, -0.5445, -0.3298],\n",
      "         [ 1.2085, -0.4792, -0.1983],\n",
      "         [ 1.2068, -0.5180, -0.2833],\n",
      "         [ 1.2185, -0.5013, -0.2885],\n",
      "         [ 1.2011, -0.4299, -0.1666],\n",
      "         [ 1.2202, -0.4481, -0.1928],\n",
      "         [ 1.2090, -0.4919, -0.2720],\n",
      "         [ 1.2688, -0.5411, -0.3224],\n",
      "         [ 1.2486, -0.5408, -0.3156],\n",
      "         [ 1.2202, -0.4654, -0.2199],\n",
      "         [ 1.2262, -0.5406, -0.3191],\n",
      "         [ 1.2245, -0.5390, -0.3053],\n",
      "         [ 1.2074, -0.4398, -0.1702],\n",
      "         [ 1.2039, -0.5303, -0.3117],\n",
      "         [ 1.2001, -0.4999, -0.2811],\n",
      "         [ 1.2205, -0.5497, -0.3385],\n",
      "         [ 1.1946, -0.4523, -0.1820],\n",
      "         [ 1.2014, -0.5073, -0.2719],\n",
      "         [ 1.2204, -0.5226, -0.3138],\n",
      "         [ 1.2136, -0.4692, -0.2213],\n",
      "         [ 1.1732, -0.4544, -0.2059],\n",
      "         [ 1.2767, -0.5596, -0.3488],\n",
      "         [ 1.2059, -0.4770, -0.2383],\n",
      "         [ 1.2354, -0.5165, -0.2756],\n",
      "         [ 1.2699, -0.5386, -0.3195],\n",
      "         [ 1.2185, -0.5167, -0.2617],\n",
      "         [ 1.1770, -0.4624, -0.2041],\n",
      "         [ 1.2235, -0.5426, -0.3264],\n",
      "         [ 1.2546, -0.4344, -0.0800],\n",
      "         [ 1.1991, -0.4748, -0.2339],\n",
      "         [ 1.1827, -0.4282, -0.1536],\n",
      "         [ 1.2429, -0.5364, -0.3139]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5954, -0.2061, -0.1011],\n",
      "         [ 0.6050, -0.2161, -0.0917],\n",
      "         [ 0.5920, -0.2021, -0.0935],\n",
      "         [ 0.4889, -0.1687, -0.0210],\n",
      "         [ 0.5745, -0.2239, -0.1011],\n",
      "         [ 0.6130, -0.2172, -0.1001],\n",
      "         [ 0.6228, -0.2267, -0.1091],\n",
      "         [ 0.6021, -0.2145, -0.0960],\n",
      "         [ 0.5991, -0.2045, -0.0901],\n",
      "         [ 0.5605, -0.1988, -0.0784],\n",
      "         [ 0.6087, -0.2203, -0.1066],\n",
      "         [ 0.5685, -0.2163, -0.0900],\n",
      "         [ 0.5411, -0.2070, -0.1084],\n",
      "         [ 0.6118, -0.2260, -0.1080],\n",
      "         [ 0.4783, -0.1577, -0.0391],\n",
      "         [ 0.5955, -0.2176, -0.0994],\n",
      "         [ 0.6100, -0.2230, -0.1075],\n",
      "         [ 0.5934, -0.2043, -0.0968],\n",
      "         [ 0.6162, -0.2226, -0.1069],\n",
      "         [ 0.4819, -0.1484, -0.0151],\n",
      "         [ 0.6282, -0.2306, -0.1100],\n",
      "         [ 0.4793, -0.1455, -0.0237],\n",
      "         [ 0.6047, -0.2222, -0.1031],\n",
      "         [ 0.6096, -0.2229, -0.1072],\n",
      "         [ 0.6129, -0.2146, -0.0669],\n",
      "         [ 0.5043, -0.1918, -0.0251],\n",
      "         [ 0.6050, -0.2202, -0.1055],\n",
      "         [ 0.6153, -0.2254, -0.1105],\n",
      "         [ 0.5310, -0.2054, -0.0641],\n",
      "         [ 0.5665, -0.2159, -0.0986],\n",
      "         [ 0.6144, -0.2249, -0.0943],\n",
      "         [ 0.5958, -0.2076, -0.1004]]], grad_fn=<StackBackward0>), tensor([[[ 1.2097, -0.5111, -0.3006],\n",
      "         [ 1.2000, -0.5228, -0.2919],\n",
      "         [ 1.1999, -0.4970, -0.2766],\n",
      "         [ 1.2358, -0.3818, -0.0433],\n",
      "         [ 1.2171, -0.5320, -0.2815],\n",
      "         [ 1.2055, -0.5330, -0.3277],\n",
      "         [ 1.2688, -0.5599, -0.3518],\n",
      "         [ 1.2393, -0.5340, -0.2862],\n",
      "         [ 1.2068, -0.4976, -0.2680],\n",
      "         [ 1.1647, -0.4555, -0.2154],\n",
      "         [ 1.2733, -0.5452, -0.3256],\n",
      "         [ 1.1955, -0.5069, -0.2491],\n",
      "         [ 1.2203, -0.5048, -0.2685],\n",
      "         [ 1.2209, -0.5575, -0.3478],\n",
      "         [ 1.1683, -0.3579, -0.0804],\n",
      "         [ 1.2132, -0.5336, -0.3026],\n",
      "         [ 1.2569, -0.5515, -0.3356],\n",
      "         [ 1.2054, -0.5050, -0.2870],\n",
      "         [ 1.2144, -0.5506, -0.3521],\n",
      "         [ 1.1832, -0.3278, -0.0311],\n",
      "         [ 1.2944, -0.5725, -0.3727],\n",
      "         [ 1.1553, -0.3215, -0.0488],\n",
      "         [ 1.2309, -0.5464, -0.3214],\n",
      "         [ 1.2551, -0.5511, -0.3345],\n",
      "         [ 1.2088, -0.4974, -0.2135],\n",
      "         [ 1.2387, -0.4111, -0.0573],\n",
      "         [ 1.2532, -0.5438, -0.3224],\n",
      "         [ 1.2080, -0.5603, -0.3655],\n",
      "         [ 1.1875, -0.4627, -0.1563],\n",
      "         [ 1.2092, -0.5131, -0.2721],\n",
      "         [ 1.2244, -0.5468, -0.3165],\n",
      "         [ 1.2120, -0.5153, -0.2985]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6047, -0.2257, -0.1016],\n",
      "         [ 0.6128, -0.2205, -0.0998],\n",
      "         [ 0.6159, -0.2300, -0.1114],\n",
      "         [ 0.6220, -0.2318, -0.1002],\n",
      "         [ 0.6099, -0.2269, -0.1155],\n",
      "         [ 0.5962, -0.2084, -0.0991],\n",
      "         [ 0.5567, -0.2262, -0.0858],\n",
      "         [ 0.5762, -0.2282, -0.1002],\n",
      "         [ 0.6062, -0.2240, -0.1071],\n",
      "         [ 0.6193, -0.2181, -0.0881],\n",
      "         [ 0.6153, -0.2298, -0.1066],\n",
      "         [ 0.5123, -0.2012, -0.0456],\n",
      "         [ 0.5037, -0.1887, -0.0322],\n",
      "         [ 0.5066, -0.1963, -0.0316],\n",
      "         [ 0.6123, -0.2274, -0.1092],\n",
      "         [ 0.5343, -0.2127, -0.0683],\n",
      "         [ 0.6162, -0.2306, -0.1124],\n",
      "         [ 0.5977, -0.2040, -0.0840],\n",
      "         [ 0.6310, -0.2331, -0.1085],\n",
      "         [ 0.6124, -0.2278, -0.1098],\n",
      "         [ 0.6278, -0.2345, -0.1117],\n",
      "         [ 0.5460, -0.1989, -0.0807],\n",
      "         [ 0.6101, -0.2254, -0.1126],\n",
      "         [ 0.5996, -0.2112, -0.1052],\n",
      "         [ 0.5346, -0.2135, -0.0686],\n",
      "         [ 0.6267, -0.2316, -0.1053],\n",
      "         [ 0.6301, -0.2341, -0.1120],\n",
      "         [ 0.5088, -0.2018, -0.0343],\n",
      "         [ 0.6235, -0.2317, -0.1018],\n",
      "         [ 0.5413, -0.2018, -0.0965],\n",
      "         [ 0.5802, -0.2284, -0.1025],\n",
      "         [ 0.6107, -0.2271, -0.1153]]], grad_fn=<StackBackward0>), tensor([[[ 1.2228, -0.5540, -0.3208],\n",
      "         [ 1.1930, -0.5396, -0.3317],\n",
      "         [ 1.2223, -0.5685, -0.3660],\n",
      "         [ 1.2460, -0.5686, -0.3432],\n",
      "         [ 1.2679, -0.5663, -0.3606],\n",
      "         [ 1.2001, -0.5160, -0.2991],\n",
      "         [ 1.2402, -0.5252, -0.2229],\n",
      "         [ 1.2020, -0.5414, -0.2852],\n",
      "         [ 1.2406, -0.5530, -0.3333],\n",
      "         [ 1.1951, -0.5208, -0.2931],\n",
      "         [ 1.2221, -0.5662, -0.3492],\n",
      "         [ 1.2300, -0.4345, -0.1057],\n",
      "         [ 1.2101, -0.4026, -0.0743],\n",
      "         [ 1.2304, -0.4215, -0.0730],\n",
      "         [ 1.2549, -0.5620, -0.3464],\n",
      "         [ 1.1835, -0.4823, -0.1689],\n",
      "         [ 1.2235, -0.5710, -0.3698],\n",
      "         [ 1.1899, -0.4921, -0.2532],\n",
      "         [ 1.2821, -0.5754, -0.3736],\n",
      "         [ 1.2552, -0.5634, -0.3484],\n",
      "         [ 1.2744, -0.5820, -0.3855],\n",
      "         [ 1.2036, -0.4670, -0.2006],\n",
      "         [ 1.2718, -0.5614, -0.3507],\n",
      "         [ 1.2107, -0.5259, -0.3188],\n",
      "         [ 1.1847, -0.4846, -0.1695],\n",
      "         [ 1.2613, -0.5691, -0.3621],\n",
      "         [ 1.2877, -0.5813, -0.3861],\n",
      "         [ 1.2429, -0.4361, -0.0792],\n",
      "         [ 1.2459, -0.5673, -0.3493],\n",
      "         [ 1.2036, -0.4833, -0.2399],\n",
      "         [ 1.2436, -0.5431, -0.2943],\n",
      "         [ 1.2726, -0.5673, -0.3600]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6057, -0.2319, -0.1010],\n",
      "         [ 0.6040, -0.2259, -0.0935],\n",
      "         [ 0.6083, -0.2320, -0.1106],\n",
      "         [ 0.6251, -0.2299, -0.0984],\n",
      "         [ 0.6172, -0.2285, -0.1070],\n",
      "         [ 0.5431, -0.2067, -0.1034],\n",
      "         [ 0.6146, -0.2310, -0.1179],\n",
      "         [ 0.5084, -0.2290, -0.0970],\n",
      "         [ 0.5822, -0.2161, -0.0659],\n",
      "         [ 0.5801, -0.2358, -0.0946],\n",
      "         [ 0.5437, -0.1989, -0.0771],\n",
      "         [ 0.5459, -0.2035, -0.0830],\n",
      "         [ 0.4838, -0.1773, -0.0486],\n",
      "         [ 0.5460, -0.2039, -0.0819],\n",
      "         [ 0.5969, -0.2128, -0.1012],\n",
      "         [ 0.6177, -0.2328, -0.1053],\n",
      "         [ 0.5149, -0.2040, -0.0440],\n",
      "         [ 0.6009, -0.2153, -0.1077],\n",
      "         [ 0.6102, -0.2227, -0.0926],\n",
      "         [ 0.4902, -0.1648, -0.0436],\n",
      "         [ 0.6185, -0.2337, -0.1134],\n",
      "         [ 0.5774, -0.2283, -0.1026],\n",
      "         [ 0.5557, -0.2343, -0.0933],\n",
      "         [ 0.6053, -0.2131, -0.0955],\n",
      "         [ 0.6191, -0.2301, -0.1101],\n",
      "         [ 0.5486, -0.2080, -0.0892],\n",
      "         [ 0.6207, -0.2213, -0.0888],\n",
      "         [ 0.5749, -0.2232, -0.0945],\n",
      "         [ 0.6290, -0.2381, -0.1135],\n",
      "         [ 0.6299, -0.2376, -0.1130],\n",
      "         [ 0.6280, -0.2352, -0.1071],\n",
      "         [ 0.6042, -0.2299, -0.1121]]], grad_fn=<StackBackward0>), tensor([[[ 1.2196, -0.5690, -0.3227],\n",
      "         [ 1.2358, -0.5543, -0.2926],\n",
      "         [ 1.2326, -0.5730, -0.3552],\n",
      "         [ 1.1998, -0.5564, -0.3351],\n",
      "         [ 1.2004, -0.5642, -0.3626],\n",
      "         [ 1.2052, -0.4971, -0.2586],\n",
      "         [ 1.2751, -0.5776, -0.3750],\n",
      "         [ 1.2490, -0.5274, -0.2273],\n",
      "         [ 1.1970, -0.4857, -0.1896],\n",
      "         [ 1.2215, -0.5568, -0.2757],\n",
      "         [ 1.1922, -0.4644, -0.1921],\n",
      "         [ 1.2003, -0.4792, -0.2073],\n",
      "         [ 1.1680, -0.4095, -0.1017],\n",
      "         [ 1.2019, -0.4802, -0.2044],\n",
      "         [ 1.1879, -0.5272, -0.3102],\n",
      "         [ 1.2211, -0.5725, -0.3500],\n",
      "         [ 1.2395, -0.4404, -0.1028],\n",
      "         [ 1.2001, -0.5365, -0.3316],\n",
      "         [ 1.1962, -0.5384, -0.3045],\n",
      "         [ 1.1694, -0.3719, -0.0913],\n",
      "         [ 1.2201, -0.5781, -0.3791],\n",
      "         [ 1.2120, -0.5389, -0.2984],\n",
      "         [ 1.2433, -0.5506, -0.2441],\n",
      "         [ 1.1990, -0.5204, -0.2942],\n",
      "         [ 1.2039, -0.5687, -0.3743],\n",
      "         [ 1.2039, -0.4921, -0.2236],\n",
      "         [ 1.1904, -0.5278, -0.2996],\n",
      "         [ 1.1798, -0.5224, -0.2732],\n",
      "         [ 1.2671, -0.5907, -0.3979],\n",
      "         [ 1.2720, -0.5891, -0.3958],\n",
      "         [ 1.2551, -0.5780, -0.3738],\n",
      "         [ 1.2187, -0.5703, -0.3555]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5475, -0.2073, -0.0884],\n",
      "         [ 0.6227, -0.2364, -0.1109],\n",
      "         [ 0.6200, -0.2374, -0.1111],\n",
      "         [ 0.6024, -0.2190, -0.1080],\n",
      "         [ 0.5462, -0.2136, -0.1037],\n",
      "         [ 0.5558, -0.2355, -0.0964],\n",
      "         [ 0.6109, -0.2338, -0.1159],\n",
      "         [ 0.6156, -0.2334, -0.1144],\n",
      "         [ 0.5989, -0.2242, -0.0949],\n",
      "         [ 0.5859, -0.2379, -0.1072],\n",
      "         [ 0.6148, -0.2334, -0.1166],\n",
      "         [ 0.5089, -0.1936, -0.0420],\n",
      "         [ 0.6356, -0.2411, -0.1160],\n",
      "         [ 0.6034, -0.2190, -0.1096],\n",
      "         [ 0.6300, -0.2389, -0.1088],\n",
      "         [ 0.6104, -0.2285, -0.0943],\n",
      "         [ 0.5139, -0.2133, -0.0459],\n",
      "         [ 0.6327, -0.2417, -0.1158],\n",
      "         [ 0.6217, -0.2396, -0.1034],\n",
      "         [ 0.5864, -0.2370, -0.1069],\n",
      "         [ 0.6276, -0.2367, -0.0744],\n",
      "         [ 0.6149, -0.2348, -0.1189],\n",
      "         [ 0.6211, -0.2369, -0.1155],\n",
      "         [ 0.6034, -0.2137, -0.0894],\n",
      "         [ 0.6201, -0.2383, -0.1135],\n",
      "         [ 0.5455, -0.2093, -0.0576],\n",
      "         [ 0.5433, -0.2111, -0.1050],\n",
      "         [ 0.6028, -0.2195, -0.1069],\n",
      "         [ 0.6207, -0.2372, -0.1168],\n",
      "         [ 0.6175, -0.2365, -0.1128],\n",
      "         [ 0.5433, -0.2111, -0.1050],\n",
      "         [ 0.6345, -0.2411, -0.1157]]], grad_fn=<StackBackward0>), tensor([[[ 1.1941, -0.4877, -0.2226],\n",
      "         [ 1.2256, -0.5818, -0.3753],\n",
      "         [ 1.2123, -0.5843, -0.3765],\n",
      "         [ 1.1938, -0.5454, -0.3376],\n",
      "         [ 1.2189, -0.5153, -0.2609],\n",
      "         [ 1.2433, -0.5524, -0.2530],\n",
      "         [ 1.2420, -0.5811, -0.3737],\n",
      "         [ 1.2556, -0.5789, -0.3688],\n",
      "         [ 1.1932, -0.5445, -0.3014],\n",
      "         [ 1.2065, -0.5673, -0.3184],\n",
      "         [ 1.2629, -0.5811, -0.3760],\n",
      "         [ 1.1944, -0.4116, -0.0988],\n",
      "         [ 1.2906, -0.5987, -0.4124],\n",
      "         [ 1.1975, -0.5461, -0.3428],\n",
      "         [ 1.2542, -0.5871, -0.3851],\n",
      "         [ 1.1861, -0.5529, -0.3148],\n",
      "         [ 1.2366, -0.4646, -0.1083],\n",
      "         [ 1.2745, -0.6000, -0.4119],\n",
      "         [ 1.2157, -0.5854, -0.3651],\n",
      "         [ 1.2089, -0.5644, -0.3173],\n",
      "         [ 1.2357, -0.5582, -0.2495],\n",
      "         [ 1.2611, -0.5858, -0.3840],\n",
      "         [ 1.2017, -0.5887, -0.3998],\n",
      "         [ 1.1837, -0.5177, -0.2782],\n",
      "         [ 1.2102, -0.5878, -0.3856],\n",
      "         [ 1.1918, -0.4636, -0.1455],\n",
      "         [ 1.2007, -0.5078, -0.2643],\n",
      "         [ 1.1961, -0.5465, -0.3340],\n",
      "         [ 1.1988, -0.5900, -0.4049],\n",
      "         [ 1.2061, -0.5832, -0.3820],\n",
      "         [ 1.2007, -0.5078, -0.2643],\n",
      "         [ 1.2844, -0.5986, -0.4114]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6300, -0.2441, -0.1136],\n",
      "         [ 0.5548, -0.2398, -0.1170],\n",
      "         [ 0.6104, -0.2384, -0.1157],\n",
      "         [ 0.5084, -0.2284, -0.0922],\n",
      "         [ 0.6273, -0.2438, -0.1066],\n",
      "         [ 0.6344, -0.2438, -0.1130],\n",
      "         [ 0.6215, -0.2420, -0.1143],\n",
      "         [ 0.4837, -0.1651, -0.0551],\n",
      "         [ 0.5437, -0.2256, -0.0802],\n",
      "         [ 0.5444, -0.2068, -0.0791],\n",
      "         [ 0.6119, -0.2402, -0.1117],\n",
      "         [ 0.5842, -0.2435, -0.1044],\n",
      "         [ 0.5848, -0.2449, -0.1177],\n",
      "         [ 0.5243, -0.2331, -0.0529],\n",
      "         [ 0.6334, -0.2443, -0.1155],\n",
      "         [ 0.6325, -0.2443, -0.1142],\n",
      "         [ 0.6038, -0.2205, -0.1059],\n",
      "         [ 0.6346, -0.2444, -0.1137],\n",
      "         [ 0.5519, -0.2169, -0.0751],\n",
      "         [ 0.6272, -0.2447, -0.1116],\n",
      "         [ 0.5859, -0.2390, -0.1013],\n",
      "         [ 0.6214, -0.2414, -0.1164],\n",
      "         [ 0.6066, -0.2336, -0.0878],\n",
      "         [ 0.6207, -0.2404, -0.1135],\n",
      "         [ 0.6194, -0.2363, -0.1105],\n",
      "         [ 0.5787, -0.2342, -0.1054],\n",
      "         [ 0.5493, -0.2122, -0.0915],\n",
      "         [ 0.6288, -0.2400, -0.0803],\n",
      "         [ 0.5851, -0.2405, -0.1173],\n",
      "         [ 0.5902, -0.2258, -0.0702],\n",
      "         [ 0.6075, -0.2189, -0.0932],\n",
      "         [ 0.6122, -0.2396, -0.1135]]], grad_fn=<StackBackward0>), tensor([[[ 1.2522, -0.6026, -0.4082],\n",
      "         [ 1.2388, -0.5684, -0.3096],\n",
      "         [ 1.2162, -0.5894, -0.3787],\n",
      "         [ 1.2294, -0.5201, -0.2190],\n",
      "         [ 1.2317, -0.5970, -0.3814],\n",
      "         [ 1.2640, -0.6007, -0.4063],\n",
      "         [ 1.2078, -0.5963, -0.3932],\n",
      "         [ 1.1356, -0.3771, -0.1165],\n",
      "         [ 1.1735, -0.5170, -0.2061],\n",
      "         [ 1.1790, -0.4818, -0.2000],\n",
      "         [ 1.2268, -0.5899, -0.3667],\n",
      "         [ 1.2292, -0.5769, -0.3087],\n",
      "         [ 1.2088, -0.5925, -0.3546],\n",
      "         [ 1.2901, -0.5180, -0.1259],\n",
      "         [ 1.2685, -0.6042, -0.4156],\n",
      "         [ 1.2644, -0.6035, -0.4104],\n",
      "         [ 1.1900, -0.5477, -0.3353],\n",
      "         [ 1.2646, -0.6027, -0.4091],\n",
      "         [ 1.1843, -0.4864, -0.1934],\n",
      "         [ 1.2277, -0.6014, -0.4015],\n",
      "         [ 1.2003, -0.5656, -0.3034],\n",
      "         [ 1.2131, -0.5967, -0.4004],\n",
      "         [ 1.1881, -0.5542, -0.2860],\n",
      "         [ 1.1941, -0.5961, -0.3970],\n",
      "         [ 1.1869, -0.5812, -0.3857],\n",
      "         [ 1.1848, -0.5562, -0.3146],\n",
      "         [ 1.1917, -0.4999, -0.2324],\n",
      "         [ 1.2272, -0.5679, -0.2734],\n",
      "         [ 1.2073, -0.5780, -0.3530],\n",
      "         [ 1.2008, -0.5085, -0.2088],\n",
      "         [ 1.1854, -0.5324, -0.2951],\n",
      "         [ 1.2267, -0.5885, -0.3732]]], grad_fn=<StackBackward0>))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.5864, -0.2475, -0.0907],\n",
      "         [ 0.6239, -0.2452, -0.1149],\n",
      "         [ 0.5478, -0.2194, -0.1134],\n",
      "         [ 0.6178, -0.2394, -0.1075],\n",
      "         [ 0.5440, -0.2168, -0.0784],\n",
      "         [ 0.5836, -0.2415, -0.1111],\n",
      "         [ 0.5441, -0.2168, -0.1016],\n",
      "         [ 0.6165, -0.2283, -0.0928],\n",
      "         [ 0.6268, -0.2366, -0.0990],\n",
      "         [ 0.6101, -0.2280, -0.1125],\n",
      "         [ 0.6247, -0.2466, -0.1006],\n",
      "         [ 0.5538, -0.2428, -0.1175],\n",
      "         [ 0.5071, -0.1957, -0.0695],\n",
      "         [ 0.5869, -0.2499, -0.1193],\n",
      "         [ 0.5420, -0.2142, -0.0742],\n",
      "         [ 0.5420, -0.2197, -0.1046],\n",
      "         [ 0.6200, -0.2399, -0.1105],\n",
      "         [ 0.6190, -0.2465, -0.0960],\n",
      "         [ 0.5170, -0.2237, -0.0499],\n",
      "         [ 0.5207, -0.2372, -0.0380],\n",
      "         [ 0.5573, -0.2353, -0.0940],\n",
      "         [ 0.5839, -0.2479, -0.0925],\n",
      "         [ 0.6351, -0.2478, -0.1171],\n",
      "         [ 0.5213, -0.2382, -0.0456],\n",
      "         [ 0.6300, -0.2468, -0.1112],\n",
      "         [ 0.6134, -0.2445, -0.0991],\n",
      "         [ 0.6269, -0.2385, -0.0988],\n",
      "         [ 0.6216, -0.2415, -0.1131],\n",
      "         [ 0.6324, -0.2486, -0.1170],\n",
      "         [ 0.5543, -0.2228, -0.0998],\n",
      "         [ 0.5479, -0.2204, -0.1129],\n",
      "         [ 0.5519, -0.2176, -0.0961]]], grad_fn=<StackBackward0>), tensor([[[ 1.2335, -0.5795, -0.2686],\n",
      "         [ 1.2113, -0.6033, -0.3998],\n",
      "         [ 1.1978, -0.5287, -0.2917],\n",
      "         [ 1.1918, -0.5829, -0.3711],\n",
      "         [ 1.1593, -0.4914, -0.2037],\n",
      "         [ 1.1943, -0.5764, -0.3374],\n",
      "         [ 1.1882, -0.5189, -0.2599],\n",
      "         [ 1.1633, -0.5447, -0.3237],\n",
      "         [ 1.1831, -0.5673, -0.3491],\n",
      "         [ 1.1999, -0.5698, -0.3638],\n",
      "         [ 1.2184, -0.6002, -0.3620],\n",
      "         [ 1.2346, -0.5740, -0.3115],\n",
      "         [ 1.2236, -0.4586, -0.1492],\n",
      "         [ 1.2059, -0.6044, -0.3647],\n",
      "         [ 1.1642, -0.4778, -0.1939],\n",
      "         [ 1.1874, -0.5292, -0.2677],\n",
      "         [ 1.1860, -0.5904, -0.3894],\n",
      "         [ 1.2366, -0.5942, -0.3173],\n",
      "         [ 1.2367, -0.4889, -0.1192],\n",
      "         [ 1.2736, -0.5222, -0.0909],\n",
      "         [ 1.2186, -0.5489, -0.2461],\n",
      "         [ 1.2189, -0.5810, -0.2743],\n",
      "         [ 1.2688, -0.6127, -0.4264],\n",
      "         [ 1.2693, -0.5269, -0.1091],\n",
      "         [ 1.2455, -0.6063, -0.4028],\n",
      "         [ 1.2038, -0.5875, -0.3284],\n",
      "         [ 1.1826, -0.5723, -0.3488],\n",
      "         [ 1.1880, -0.5949, -0.4001],\n",
      "         [ 1.2533, -0.6140, -0.4265],\n",
      "         [ 1.1983, -0.5299, -0.2572],\n",
      "         [ 1.1990, -0.5316, -0.2904],\n",
      "         [ 1.1889, -0.5138, -0.2471]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5465, -0.2237, -0.1086],\n",
      "         [ 0.6237, -0.2470, -0.1195],\n",
      "         [ 0.4994, -0.1930, -0.0368],\n",
      "         [ 0.6279, -0.2417, -0.1015],\n",
      "         [ 0.6241, -0.2498, -0.1182],\n",
      "         [ 0.5515, -0.2364, -0.0885],\n",
      "         [ 0.6345, -0.2526, -0.0974],\n",
      "         [ 0.5933, -0.2566, -0.1107],\n",
      "         [ 0.5457, -0.2444, -0.1016],\n",
      "         [ 0.5439, -0.1922, -0.0643],\n",
      "         [ 0.5213, -0.2241, -0.0637],\n",
      "         [ 0.6384, -0.2518, -0.1163],\n",
      "         [ 0.6144, -0.2488, -0.0994],\n",
      "         [ 0.6204, -0.2431, -0.1104],\n",
      "         [ 0.5440, -0.2337, -0.0794],\n",
      "         [ 0.4880, -0.1871, -0.0796],\n",
      "         [ 0.6098, -0.2315, -0.1125],\n",
      "         [ 0.5430, -0.2303, -0.0781],\n",
      "         [ 0.5897, -0.2528, -0.1106],\n",
      "         [ 0.6219, -0.2464, -0.1247],\n",
      "         [ 0.6175, -0.2468, -0.1234],\n",
      "         [ 0.6148, -0.2485, -0.0908],\n",
      "         [ 0.5448, -0.2355, -0.0853],\n",
      "         [ 0.5468, -0.2458, -0.1104],\n",
      "         [ 0.6133, -0.2464, -0.1191],\n",
      "         [ 0.4833, -0.1722, -0.0662],\n",
      "         [ 0.5460, -0.2229, -0.1049],\n",
      "         [ 0.5497, -0.2251, -0.1150],\n",
      "         [ 0.5481, -0.2320, -0.1216],\n",
      "         [ 0.5438, -0.2201, -0.1015],\n",
      "         [ 0.5438, -0.2201, -0.1015],\n",
      "         [ 0.6369, -0.2519, -0.1194]]], grad_fn=<StackBackward0>), tensor([[[ 1.1887, -0.5381, -0.2809],\n",
      "         [ 1.1876, -0.6114, -0.4300],\n",
      "         [ 1.2056, -0.4445, -0.0791],\n",
      "         [ 1.1794, -0.5806, -0.3625],\n",
      "         [ 1.2018, -0.6152, -0.4176],\n",
      "         [ 1.1691, -0.5466, -0.2352],\n",
      "         [ 1.2173, -0.6053, -0.3434],\n",
      "         [ 1.2120, -0.6155, -0.3421],\n",
      "         [ 1.2207, -0.5727, -0.2654],\n",
      "         [ 1.1191, -0.4157, -0.1692],\n",
      "         [ 1.2099, -0.4881, -0.1543],\n",
      "         [ 1.2676, -0.6199, -0.4281],\n",
      "         [ 1.2026, -0.5960, -0.3320],\n",
      "         [ 1.1806, -0.5968, -0.3935],\n",
      "         [ 1.1809, -0.5285, -0.2074],\n",
      "         [ 1.1514, -0.4407, -0.1707],\n",
      "         [ 1.1845, -0.5772, -0.3695],\n",
      "         [ 1.1854, -0.5210, -0.2032],\n",
      "         [ 1.1931, -0.6028, -0.3416],\n",
      "         [ 1.2635, -0.6132, -0.4195],\n",
      "         [ 1.2401, -0.6129, -0.4150],\n",
      "         [ 1.2152, -0.5933, -0.3015],\n",
      "         [ 1.1862, -0.5363, -0.2229],\n",
      "         [ 1.2164, -0.5772, -0.2897],\n",
      "         [ 1.2107, -0.6084, -0.4003],\n",
      "         [ 1.1365, -0.3992, -0.1413],\n",
      "         [ 1.1874, -0.5348, -0.2708],\n",
      "         [ 1.1983, -0.5431, -0.2985],\n",
      "         [ 1.1977, -0.5661, -0.3170],\n",
      "         [ 1.1775, -0.5253, -0.2617],\n",
      "         [ 1.1775, -0.5253, -0.2617],\n",
      "         [ 1.2687, -0.6223, -0.4401]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6294, -0.2543, -0.1010],\n",
      "         [ 0.6281, -0.2527, -0.1187],\n",
      "         [ 0.6313, -0.2502, -0.1048],\n",
      "         [ 0.6172, -0.2493, -0.1180],\n",
      "         [ 0.6396, -0.2554, -0.1208],\n",
      "         [ 0.6303, -0.2519, -0.1178],\n",
      "         [ 0.5894, -0.2550, -0.1248],\n",
      "         [ 0.5230, -0.2344, -0.0560],\n",
      "         [ 0.5402, -0.2474, -0.0981],\n",
      "         [ 0.5581, -0.2357, -0.1240],\n",
      "         [ 0.6208, -0.2527, -0.0964],\n",
      "         [ 0.6316, -0.2539, -0.1118],\n",
      "         [ 0.5639, -0.2290, -0.0697],\n",
      "         [ 0.5882, -0.2571, -0.1076],\n",
      "         [ 0.6095, -0.2319, -0.1070],\n",
      "         [ 0.6183, -0.2488, -0.1182],\n",
      "         [ 0.5881, -0.2543, -0.1220],\n",
      "         [ 0.5845, -0.2522, -0.1149],\n",
      "         [ 0.5633, -0.2282, -0.0689],\n",
      "         [ 0.6248, -0.2498, -0.1257],\n",
      "         [ 0.6109, -0.2498, -0.1061],\n",
      "         [ 0.6073, -0.2386, -0.0993],\n",
      "         [ 0.6309, -0.2493, -0.0874],\n",
      "         [ 0.6087, -0.2504, -0.1100],\n",
      "         [ 0.6235, -0.2503, -0.1255],\n",
      "         [ 0.6349, -0.2545, -0.1148],\n",
      "         [ 0.6319, -0.2555, -0.1132],\n",
      "         [ 0.6165, -0.2496, -0.1182],\n",
      "         [ 0.6298, -0.2453, -0.0746],\n",
      "         [ 0.5157, -0.2441, -0.0816],\n",
      "         [ 0.5531, -0.2289, -0.0873],\n",
      "         [ 0.5903, -0.2271, -0.0787]]], grad_fn=<StackBackward0>), tensor([[[ 1.2297, -0.6177, -0.3703],\n",
      "         [ 1.2141, -0.6216, -0.4236],\n",
      "         [ 1.1917, -0.6053, -0.3787],\n",
      "         [ 1.2233, -0.6142, -0.4005],\n",
      "         [ 1.2762, -0.6305, -0.4502],\n",
      "         [ 1.2263, -0.6195, -0.4196],\n",
      "         [ 1.1957, -0.6123, -0.3914],\n",
      "         [ 1.2404, -0.5146, -0.1367],\n",
      "         [ 1.2109, -0.5760, -0.2541],\n",
      "         [ 1.2003, -0.5754, -0.3350],\n",
      "         [ 1.2354, -0.6047, -0.3234],\n",
      "         [ 1.2307, -0.6196, -0.4136],\n",
      "         [ 1.1953, -0.5135, -0.1874],\n",
      "         [ 1.2403, -0.6111, -0.3245],\n",
      "         [ 1.1770, -0.5744, -0.3548],\n",
      "         [ 1.2290, -0.6129, -0.4008],\n",
      "         [ 1.1903, -0.6088, -0.3819],\n",
      "         [ 1.2344, -0.6010, -0.3457],\n",
      "         [ 1.1930, -0.5109, -0.1853],\n",
      "         [ 1.2687, -0.6210, -0.4284],\n",
      "         [ 1.2146, -0.6054, -0.3551],\n",
      "         [ 1.1948, -0.5760, -0.3312],\n",
      "         [ 1.2019, -0.5862, -0.3087],\n",
      "         [ 1.1975, -0.6066, -0.3700],\n",
      "         [ 1.2619, -0.6220, -0.4275],\n",
      "         [ 1.2457, -0.6236, -0.4258],\n",
      "         [ 1.2307, -0.6251, -0.4196],\n",
      "         [ 1.2194, -0.6151, -0.4013],\n",
      "         [ 1.2127, -0.5707, -0.2611],\n",
      "         [ 1.2576, -0.5583, -0.1981],\n",
      "         [ 1.1567, -0.5228, -0.2350],\n",
      "         [ 1.1655, -0.5050, -0.2423]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6176, -0.2372, -0.0987],\n",
      "         [ 0.5887, -0.2616, -0.1034],\n",
      "         [ 0.4887, -0.1613, -0.0386],\n",
      "         [ 0.6169, -0.2388, -0.1171],\n",
      "         [ 0.6300, -0.2465, -0.0839],\n",
      "         [ 0.5479, -0.2543, -0.0814],\n",
      "         [ 0.5955, -0.2442, -0.0817],\n",
      "         [ 0.6289, -0.2547, -0.1015],\n",
      "         [ 0.6342, -0.2605, -0.1083],\n",
      "         [ 0.5577, -0.2288, -0.0870],\n",
      "         [ 0.5691, -0.2349, -0.0986],\n",
      "         [ 0.5675, -0.2362, -0.1218],\n",
      "         [ 0.6239, -0.2522, -0.1194],\n",
      "         [ 0.6419, -0.2590, -0.1220],\n",
      "         [ 0.6296, -0.2555, -0.1228],\n",
      "         [ 0.6160, -0.2378, -0.1158],\n",
      "         [ 0.6156, -0.2382, -0.1141],\n",
      "         [ 0.6269, -0.2570, -0.1220],\n",
      "         [ 0.6156, -0.2382, -0.1141],\n",
      "         [ 0.6131, -0.2440, -0.1001],\n",
      "         [ 0.5717, -0.2367, -0.1056],\n",
      "         [ 0.5380, -0.2376, -0.0852],\n",
      "         [ 0.6328, -0.2559, -0.1216],\n",
      "         [ 0.6214, -0.2418, -0.1075],\n",
      "         [ 0.6158, -0.2378, -0.1151],\n",
      "         [ 0.5229, -0.2549, -0.1178],\n",
      "         [ 0.6190, -0.2399, -0.1155],\n",
      "         [ 0.5663, -0.2486, -0.0976],\n",
      "         [ 0.5693, -0.2357, -0.1225],\n",
      "         [ 0.6318, -0.2581, -0.1150],\n",
      "         [ 0.5613, -0.2489, -0.0955],\n",
      "         [ 0.6398, -0.2606, -0.1001]]], grad_fn=<StackBackward0>), tensor([[[ 1.1837, -0.5800, -0.3325],\n",
      "         [ 1.2395, -0.6200, -0.3136],\n",
      "         [ 1.1249, -0.3585, -0.0829],\n",
      "         [ 1.1919, -0.5964, -0.3975],\n",
      "         [ 1.2021, -0.5757, -0.2975],\n",
      "         [ 1.2319, -0.5809, -0.2102],\n",
      "         [ 1.1762, -0.5490, -0.2556],\n",
      "         [ 1.2345, -0.6219, -0.3593],\n",
      "         [ 1.2385, -0.6355, -0.4041],\n",
      "         [ 1.1636, -0.5219, -0.2378],\n",
      "         [ 1.1906, -0.5591, -0.2734],\n",
      "         [ 1.1965, -0.5732, -0.3398],\n",
      "         [ 1.2385, -0.6220, -0.4136],\n",
      "         [ 1.2803, -0.6387, -0.4598],\n",
      "         [ 1.2039, -0.6322, -0.4523],\n",
      "         [ 1.1882, -0.5925, -0.3926],\n",
      "         [ 1.1871, -0.5928, -0.3863],\n",
      "         [ 1.2070, -0.6334, -0.4408],\n",
      "         [ 1.1871, -0.5928, -0.3863],\n",
      "         [ 1.2048, -0.5909, -0.3408],\n",
      "         [ 1.1959, -0.5658, -0.2938],\n",
      "         [ 1.1923, -0.5374, -0.2192],\n",
      "         [ 1.2376, -0.6316, -0.4384],\n",
      "         [ 1.1923, -0.5970, -0.3651],\n",
      "         [ 1.1873, -0.5920, -0.3901],\n",
      "         [ 1.2552, -0.5927, -0.2937],\n",
      "         [ 1.2033, -0.5998, -0.3916],\n",
      "         [ 1.1995, -0.5824, -0.2686],\n",
      "         [ 1.2058, -0.5721, -0.3417],\n",
      "         [ 1.2234, -0.6301, -0.4314],\n",
      "         [ 1.1744, -0.5817, -0.2631],\n",
      "         [ 1.2289, -0.6247, -0.3606]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6001, -0.2654, -0.1034],\n",
      "         [ 0.5827, -0.2450, -0.1169],\n",
      "         [ 0.6010, -0.2482, -0.0896],\n",
      "         [ 0.5337, -0.2360, -0.0816],\n",
      "         [ 0.6306, -0.2579, -0.1209],\n",
      "         [ 0.5780, -0.2417, -0.1236],\n",
      "         [ 0.5314, -0.2271, -0.0777],\n",
      "         [ 0.5328, -0.2508, -0.0580],\n",
      "         [ 0.6244, -0.2560, -0.1108],\n",
      "         [ 0.6251, -0.2440, -0.1085],\n",
      "         [ 0.6419, -0.2623, -0.0882],\n",
      "         [ 0.6222, -0.2421, -0.1182],\n",
      "         [ 0.5648, -0.2382, -0.0846],\n",
      "         [ 0.5804, -0.2460, -0.1167],\n",
      "         [ 0.6256, -0.2567, -0.1229],\n",
      "         [ 0.5620, -0.2151, -0.0685],\n",
      "         [ 0.6342, -0.2542, -0.1094],\n",
      "         [ 0.4932, -0.2062, -0.0765],\n",
      "         [ 0.5744, -0.2535, -0.1015],\n",
      "         [ 0.6302, -0.2566, -0.1170],\n",
      "         [ 0.6435, -0.2623, -0.1226],\n",
      "         [ 0.6232, -0.2541, -0.1028],\n",
      "         [ 0.6215, -0.2421, -0.1029],\n",
      "         [ 0.6204, -0.2414, -0.1194],\n",
      "         [ 0.5802, -0.2423, -0.1069],\n",
      "         [ 0.5009, -0.1881, -0.0384],\n",
      "         [ 0.5479, -0.2579, -0.1234],\n",
      "         [ 0.6015, -0.2675, -0.1219],\n",
      "         [ 0.6272, -0.2557, -0.1181],\n",
      "         [ 0.6373, -0.2601, -0.1215],\n",
      "         [ 0.5980, -0.2650, -0.1314],\n",
      "         [ 0.6198, -0.2417, -0.1193]]], grad_fn=<StackBackward0>), tensor([[[ 1.2638, -0.6239, -0.3179],\n",
      "         [ 1.2087, -0.5924, -0.3377],\n",
      "         [ 1.1856, -0.5597, -0.2840],\n",
      "         [ 1.1875, -0.5289, -0.2085],\n",
      "         [ 1.2042, -0.6350, -0.4491],\n",
      "         [ 1.2110, -0.5885, -0.3562],\n",
      "         [ 1.1758, -0.5032, -0.1981],\n",
      "         [ 1.2655, -0.5548, -0.1451],\n",
      "         [ 1.2338, -0.6292, -0.3878],\n",
      "         [ 1.2003, -0.6024, -0.3737],\n",
      "         [ 1.2509, -0.6235, -0.3173],\n",
      "         [ 1.2051, -0.6045, -0.4074],\n",
      "         [ 1.1816, -0.5466, -0.2352],\n",
      "         [ 1.2315, -0.6017, -0.3349],\n",
      "         [ 1.2276, -0.6343, -0.4352],\n",
      "         [ 1.1417, -0.4701, -0.1903],\n",
      "         [ 1.1982, -0.6139, -0.4040],\n",
      "         [ 1.1684, -0.4898, -0.1650],\n",
      "         [ 1.2180, -0.5970, -0.2849],\n",
      "         [ 1.2072, -0.6308, -0.4321],\n",
      "         [ 1.2815, -0.6455, -0.4669],\n",
      "         [ 1.2379, -0.6224, -0.3570],\n",
      "         [ 1.1902, -0.5948, -0.3526],\n",
      "         [ 1.1953, -0.6024, -0.4120],\n",
      "         [ 1.2066, -0.5822, -0.3069],\n",
      "         [ 1.1965, -0.4273, -0.0826],\n",
      "         [ 1.2757, -0.6068, -0.3195],\n",
      "         [ 1.2219, -0.6401, -0.3920],\n",
      "         [ 1.2392, -0.6301, -0.4161],\n",
      "         [ 1.2486, -0.6403, -0.4430],\n",
      "         [ 1.2209, -0.6381, -0.4232],\n",
      "         [ 1.1927, -0.6030, -0.4118]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6230, -0.2450, -0.1203],\n",
      "         [ 0.6179, -0.2651, -0.1094],\n",
      "         [ 0.5315, -0.2642, -0.1250],\n",
      "         [ 0.6261, -0.2667, -0.1235],\n",
      "         [ 0.6209, -0.2452, -0.1131],\n",
      "         [ 0.5906, -0.2634, -0.1196],\n",
      "         [ 0.6336, -0.2615, -0.1223],\n",
      "         [ 0.5776, -0.2432, -0.0778],\n",
      "         [ 0.5882, -0.2477, -0.1117],\n",
      "         [ 0.5955, -0.2661, -0.1077],\n",
      "         [ 0.5826, -0.2471, -0.0897],\n",
      "         [ 0.6306, -0.2595, -0.1175],\n",
      "         [ 0.5275, -0.2262, -0.0549],\n",
      "         [ 0.6265, -0.2692, -0.0937],\n",
      "         [ 0.6349, -0.2643, -0.1198],\n",
      "         [ 0.6422, -0.2681, -0.1141],\n",
      "         [ 0.6447, -0.2654, -0.1230],\n",
      "         [ 0.5889, -0.2472, -0.1169],\n",
      "         [ 0.5787, -0.2413, -0.1120],\n",
      "         [ 0.4993, -0.2137, -0.1053],\n",
      "         [ 0.6397, -0.2670, -0.1091],\n",
      "         [ 0.6344, -0.2630, -0.1239],\n",
      "         [ 0.6470, -0.2656, -0.1242],\n",
      "         [ 0.5747, -0.2387, -0.0792],\n",
      "         [ 0.6419, -0.2672, -0.1188],\n",
      "         [ 0.6281, -0.2596, -0.1244],\n",
      "         [ 0.5098, -0.2119, -0.0768],\n",
      "         [ 0.6258, -0.2466, -0.1086],\n",
      "         [ 0.5345, -0.2454, -0.0817],\n",
      "         [ 0.5849, -0.2508, -0.1308],\n",
      "         [ 0.5413, -0.2665, -0.0625],\n",
      "         [ 0.5366, -0.2532, -0.0899]]], grad_fn=<StackBackward0>), tensor([[[ 1.1970, -0.6108, -0.4210],\n",
      "         [ 1.2402, -0.6407, -0.3759],\n",
      "         [ 1.2675, -0.6160, -0.3204],\n",
      "         [ 1.2708, -0.6506, -0.4293],\n",
      "         [ 1.1935, -0.6095, -0.3931],\n",
      "         [ 1.2496, -0.6278, -0.3693],\n",
      "         [ 1.2129, -0.6435, -0.4587],\n",
      "         [ 1.1837, -0.5465, -0.2215],\n",
      "         [ 1.2135, -0.5979, -0.3305],\n",
      "         [ 1.1974, -0.6258, -0.3450],\n",
      "         [ 1.1963, -0.5621, -0.2567],\n",
      "         [ 1.2447, -0.6390, -0.4196],\n",
      "         [ 1.1982, -0.4819, -0.1385],\n",
      "         [ 1.2496, -0.6375, -0.3221],\n",
      "         [ 1.2231, -0.6530, -0.4480],\n",
      "         [ 1.2635, -0.6553, -0.4359],\n",
      "         [ 1.2829, -0.6519, -0.4726],\n",
      "         [ 1.2107, -0.5971, -0.3473],\n",
      "         [ 1.1938, -0.5813, -0.3295],\n",
      "         [ 1.2150, -0.5193, -0.2277],\n",
      "         [ 1.2536, -0.6490, -0.4148],\n",
      "         [ 1.2310, -0.6465, -0.4569],\n",
      "         [ 1.2943, -0.6533, -0.4778],\n",
      "         [ 1.1676, -0.5340, -0.2255],\n",
      "         [ 1.2710, -0.6557, -0.4548],\n",
      "         [ 1.2323, -0.6422, -0.4472],\n",
      "         [ 1.2366, -0.5022, -0.1660],\n",
      "         [ 1.1987, -0.6088, -0.3785],\n",
      "         [ 1.2094, -0.5530, -0.2079],\n",
      "         [ 1.2159, -0.6166, -0.3905],\n",
      "         [ 1.3028, -0.5992, -0.1584],\n",
      "         [ 1.2103, -0.5760, -0.2297]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6259, -0.2720, -0.1230],\n",
      "         [ 0.5438, -0.2653, -0.0753],\n",
      "         [ 0.6453, -0.2721, -0.0864],\n",
      "         [ 0.6446, -0.2697, -0.1242],\n",
      "         [ 0.5958, -0.2569, -0.1219],\n",
      "         [ 0.6384, -0.2636, -0.0794],\n",
      "         [ 0.4976, -0.2156, -0.1051],\n",
      "         [ 0.5413, -0.2643, -0.1255],\n",
      "         [ 0.6272, -0.2570, -0.1098],\n",
      "         [ 0.6285, -0.2607, -0.1179],\n",
      "         [ 0.5922, -0.2676, -0.1197],\n",
      "         [ 0.5814, -0.2610, -0.0852],\n",
      "         [ 0.5940, -0.2541, -0.1183],\n",
      "         [ 0.6307, -0.2560, -0.0834],\n",
      "         [ 0.6271, -0.2502, -0.1149],\n",
      "         [ 0.6271, -0.2502, -0.1149],\n",
      "         [ 0.6362, -0.2636, -0.1105],\n",
      "         [ 0.6307, -0.2546, -0.1166],\n",
      "         [ 0.6306, -0.2633, -0.1215],\n",
      "         [ 0.5930, -0.2546, -0.1132],\n",
      "         [ 0.6352, -0.2628, -0.1101],\n",
      "         [ 0.6369, -0.2628, -0.1265],\n",
      "         [ 0.6343, -0.2650, -0.1214],\n",
      "         [ 0.6288, -0.2623, -0.1130],\n",
      "         [ 0.6193, -0.2692, -0.1253],\n",
      "         [ 0.5394, -0.2647, -0.0601],\n",
      "         [ 0.6485, -0.2727, -0.0961],\n",
      "         [ 0.6386, -0.2692, -0.1215],\n",
      "         [ 0.4925, -0.2050, -0.0747],\n",
      "         [ 0.6344, -0.2636, -0.1165],\n",
      "         [ 0.6274, -0.2645, -0.1261],\n",
      "         [ 0.6455, -0.2686, -0.1230]]], grad_fn=<StackBackward0>), tensor([[[ 1.2715, -0.6618, -0.4287],\n",
      "         [ 1.2807, -0.5937, -0.1934],\n",
      "         [ 1.2627, -0.6458, -0.3148],\n",
      "         [ 1.2788, -0.6614, -0.4802],\n",
      "         [ 1.2179, -0.6266, -0.3724],\n",
      "         [ 1.2330, -0.6143, -0.2878],\n",
      "         [ 1.2058, -0.5235, -0.2272],\n",
      "         [ 1.2617, -0.6179, -0.3223],\n",
      "         [ 1.1874, -0.6194, -0.4085],\n",
      "         [ 1.2258, -0.6387, -0.4255],\n",
      "         [ 1.2612, -0.6384, -0.3708],\n",
      "         [ 1.2007, -0.6041, -0.2475],\n",
      "         [ 1.2103, -0.6162, -0.3601],\n",
      "         [ 1.1941, -0.5930, -0.3025],\n",
      "         [ 1.1946, -0.6188, -0.4060],\n",
      "         [ 1.1946, -0.6188, -0.4060],\n",
      "         [ 1.2079, -0.6382, -0.4136],\n",
      "         [ 1.2103, -0.6340, -0.4127],\n",
      "         [ 1.2340, -0.6489, -0.4400],\n",
      "         [ 1.2117, -0.6171, -0.3433],\n",
      "         [ 1.2014, -0.6343, -0.4124],\n",
      "         [ 1.2639, -0.6505, -0.4599],\n",
      "         [ 1.2261, -0.6466, -0.4489],\n",
      "         [ 1.2329, -0.6433, -0.4060],\n",
      "         [ 1.2462, -0.6535, -0.4368],\n",
      "         [ 1.2771, -0.5894, -0.1537],\n",
      "         [ 1.2640, -0.6501, -0.3530],\n",
      "         [ 1.2490, -0.6573, -0.4688],\n",
      "         [ 1.1802, -0.4862, -0.1603],\n",
      "         [ 1.2201, -0.6452, -0.4362],\n",
      "         [ 1.2186, -0.6542, -0.4593],\n",
      "         [ 1.2788, -0.6572, -0.4748]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6353, -0.2668, -0.1221],\n",
      "         [ 0.5339, -0.2692, -0.1125],\n",
      "         [ 0.6248, -0.2526, -0.1210],\n",
      "         [ 0.6442, -0.2757, -0.1156],\n",
      "         [ 0.5941, -0.2431, -0.0781],\n",
      "         [ 0.6323, -0.2661, -0.1260],\n",
      "         [ 0.5967, -0.2736, -0.0842],\n",
      "         [ 0.6404, -0.2727, -0.1245],\n",
      "         [ 0.5451, -0.2677, -0.0901],\n",
      "         [ 0.5984, -0.2591, -0.1197],\n",
      "         [ 0.6042, -0.2835, -0.1202],\n",
      "         [ 0.4921, -0.2087, -0.0903],\n",
      "         [ 0.5461, -0.2554, -0.0782],\n",
      "         [ 0.6011, -0.2797, -0.1285],\n",
      "         [ 0.6244, -0.2521, -0.1225],\n",
      "         [ 0.6008, -0.2602, -0.1260],\n",
      "         [ 0.6367, -0.2662, -0.1272],\n",
      "         [ 0.6359, -0.2698, -0.1271],\n",
      "         [ 0.6191, -0.2719, -0.1206],\n",
      "         [ 0.6400, -0.2731, -0.1207],\n",
      "         [ 0.6331, -0.2672, -0.1232],\n",
      "         [ 0.6001, -0.2766, -0.1328],\n",
      "         [ 0.5883, -0.2685, -0.1251],\n",
      "         [ 0.6478, -0.2757, -0.1067],\n",
      "         [ 0.5030, -0.1939, -0.0666],\n",
      "         [ 0.6245, -0.2522, -0.1218],\n",
      "         [ 0.6238, -0.2524, -0.1200],\n",
      "         [ 0.6440, -0.2725, -0.1251],\n",
      "         [ 0.6066, -0.2767, -0.1064],\n",
      "         [ 0.6390, -0.2675, -0.1166],\n",
      "         [ 0.4908, -0.2082, -0.0845],\n",
      "         [ 0.6126, -0.2692, -0.1071]]], grad_fn=<StackBackward0>), tensor([[[ 1.2457, -0.6587, -0.4483],\n",
      "         [ 1.2348, -0.6259, -0.2864],\n",
      "         [ 1.1933, -0.6282, -0.4308],\n",
      "         [ 1.2682, -0.6719, -0.4460],\n",
      "         [ 1.1604, -0.5306, -0.2490],\n",
      "         [ 1.2278, -0.6568, -0.4648],\n",
      "         [ 1.2523, -0.6307, -0.2602],\n",
      "         [ 1.2538, -0.6658, -0.4841],\n",
      "         [ 1.2554, -0.6090, -0.2293],\n",
      "         [ 1.2317, -0.6350, -0.3712],\n",
      "         [ 1.2326, -0.6781, -0.3921],\n",
      "         [ 1.1865, -0.4992, -0.1938],\n",
      "         [ 1.2363, -0.5597, -0.2042],\n",
      "         [ 1.2344, -0.6718, -0.4196],\n",
      "         [ 1.1906, -0.6270, -0.4367],\n",
      "         [ 1.2156, -0.6358, -0.3951],\n",
      "         [ 1.2539, -0.6587, -0.4696],\n",
      "         [ 1.2160, -0.6639, -0.4852],\n",
      "         [ 1.2352, -0.6531, -0.4208],\n",
      "         [ 1.2203, -0.6693, -0.4602],\n",
      "         [ 1.2355, -0.6604, -0.4534],\n",
      "         [ 1.2241, -0.6619, -0.4351],\n",
      "         [ 1.2282, -0.6370, -0.3912],\n",
      "         [ 1.2520, -0.6607, -0.3967],\n",
      "         [ 1.1836, -0.4458, -0.1430],\n",
      "         [ 1.1914, -0.6271, -0.4339],\n",
      "         [ 1.1908, -0.6277, -0.4265],\n",
      "         [ 1.2722, -0.6663, -0.4865],\n",
      "         [ 1.2811, -0.6490, -0.3333],\n",
      "         [ 1.2142, -0.6489, -0.4417],\n",
      "         [ 1.1696, -0.4956, -0.1813],\n",
      "         [ 1.2154, -0.6408, -0.3692]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6450, -0.2754, -0.1255],\n",
      "         [ 0.6359, -0.2748, -0.1246],\n",
      "         [ 0.6326, -0.2715, -0.1252],\n",
      "         [ 0.6002, -0.2628, -0.1212],\n",
      "         [ 0.4960, -0.2177, -0.1110],\n",
      "         [ 0.6378, -0.2719, -0.1185],\n",
      "         [ 0.5940, -0.2533, -0.0959],\n",
      "         [ 0.6391, -0.2719, -0.0833],\n",
      "         [ 0.5286, -0.2698, -0.1034],\n",
      "         [ 0.5421, -0.2707, -0.0920],\n",
      "         [ 0.6235, -0.2557, -0.1240],\n",
      "         [ 0.6437, -0.2757, -0.1264],\n",
      "         [ 0.6227, -0.2765, -0.1330],\n",
      "         [ 0.5014, -0.1985, -0.0622],\n",
      "         [ 0.6337, -0.2703, -0.1124],\n",
      "         [ 0.5548, -0.2812, -0.1012],\n",
      "         [ 0.4913, -0.2136, -0.0918],\n",
      "         [ 0.5983, -0.2611, -0.0879],\n",
      "         [ 0.6233, -0.2555, -0.1240],\n",
      "         [ 0.5965, -0.2819, -0.1079],\n",
      "         [ 0.6382, -0.2739, -0.1191],\n",
      "         [ 0.6179, -0.2802, -0.0968],\n",
      "         [ 0.6030, -0.2737, -0.1092],\n",
      "         [ 0.6258, -0.2567, -0.1235],\n",
      "         [ 0.5887, -0.2757, -0.1221],\n",
      "         [ 0.5427, -0.2719, -0.0868],\n",
      "         [ 0.6403, -0.2757, -0.1192],\n",
      "         [ 0.6196, -0.2787, -0.0860],\n",
      "         [ 0.6263, -0.2572, -0.1192],\n",
      "         [ 0.6316, -0.2736, -0.1155],\n",
      "         [ 0.6379, -0.2666, -0.0856],\n",
      "         [ 0.6266, -0.2570, -0.1196]]], grad_fn=<StackBackward0>), tensor([[[ 1.2727, -0.6724, -0.4889],\n",
      "         [ 1.2168, -0.6756, -0.4753],\n",
      "         [ 1.2001, -0.6642, -0.4781],\n",
      "         [ 1.1982, -0.6392, -0.3849],\n",
      "         [ 1.2047, -0.5286, -0.2389],\n",
      "         [ 1.2078, -0.6600, -0.4508],\n",
      "         [ 1.1875, -0.5745, -0.2882],\n",
      "         [ 1.2339, -0.6348, -0.3042],\n",
      "         [ 1.2014, -0.6198, -0.2614],\n",
      "         [ 1.2454, -0.6144, -0.2329],\n",
      "         [ 1.1825, -0.6353, -0.4441],\n",
      "         [ 1.2647, -0.6732, -0.4931],\n",
      "         [ 1.2638, -0.6704, -0.4682],\n",
      "         [ 1.1840, -0.4568, -0.1331],\n",
      "         [ 1.1947, -0.6527, -0.4247],\n",
      "         [ 1.2857, -0.6431, -0.2702],\n",
      "         [ 1.1891, -0.5131, -0.1964],\n",
      "         [ 1.1661, -0.5797, -0.2832],\n",
      "         [ 1.1811, -0.6346, -0.4442],\n",
      "         [ 1.2705, -0.6685, -0.3350],\n",
      "         [ 1.2098, -0.6670, -0.4536],\n",
      "         [ 1.2039, -0.6555, -0.3359],\n",
      "         [ 1.2470, -0.6467, -0.3311],\n",
      "         [ 1.1935, -0.6390, -0.4423],\n",
      "         [ 1.2435, -0.6568, -0.3797],\n",
      "         [ 1.2566, -0.6177, -0.2193],\n",
      "         [ 1.2517, -0.6696, -0.4614],\n",
      "         [ 1.2247, -0.6472, -0.2959],\n",
      "         [ 1.1833, -0.6360, -0.4271],\n",
      "         [ 1.2171, -0.6640, -0.4275],\n",
      "         [ 1.2281, -0.6205, -0.3124],\n",
      "         [ 1.1844, -0.6357, -0.4287]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6380, -0.2794, -0.1278],\n",
      "         [ 0.5883, -0.2731, -0.1024],\n",
      "         [ 0.6420, -0.2775, -0.1273],\n",
      "         [ 0.5996, -0.2501, -0.0773],\n",
      "         [ 0.6367, -0.2767, -0.1308],\n",
      "         [ 0.5973, -0.2621, -0.1314],\n",
      "         [ 0.6455, -0.2798, -0.1291],\n",
      "         [ 0.6231, -0.2576, -0.1240],\n",
      "         [ 0.6360, -0.2801, -0.1230],\n",
      "         [ 0.5991, -0.2621, -0.1299],\n",
      "         [ 0.5996, -0.2501, -0.0773],\n",
      "         [ 0.5922, -0.2856, -0.1093],\n",
      "         [ 0.6029, -0.2892, -0.1293],\n",
      "         [ 0.5421, -0.2619, -0.0746],\n",
      "         [ 0.6329, -0.2731, -0.1096],\n",
      "         [ 0.6305, -0.2779, -0.1298],\n",
      "         [ 0.6362, -0.2712, -0.1287],\n",
      "         [ 0.4911, -0.2190, -0.1102],\n",
      "         [ 0.6307, -0.2723, -0.1227],\n",
      "         [ 0.6247, -0.2587, -0.1158],\n",
      "         [ 0.6402, -0.2804, -0.1287],\n",
      "         [ 0.5930, -0.2540, -0.0873],\n",
      "         [ 0.6026, -0.2664, -0.1271],\n",
      "         [ 0.5939, -0.2499, -0.0817],\n",
      "         [ 0.5533, -0.2855, -0.0811],\n",
      "         [ 0.6335, -0.2757, -0.1212],\n",
      "         [ 0.6385, -0.2788, -0.1275],\n",
      "         [ 0.6267, -0.2863, -0.1047],\n",
      "         [ 0.6292, -0.2630, -0.1157],\n",
      "         [ 0.6268, -0.2591, -0.1228],\n",
      "         [ 0.5327, -0.2743, -0.0953],\n",
      "         [ 0.6342, -0.2796, -0.1225]]], grad_fn=<StackBackward0>), tensor([[[ 1.2395, -0.6802, -0.4997],\n",
      "         [ 1.2004, -0.6476, -0.3102],\n",
      "         [ 1.2580, -0.6755, -0.4963],\n",
      "         [ 1.1863, -0.5440, -0.2468],\n",
      "         [ 1.2353, -0.6763, -0.4915],\n",
      "         [ 1.1788, -0.6397, -0.4249],\n",
      "         [ 1.2782, -0.6841, -0.5050],\n",
      "         [ 1.1790, -0.6383, -0.4446],\n",
      "         [ 1.2006, -0.6844, -0.4711],\n",
      "         [ 1.1883, -0.6397, -0.4194],\n",
      "         [ 1.1863, -0.5440, -0.2468],\n",
      "         [ 1.2605, -0.6765, -0.3364],\n",
      "         [ 1.2188, -0.6902, -0.4263],\n",
      "         [ 1.2211, -0.5763, -0.1989],\n",
      "         [ 1.1957, -0.6562, -0.4127],\n",
      "         [ 1.2053, -0.6787, -0.4882],\n",
      "         [ 1.2299, -0.6704, -0.4846],\n",
      "         [ 1.1900, -0.5317, -0.2355],\n",
      "         [ 1.1960, -0.6623, -0.4656],\n",
      "         [ 1.1777, -0.6361, -0.4137],\n",
      "         [ 1.2502, -0.6844, -0.5039],\n",
      "         [ 1.1747, -0.5706, -0.2645],\n",
      "         [ 1.1917, -0.6504, -0.4113],\n",
      "         [ 1.1498, -0.5423, -0.2621],\n",
      "         [ 1.2852, -0.6488, -0.2175],\n",
      "         [ 1.1905, -0.6692, -0.4617],\n",
      "         [ 1.2420, -0.6783, -0.4983],\n",
      "         [ 1.2452, -0.6741, -0.3637],\n",
      "         [ 1.2158, -0.6560, -0.4112],\n",
      "         [ 1.1971, -0.6438, -0.4393],\n",
      "         [ 1.2535, -0.6304, -0.2376],\n",
      "         [ 1.2138, -0.6876, -0.4652]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5611, -0.2897, -0.1094],\n",
      "         [ 0.6356, -0.2780, -0.0926],\n",
      "         [ 0.6305, -0.2736, -0.0922],\n",
      "         [ 0.6125, -0.2804, -0.0887],\n",
      "         [ 0.6059, -0.2795, -0.1088],\n",
      "         [ 0.6228, -0.2641, -0.1214],\n",
      "         [ 0.4839, -0.2155, -0.0762],\n",
      "         [ 0.5964, -0.2653, -0.1312],\n",
      "         [ 0.6401, -0.2845, -0.1258],\n",
      "         [ 0.6047, -0.2730, -0.1261],\n",
      "         [ 0.6316, -0.2746, -0.1297],\n",
      "         [ 0.5585, -0.2900, -0.0939],\n",
      "         [ 0.6073, -0.2682, -0.1225],\n",
      "         [ 0.4858, -0.2218, -0.1099],\n",
      "         [ 0.6194, -0.2627, -0.1265],\n",
      "         [ 0.6254, -0.2888, -0.1053],\n",
      "         [ 0.6152, -0.2851, -0.1374],\n",
      "         [ 0.5268, -0.2724, -0.0848],\n",
      "         [ 0.6419, -0.2837, -0.1308],\n",
      "         [ 0.6179, -0.2626, -0.1229],\n",
      "         [ 0.4861, -0.1769, -0.0457],\n",
      "         [ 0.6302, -0.2745, -0.0904],\n",
      "         [ 0.5994, -0.2681, -0.1230],\n",
      "         [ 0.6299, -0.2816, -0.1317],\n",
      "         [ 0.6318, -0.2752, -0.1179],\n",
      "         [ 0.5580, -0.2926, -0.0888],\n",
      "         [ 0.5469, -0.2851, -0.1368],\n",
      "         [ 0.5578, -0.2936, -0.0807],\n",
      "         [ 0.5296, -0.2768, -0.1051],\n",
      "         [ 0.6297, -0.2777, -0.1200],\n",
      "         [ 0.5610, -0.2887, -0.1096],\n",
      "         [ 0.6328, -0.2723, -0.1297]]], grad_fn=<StackBackward0>), tensor([[[ 1.2814, -0.6660, -0.3005],\n",
      "         [ 1.2161, -0.6510, -0.3395],\n",
      "         [ 1.1955, -0.6379, -0.3376],\n",
      "         [ 1.1995, -0.6433, -0.3026],\n",
      "         [ 1.2296, -0.6590, -0.3371],\n",
      "         [ 1.1684, -0.6538, -0.4355],\n",
      "         [ 1.1892, -0.5175, -0.1599],\n",
      "         [ 1.1623, -0.6467, -0.4288],\n",
      "         [ 1.2535, -0.6932, -0.4894],\n",
      "         [ 1.1923, -0.6697, -0.4118],\n",
      "         [ 1.2007, -0.6782, -0.4916],\n",
      "         [ 1.2885, -0.6644, -0.2556],\n",
      "         [ 1.2075, -0.6537, -0.3982],\n",
      "         [ 1.1804, -0.5406, -0.2328],\n",
      "         [ 1.1603, -0.6520, -0.4546],\n",
      "         [ 1.2445, -0.6771, -0.3636],\n",
      "         [ 1.2375, -0.6884, -0.4813],\n",
      "         [ 1.1957, -0.6118, -0.2104],\n",
      "         [ 1.2604, -0.6928, -0.5116],\n",
      "         [ 1.1565, -0.6509, -0.4402],\n",
      "         [ 1.1375, -0.3953, -0.0959],\n",
      "         [ 1.1923, -0.6380, -0.3308],\n",
      "         [ 1.1699, -0.6520, -0.4004],\n",
      "         [ 1.2042, -0.6874, -0.4955],\n",
      "         [ 1.2113, -0.6762, -0.4410],\n",
      "         [ 1.2907, -0.6709, -0.2415],\n",
      "         [ 1.2477, -0.6664, -0.3773],\n",
      "         [ 1.2966, -0.6712, -0.2188],\n",
      "         [ 1.2458, -0.6383, -0.2605],\n",
      "         [ 1.1798, -0.6717, -0.4543],\n",
      "         [ 1.2806, -0.6630, -0.3010],\n",
      "         [ 1.2073, -0.6708, -0.4908]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5972, -0.2685, -0.1257],\n",
      "         [ 0.5782, -0.2794, -0.0928],\n",
      "         [ 0.5903, -0.2793, -0.1191],\n",
      "         [ 0.6330, -0.2829, -0.1344],\n",
      "         [ 0.5473, -0.2840, -0.1201],\n",
      "         [ 0.5874, -0.2715, -0.0997],\n",
      "         [ 0.5274, -0.2764, -0.0862],\n",
      "         [ 0.6386, -0.2870, -0.1322],\n",
      "         [ 0.6296, -0.2736, -0.1184],\n",
      "         [ 0.6264, -0.2863, -0.1304],\n",
      "         [ 0.6332, -0.2745, -0.1314],\n",
      "         [ 0.4905, -0.2235, -0.0702],\n",
      "         [ 0.6278, -0.2843, -0.1319],\n",
      "         [ 0.5897, -0.2601, -0.0835],\n",
      "         [ 0.5918, -0.2889, -0.1368],\n",
      "         [ 0.4735, -0.2027, -0.0882],\n",
      "         [ 0.5837, -0.2334, -0.0753],\n",
      "         [ 0.6268, -0.2747, -0.1240],\n",
      "         [ 0.6237, -0.2848, -0.1329],\n",
      "         [ 0.6275, -0.2828, -0.1330],\n",
      "         [ 0.6259, -0.2866, -0.1152],\n",
      "         [ 0.6037, -0.2805, -0.1104],\n",
      "         [ 0.6373, -0.2856, -0.1315],\n",
      "         [ 0.6343, -0.2867, -0.1186],\n",
      "         [ 0.6370, -0.2868, -0.1321],\n",
      "         [ 0.6352, -0.2756, -0.1316],\n",
      "         [ 0.5555, -0.2834, -0.0831],\n",
      "         [ 0.5794, -0.2846, -0.1237],\n",
      "         [ 0.5566, -0.2701, -0.0861],\n",
      "         [ 0.5447, -0.2610, -0.0760],\n",
      "         [ 0.5950, -0.2945, -0.1330],\n",
      "         [ 0.5869, -0.2515, -0.0848]]], grad_fn=<StackBackward0>), tensor([[[ 1.1614, -0.6537, -0.4113],\n",
      "         [ 1.1827, -0.6347, -0.2803],\n",
      "         [ 1.1749, -0.6672, -0.3709],\n",
      "         [ 1.2222, -0.6902, -0.5047],\n",
      "         [ 1.2393, -0.6561, -0.3315],\n",
      "         [ 1.1699, -0.6189, -0.3184],\n",
      "         [ 1.2095, -0.6228, -0.2123],\n",
      "         [ 1.2444, -0.6998, -0.5171],\n",
      "         [ 1.1944, -0.6684, -0.4436],\n",
      "         [ 1.1910, -0.6983, -0.4890],\n",
      "         [ 1.2048, -0.6768, -0.4995],\n",
      "         [ 1.2249, -0.5364, -0.1463],\n",
      "         [ 1.1966, -0.6925, -0.4951],\n",
      "         [ 1.1397, -0.5806, -0.2571],\n",
      "         [ 1.1857, -0.6860, -0.4491],\n",
      "         [ 1.1350, -0.4803, -0.1834],\n",
      "         [ 1.1247, -0.5063, -0.2296],\n",
      "         [ 1.1790, -0.6735, -0.4681],\n",
      "         [ 1.1770, -0.6933, -0.4998],\n",
      "         [ 1.1942, -0.6875, -0.4996],\n",
      "         [ 1.1678, -0.6935, -0.4341],\n",
      "         [ 1.2046, -0.6599, -0.3452],\n",
      "         [ 1.2377, -0.6946, -0.5135],\n",
      "         [ 1.2312, -0.6936, -0.4563],\n",
      "         [ 1.2361, -0.6987, -0.5165],\n",
      "         [ 1.2144, -0.6811, -0.5005],\n",
      "         [ 1.2586, -0.6364, -0.2277],\n",
      "         [ 1.1993, -0.6679, -0.3782],\n",
      "         [ 1.2074, -0.5900, -0.2371],\n",
      "         [ 1.2011, -0.5690, -0.2074],\n",
      "         [ 1.1822, -0.6996, -0.4380],\n",
      "         [ 1.1256, -0.5565, -0.2609]]], grad_fn=<StackBackward0>))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.6195, -0.2782, -0.1188],\n",
      "         [ 0.6073, -0.2927, -0.1410],\n",
      "         [ 0.5523, -0.2712, -0.0813],\n",
      "         [ 0.6313, -0.2871, -0.1327],\n",
      "         [ 0.5945, -0.3018, -0.1265],\n",
      "         [ 0.5961, -0.2967, -0.1342],\n",
      "         [ 0.6212, -0.2870, -0.1335],\n",
      "         [ 0.5763, -0.2875, -0.0976],\n",
      "         [ 0.5900, -0.2816, -0.1207],\n",
      "         [ 0.6259, -0.2898, -0.1227],\n",
      "         [ 0.5509, -0.2932, -0.1388],\n",
      "         [ 0.5893, -0.2528, -0.0809],\n",
      "         [ 0.5581, -0.2921, -0.0882],\n",
      "         [ 0.6245, -0.2855, -0.1331],\n",
      "         [ 0.6205, -0.2739, -0.1077],\n",
      "         [ 0.5088, -0.2565, -0.0965],\n",
      "         [ 0.5231, -0.2795, -0.1345],\n",
      "         [ 0.6124, -0.2683, -0.1265],\n",
      "         [ 0.4748, -0.1803, -0.0458],\n",
      "         [ 0.6377, -0.2897, -0.1336],\n",
      "         [ 0.5953, -0.2680, -0.0931],\n",
      "         [ 0.4716, -0.1996, -0.0628],\n",
      "         [ 0.5558, -0.2686, -0.0804],\n",
      "         [ 0.5982, -0.2706, -0.1305],\n",
      "         [ 0.5800, -0.2893, -0.1200],\n",
      "         [ 0.6222, -0.2740, -0.1069],\n",
      "         [ 0.6369, -0.2919, -0.1195],\n",
      "         [ 0.6105, -0.2672, -0.1283],\n",
      "         [ 0.6289, -0.2857, -0.1300],\n",
      "         [ 0.5966, -0.2707, -0.1267],\n",
      "         [ 0.6397, -0.2943, -0.1022],\n",
      "         [ 0.5962, -0.2732, -0.1271]]], grad_fn=<StackBackward0>), tensor([[[ 1.1546, -0.6670, -0.4447],\n",
      "         [ 1.2135, -0.7042, -0.4892],\n",
      "         [ 1.2254, -0.5985, -0.2248],\n",
      "         [ 1.2079, -0.6959, -0.5179],\n",
      "         [ 1.1870, -0.7199, -0.4143],\n",
      "         [ 1.1878, -0.7047, -0.4422],\n",
      "         [ 1.1544, -0.7004, -0.5094],\n",
      "         [ 1.2157, -0.6688, -0.2918],\n",
      "         [ 1.1641, -0.6730, -0.3792],\n",
      "         [ 1.1656, -0.7040, -0.4648],\n",
      "         [ 1.2270, -0.6862, -0.3938],\n",
      "         [ 1.1374, -0.5436, -0.2576],\n",
      "         [ 1.2515, -0.6622, -0.2454],\n",
      "         [ 1.1858, -0.6928, -0.4981],\n",
      "         [ 1.1483, -0.6448, -0.4005],\n",
      "         [ 1.1701, -0.5751, -0.2336],\n",
      "         [ 1.2376, -0.6518, -0.3317],\n",
      "         [ 1.1224, -0.6617, -0.4542],\n",
      "         [ 1.1089, -0.4043, -0.0942],\n",
      "         [ 1.2395, -0.7061, -0.5229],\n",
      "         [ 1.1464, -0.6061, -0.2902],\n",
      "         [ 1.1716, -0.4713, -0.1283],\n",
      "         [ 1.1887, -0.5803, -0.2237],\n",
      "         [ 1.1548, -0.6597, -0.4316],\n",
      "         [ 1.2125, -0.6798, -0.3640],\n",
      "         [ 1.1562, -0.6454, -0.3973],\n",
      "         [ 1.2287, -0.7052, -0.4619],\n",
      "         [ 1.1221, -0.6604, -0.4601],\n",
      "         [ 1.1980, -0.6900, -0.5058],\n",
      "         [ 1.1493, -0.6587, -0.4176],\n",
      "         [ 1.2391, -0.7033, -0.3759],\n",
      "         [ 1.1378, -0.6645, -0.4208]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6214, -0.2893, -0.1366],\n",
      "         [ 0.6344, -0.2957, -0.1185],\n",
      "         [ 0.6191, -0.2903, -0.1353],\n",
      "         [ 0.6206, -0.2881, -0.1319],\n",
      "         [ 0.6056, -0.2693, -0.1313],\n",
      "         [ 0.6310, -0.2777, -0.1312],\n",
      "         [ 0.6191, -0.2903, -0.1353],\n",
      "         [ 0.6266, -0.2814, -0.1320],\n",
      "         [ 0.6084, -0.2709, -0.1281],\n",
      "         [ 0.6071, -0.2947, -0.1004],\n",
      "         [ 0.5934, -0.2676, -0.0941],\n",
      "         [ 0.5969, -0.2773, -0.1313],\n",
      "         [ 0.6336, -0.2942, -0.1317],\n",
      "         [ 0.6294, -0.2919, -0.1083],\n",
      "         [ 0.6230, -0.2915, -0.1331],\n",
      "         [ 0.6247, -0.2941, -0.1248],\n",
      "         [ 0.6200, -0.2891, -0.1360],\n",
      "         [ 0.4678, -0.2077, -0.0962],\n",
      "         [ 0.5814, -0.2855, -0.1249],\n",
      "         [ 0.5973, -0.2757, -0.1326],\n",
      "         [ 0.4675, -0.2037, -0.0792],\n",
      "         [ 0.5924, -0.2678, -0.0920],\n",
      "         [ 0.5240, -0.2756, -0.0772],\n",
      "         [ 0.4779, -0.2126, -0.0619],\n",
      "         [ 0.6166, -0.2892, -0.1333],\n",
      "         [ 0.5807, -0.2896, -0.0890],\n",
      "         [ 0.6217, -0.2923, -0.1297],\n",
      "         [ 0.6153, -0.2679, -0.1287],\n",
      "         [ 0.5946, -0.2653, -0.0988],\n",
      "         [ 0.5533, -0.2787, -0.0840],\n",
      "         [ 0.6020, -0.2935, -0.1373],\n",
      "         [ 0.5851, -0.2972, -0.1436]]], grad_fn=<StackBackward0>), tensor([[[ 1.1696, -0.7017, -0.5136],\n",
      "         [ 1.2023, -0.7091, -0.4418],\n",
      "         [ 1.1463, -0.7086, -0.5162],\n",
      "         [ 1.1541, -0.7004, -0.5005],\n",
      "         [ 1.1009, -0.6647, -0.4713],\n",
      "         [ 1.1807, -0.6845, -0.5051],\n",
      "         [ 1.1463, -0.7086, -0.5162],\n",
      "         [ 1.1614, -0.6965, -0.5099],\n",
      "         [ 1.1062, -0.6675, -0.4596],\n",
      "         [ 1.1813, -0.6775, -0.3392],\n",
      "         [ 1.1280, -0.6030, -0.2954],\n",
      "         [ 1.1339, -0.6773, -0.4381],\n",
      "         [ 1.2211, -0.7163, -0.5135],\n",
      "         [ 1.2056, -0.6984, -0.4124],\n",
      "         [ 1.1712, -0.7059, -0.5198],\n",
      "         [ 1.1610, -0.7161, -0.4730],\n",
      "         [ 1.1675, -0.7016, -0.5100],\n",
      "         [ 1.1410, -0.4964, -0.1968],\n",
      "         [ 1.1159, -0.6830, -0.3968],\n",
      "         [ 1.1335, -0.6717, -0.4429],\n",
      "         [ 1.1506, -0.4835, -0.1610],\n",
      "         [ 1.1443, -0.6075, -0.2879],\n",
      "         [ 1.2237, -0.6164, -0.1864],\n",
      "         [ 1.1997, -0.5041, -0.1261],\n",
      "         [ 1.1363, -0.7037, -0.5075],\n",
      "         [ 1.2132, -0.6609, -0.2656],\n",
      "         [ 1.1625, -0.7155, -0.4910],\n",
      "         [ 1.1429, -0.6619, -0.4597],\n",
      "         [ 1.1294, -0.5985, -0.3107],\n",
      "         [ 1.2134, -0.6186, -0.2354],\n",
      "         [ 1.1975, -0.7006, -0.4711],\n",
      "         [ 1.1516, -0.7071, -0.4747]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5821, -0.2957, -0.1366],\n",
      "         [ 0.6096, -0.2728, -0.1316],\n",
      "         [ 0.6323, -0.3003, -0.1036],\n",
      "         [ 0.5885, -0.2759, -0.1358],\n",
      "         [ 0.5857, -0.3009, -0.1353],\n",
      "         [ 0.5565, -0.2975, -0.1457],\n",
      "         [ 0.5891, -0.2680, -0.0860],\n",
      "         [ 0.6277, -0.2787, -0.1126],\n",
      "         [ 0.6154, -0.2893, -0.1280],\n",
      "         [ 0.6012, -0.2982, -0.1111],\n",
      "         [ 0.4689, -0.1985, -0.0610],\n",
      "         [ 0.6023, -0.2720, -0.1318],\n",
      "         [ 0.5160, -0.2787, -0.0991],\n",
      "         [ 0.5685, -0.3063, -0.1050],\n",
      "         [ 0.6249, -0.2907, -0.1395],\n",
      "         [ 0.5187, -0.2783, -0.0786],\n",
      "         [ 0.6169, -0.2907, -0.1337],\n",
      "         [ 0.6350, -0.2858, -0.1173],\n",
      "         [ 0.6197, -0.2873, -0.1230],\n",
      "         [ 0.5771, -0.2665, -0.1027],\n",
      "         [ 0.6034, -0.2722, -0.1317],\n",
      "         [ 0.6292, -0.2855, -0.1305],\n",
      "         [ 0.6142, -0.2706, -0.1129],\n",
      "         [ 0.5753, -0.2941, -0.1078],\n",
      "         [ 0.6242, -0.2957, -0.1156],\n",
      "         [ 0.6111, -0.2878, -0.1266],\n",
      "         [ 0.5925, -0.2791, -0.1316],\n",
      "         [ 0.6020, -0.3012, -0.1244],\n",
      "         [ 0.5001, -0.2628, -0.1022],\n",
      "         [ 0.6305, -0.2827, -0.1341],\n",
      "         [ 0.4539, -0.1765, -0.0568],\n",
      "         [ 0.6038, -0.2991, -0.1389]]], grad_fn=<StackBackward0>), tensor([[[ 1.1428, -0.6968, -0.4474],\n",
      "         [ 1.1191, -0.6756, -0.4703],\n",
      "         [ 1.2117, -0.7165, -0.3791],\n",
      "         [ 1.0992, -0.6712, -0.4551],\n",
      "         [ 1.1410, -0.7103, -0.4457],\n",
      "         [ 1.2149, -0.6965, -0.4255],\n",
      "         [ 1.1267, -0.5793, -0.2750],\n",
      "         [ 1.1865, -0.6860, -0.3983],\n",
      "         [ 1.1245, -0.6977, -0.4834],\n",
      "         [ 1.2066, -0.7038, -0.3708],\n",
      "         [ 1.1648, -0.4643, -0.1229],\n",
      "         [ 1.0884, -0.6705, -0.4722],\n",
      "         [ 1.2242, -0.6382, -0.2376],\n",
      "         [ 1.2658, -0.7091, -0.3013],\n",
      "         [ 1.1923, -0.7061, -0.5224],\n",
      "         [ 1.2018, -0.6230, -0.1888],\n",
      "         [ 1.1396, -0.7067, -0.5072],\n",
      "         [ 1.2013, -0.7080, -0.4472],\n",
      "         [ 1.1711, -0.6873, -0.4531],\n",
      "         [ 1.1229, -0.5960, -0.3273],\n",
      "         [ 1.0930, -0.6716, -0.4715],\n",
      "         [ 1.1647, -0.7087, -0.5061],\n",
      "         [ 1.1487, -0.6639, -0.3957],\n",
      "         [ 1.2190, -0.6892, -0.3205],\n",
      "         [ 1.1724, -0.7070, -0.4438],\n",
      "         [ 1.1067, -0.6905, -0.4776],\n",
      "         [ 1.1112, -0.6800, -0.4406],\n",
      "         [ 1.1981, -0.7162, -0.4206],\n",
      "         [ 1.1361, -0.5892, -0.2448],\n",
      "         [ 1.1677, -0.7002, -0.5223],\n",
      "         [ 1.1064, -0.4068, -0.1139],\n",
      "         [ 1.1962, -0.7135, -0.4760]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6149, -0.2957, -0.1367],\n",
      "         [ 0.6127, -0.2982, -0.1316],\n",
      "         [ 0.5072, -0.2832, -0.1211],\n",
      "         [ 0.4995, -0.2741, -0.0873],\n",
      "         [ 0.5694, -0.2936, -0.0966],\n",
      "         [ 0.5889, -0.2803, -0.1343],\n",
      "         [ 0.5017, -0.2762, -0.0990],\n",
      "         [ 0.6236, -0.2974, -0.1290],\n",
      "         [ 0.6276, -0.2802, -0.1336],\n",
      "         [ 0.6282, -0.2847, -0.1339],\n",
      "         [ 0.6095, -0.2856, -0.1232],\n",
      "         [ 0.5036, -0.2746, -0.0753],\n",
      "         [ 0.6210, -0.2928, -0.1370],\n",
      "         [ 0.6302, -0.3031, -0.1172],\n",
      "         [ 0.5863, -0.2791, -0.1358],\n",
      "         [ 0.5816, -0.3013, -0.1442],\n",
      "         [ 0.6266, -0.2985, -0.1375],\n",
      "         [ 0.4708, -0.2206, -0.0799],\n",
      "         [ 0.6256, -0.2970, -0.1372],\n",
      "         [ 0.5869, -0.3116, -0.1284],\n",
      "         [ 0.5978, -0.2748, -0.1329],\n",
      "         [ 0.5474, -0.2968, -0.1264],\n",
      "         [ 0.6302, -0.2986, -0.1374],\n",
      "         [ 0.6249, -0.2804, -0.1354],\n",
      "         [ 0.5528, -0.3028, -0.1337],\n",
      "         [ 0.5877, -0.2764, -0.1373],\n",
      "         [ 0.6122, -0.2887, -0.1229],\n",
      "         [ 0.5956, -0.2995, -0.1083],\n",
      "         [ 0.6340, -0.2880, -0.1168],\n",
      "         [ 0.4716, -0.2166, -0.0989],\n",
      "         [ 0.5641, -0.3095, -0.1047],\n",
      "         [ 0.6089, -0.3022, -0.1073]]], grad_fn=<StackBackward0>), tensor([[[ 1.1328, -0.7203, -0.5187],\n",
      "         [ 1.1276, -0.7274, -0.4961],\n",
      "         [ 1.1967, -0.6550, -0.2890],\n",
      "         [ 1.1774, -0.6216, -0.2057],\n",
      "         [ 1.2147, -0.6802, -0.2819],\n",
      "         [ 1.0930, -0.6812, -0.4510],\n",
      "         [ 1.1832, -0.6313, -0.2337],\n",
      "         [ 1.1830, -0.7185, -0.4968],\n",
      "         [ 1.1578, -0.6897, -0.5189],\n",
      "         [ 1.1560, -0.7043, -0.5221],\n",
      "         [ 1.1169, -0.6814, -0.4590],\n",
      "         [ 1.1862, -0.6173, -0.1773],\n",
      "         [ 1.1656, -0.7047, -0.5329],\n",
      "         [ 1.1932, -0.7257, -0.4329],\n",
      "         [ 1.0886, -0.6789, -0.4554],\n",
      "         [ 1.1442, -0.7137, -0.4732],\n",
      "         [ 1.1898, -0.7243, -0.5367],\n",
      "         [ 1.1880, -0.5321, -0.1605],\n",
      "         [ 1.1848, -0.7188, -0.5357],\n",
      "         [ 1.1602, -0.7418, -0.4181],\n",
      "         [ 1.0725, -0.6766, -0.4748],\n",
      "         [ 1.1865, -0.6858, -0.3617],\n",
      "         [ 1.2067, -0.7260, -0.5361],\n",
      "         [ 1.1454, -0.6903, -0.5271],\n",
      "         [ 1.2198, -0.7106, -0.3840],\n",
      "         [ 1.0936, -0.6707, -0.4607],\n",
      "         [ 1.1176, -0.6895, -0.4586],\n",
      "         [ 1.1933, -0.7011, -0.3573],\n",
      "         [ 1.1948, -0.7133, -0.4455],\n",
      "         [ 1.1715, -0.5216, -0.1996],\n",
      "         [ 1.2531, -0.7169, -0.2986],\n",
      "         [ 1.2027, -0.6967, -0.3581]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5755, -0.3050, -0.1445],\n",
      "         [ 0.6076, -0.2865, -0.1133],\n",
      "         [ 0.5095, -0.2752, -0.0670],\n",
      "         [ 0.5596, -0.3106, -0.1068],\n",
      "         [ 0.5850, -0.3112, -0.1344],\n",
      "         [ 0.5861, -0.3003, -0.1177],\n",
      "         [ 0.6292, -0.2874, -0.1176],\n",
      "         [ 0.4679, -0.2210, -0.0678],\n",
      "         [ 0.5796, -0.3098, -0.1379],\n",
      "         [ 0.5944, -0.3080, -0.1237],\n",
      "         [ 0.6119, -0.2939, -0.1337],\n",
      "         [ 0.6118, -0.3009, -0.1390],\n",
      "         [ 0.5468, -0.3033, -0.1462],\n",
      "         [ 0.5680, -0.2970, -0.1124],\n",
      "         [ 0.5911, -0.3063, -0.1204],\n",
      "         [ 0.6263, -0.3022, -0.1384],\n",
      "         [ 0.5775, -0.2854, -0.1237],\n",
      "         [ 0.5827, -0.2658, -0.0848],\n",
      "         [ 0.5946, -0.3054, -0.1167],\n",
      "         [ 0.5779, -0.3075, -0.1368],\n",
      "         [ 0.5481, -0.2816, -0.0834],\n",
      "         [ 0.5653, -0.2910, -0.0955],\n",
      "         [ 0.6252, -0.2893, -0.1342],\n",
      "         [ 0.6190, -0.3031, -0.1183],\n",
      "         [ 0.6155, -0.2983, -0.1404],\n",
      "         [ 0.5592, -0.3114, -0.1148],\n",
      "         [ 0.5789, -0.3103, -0.1396],\n",
      "         [ 0.6118, -0.3000, -0.1398],\n",
      "         [ 0.5525, -0.2899, -0.1351],\n",
      "         [ 0.6218, -0.2825, -0.1363],\n",
      "         [ 0.6108, -0.2930, -0.1415],\n",
      "         [ 0.5951, -0.3079, -0.1428]]], grad_fn=<StackBackward0>), tensor([[[ 1.1227, -0.7208, -0.4725],\n",
      "         [ 1.1337, -0.6733, -0.4096],\n",
      "         [ 1.2006, -0.6090, -0.1565],\n",
      "         [ 1.2368, -0.7186, -0.3029],\n",
      "         [ 1.1543, -0.7382, -0.4376],\n",
      "         [ 1.1479, -0.6976, -0.3887],\n",
      "         [ 1.1708, -0.7065, -0.4496],\n",
      "         [ 1.1964, -0.5316, -0.1348],\n",
      "         [ 1.1474, -0.7374, -0.4476],\n",
      "         [ 1.1858, -0.7276, -0.4103],\n",
      "         [ 1.1460, -0.7048, -0.4931],\n",
      "         [ 1.1386, -0.7293, -0.5188],\n",
      "         [ 1.1850, -0.7095, -0.4216],\n",
      "         [ 1.2107, -0.6923, -0.3270],\n",
      "         [ 1.1711, -0.7198, -0.3985],\n",
      "         [ 1.1927, -0.7339, -0.5384],\n",
      "         [ 1.0861, -0.6754, -0.3976],\n",
      "         [ 1.1100, -0.5663, -0.2680],\n",
      "         [ 1.1902, -0.7165, -0.3847],\n",
      "         [ 1.1178, -0.7233, -0.4468],\n",
      "         [ 1.1914, -0.6211, -0.2332],\n",
      "         [ 1.1698, -0.6584, -0.2772],\n",
      "         [ 1.1421, -0.7179, -0.5247],\n",
      "         [ 1.1851, -0.7337, -0.4301],\n",
      "         [ 1.1533, -0.7216, -0.5240],\n",
      "         [ 1.2211, -0.7210, -0.3273],\n",
      "         [ 1.1432, -0.7395, -0.4540],\n",
      "         [ 1.1381, -0.7266, -0.5219],\n",
      "         [ 1.1494, -0.6787, -0.3957],\n",
      "         [ 1.1307, -0.6950, -0.5321],\n",
      "         [ 1.1376, -0.7040, -0.5274],\n",
      "         [ 1.1963, -0.7365, -0.4803]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.4619, -0.2173, -0.0915],\n",
      "         [ 0.4988, -0.2747, -0.0557],\n",
      "         [ 0.6170, -0.3041, -0.1394],\n",
      "         [ 0.6205, -0.3051, -0.1392],\n",
      "         [ 0.6086, -0.3058, -0.1337],\n",
      "         [ 0.6254, -0.2893, -0.1374],\n",
      "         [ 0.6101, -0.2989, -0.1386],\n",
      "         [ 0.4492, -0.1885, -0.0406],\n",
      "         [ 0.6251, -0.3125, -0.1219],\n",
      "         [ 0.6147, -0.2860, -0.1374],\n",
      "         [ 0.5928, -0.2872, -0.0910],\n",
      "         [ 0.5771, -0.2670, -0.1017],\n",
      "         [ 0.5855, -0.2877, -0.1413],\n",
      "         [ 0.5784, -0.2858, -0.1352],\n",
      "         [ 0.6049, -0.2938, -0.1284],\n",
      "         [ 0.5946, -0.2807, -0.1300],\n",
      "         [ 0.5460, -0.2957, -0.1291],\n",
      "         [ 0.5736, -0.2556, -0.0721],\n",
      "         [ 0.5425, -0.2866, -0.0890],\n",
      "         [ 0.5887, -0.2802, -0.1239],\n",
      "         [ 0.5993, -0.3021, -0.1365],\n",
      "         [ 0.6302, -0.2918, -0.1338],\n",
      "         [ 0.6029, -0.2849, -0.1323],\n",
      "         [ 0.6027, -0.3035, -0.1395],\n",
      "         [ 0.5739, -0.2904, -0.1274],\n",
      "         [ 0.6129, -0.2989, -0.1394],\n",
      "         [ 0.6035, -0.2885, -0.1131],\n",
      "         [ 0.5745, -0.2853, -0.1370],\n",
      "         [ 0.5001, -0.2830, -0.0856],\n",
      "         [ 0.6190, -0.3053, -0.1394],\n",
      "         [ 0.4468, -0.1471, -0.0119],\n",
      "         [ 0.4517, -0.2245, -0.0949]]], grad_fn=<StackBackward0>), tensor([[[ 1.1553, -0.5226, -0.1812],\n",
      "         [ 1.1826, -0.6045, -0.1278],\n",
      "         [ 1.1548, -0.7341, -0.5397],\n",
      "         [ 1.1710, -0.7386, -0.5386],\n",
      "         [ 1.1329, -0.7400, -0.4921],\n",
      "         [ 1.1485, -0.7174, -0.5347],\n",
      "         [ 1.1274, -0.7154, -0.5347],\n",
      "         [ 1.1210, -0.4353, -0.0795],\n",
      "         [ 1.1852, -0.7559, -0.4645],\n",
      "         [ 1.1038, -0.7031, -0.5355],\n",
      "         [ 1.1243, -0.6554, -0.2897],\n",
      "         [ 1.0978, -0.6125, -0.3200],\n",
      "         [ 1.0862, -0.7044, -0.4765],\n",
      "         [ 1.0517, -0.6926, -0.4541],\n",
      "         [ 1.1031, -0.7018, -0.4762],\n",
      "         [ 1.0618, -0.6887, -0.4594],\n",
      "         [ 1.1444, -0.6933, -0.3713],\n",
      "         [ 1.1024, -0.5330, -0.2227],\n",
      "         [ 1.1711, -0.6348, -0.2473],\n",
      "         [ 1.1034, -0.6751, -0.4093],\n",
      "         [ 1.0780, -0.7309, -0.5130],\n",
      "         [ 1.1684, -0.7247, -0.5194],\n",
      "         [ 1.0927, -0.7056, -0.4687],\n",
      "         [ 1.0907, -0.7377, -0.5259],\n",
      "         [ 1.0696, -0.6899, -0.4113],\n",
      "         [ 1.1391, -0.7165, -0.5378],\n",
      "         [ 1.1221, -0.6757, -0.4058],\n",
      "         [ 1.0424, -0.6924, -0.4601],\n",
      "         [ 1.2108, -0.6429, -0.1968],\n",
      "         [ 1.1637, -0.7388, -0.5395],\n",
      "         [ 1.0723, -0.3193, -0.0233],\n",
      "         [ 1.1278, -0.5477, -0.1878]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.6052, -0.2998, -0.1318],\n",
      "         [ 0.6124, -0.2885, -0.1364],\n",
      "         [ 0.5814, -0.2827, -0.1359],\n",
      "         [ 0.5768, -0.2922, -0.1371],\n",
      "         [ 0.5784, -0.2830, -0.1347],\n",
      "         [ 0.5991, -0.2999, -0.1348],\n",
      "         [ 0.5792, -0.2840, -0.1335],\n",
      "         [ 0.5848, -0.3203, -0.1298],\n",
      "         [ 0.4454, -0.1531, -0.0123],\n",
      "         [ 0.5804, -0.3151, -0.1256],\n",
      "         [ 0.5612, -0.3147, -0.1368],\n",
      "         [ 0.6226, -0.2944, -0.1364],\n",
      "         [ 0.4755, -0.2753, -0.1103],\n",
      "         [ 0.5501, -0.2953, -0.0930],\n",
      "         [ 0.6087, -0.2882, -0.1281],\n",
      "         [ 0.6102, -0.3027, -0.1204],\n",
      "         [ 0.5743, -0.2547, -0.0882],\n",
      "         [ 0.5721, -0.2887, -0.1346],\n",
      "         [ 0.5308, -0.2963, -0.1395],\n",
      "         [ 0.5509, -0.3119, -0.1075],\n",
      "         [ 0.6093, -0.3096, -0.1401],\n",
      "         [ 0.6165, -0.2919, -0.1385],\n",
      "         [ 0.6078, -0.3025, -0.1406],\n",
      "         [ 0.5998, -0.3062, -0.1408],\n",
      "         [ 0.5749, -0.3113, -0.1507],\n",
      "         [ 0.5670, -0.2886, -0.1121],\n",
      "         [ 0.6102, -0.2887, -0.1206],\n",
      "         [ 0.4474, -0.2185, -0.1043],\n",
      "         [ 0.6004, -0.3099, -0.1270],\n",
      "         [ 0.5632, -0.3159, -0.1412],\n",
      "         [ 0.5764, -0.2727, -0.0735],\n",
      "         [ 0.6092, -0.2847, -0.1204]]], grad_fn=<StackBackward0>), tensor([[[ 1.1246, -0.7163, -0.4778],\n",
      "         [ 1.0988, -0.7088, -0.5254],\n",
      "         [ 1.0211, -0.6940, -0.4774],\n",
      "         [ 1.0513, -0.7129, -0.4562],\n",
      "         [ 1.0108, -0.6939, -0.4728],\n",
      "         [ 1.0823, -0.7217, -0.4996],\n",
      "         [ 1.0093, -0.6956, -0.4693],\n",
      "         [ 1.1799, -0.7583, -0.4181],\n",
      "         [ 1.0713, -0.3337, -0.0240],\n",
      "         [ 1.1583, -0.7367, -0.4033],\n",
      "         [ 1.0762, -0.7359, -0.4328],\n",
      "         [ 1.1383, -0.7308, -0.5264],\n",
      "         [ 1.1087, -0.6242, -0.2506],\n",
      "         [ 1.1292, -0.6629, -0.2627],\n",
      "         [ 1.0884, -0.7044, -0.4882],\n",
      "         [ 1.1558, -0.7257, -0.4308],\n",
      "         [ 1.0368, -0.5525, -0.2790],\n",
      "         [ 1.0347, -0.6995, -0.4466],\n",
      "         [ 1.0803, -0.6905, -0.3995],\n",
      "         [ 1.2163, -0.7188, -0.2990],\n",
      "         [ 1.1280, -0.7493, -0.5371],\n",
      "         [ 1.1152, -0.7228, -0.5348],\n",
      "         [ 1.1236, -0.7259, -0.5369],\n",
      "         [ 1.0991, -0.7397, -0.5175],\n",
      "         [ 1.1406, -0.7343, -0.4926],\n",
      "         [ 1.1128, -0.6481, -0.3456],\n",
      "         [ 1.0919, -0.7021, -0.4572],\n",
      "         [ 1.0977, -0.5291, -0.2063],\n",
      "         [ 1.0766, -0.7496, -0.4704],\n",
      "         [ 1.1061, -0.7480, -0.4447],\n",
      "         [ 1.1254, -0.5760, -0.2239],\n",
      "         [ 1.0879, -0.6883, -0.4560]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5456, -0.3011, -0.0937],\n",
      "         [ 0.6025, -0.2931, -0.1392],\n",
      "         [ 0.6117, -0.2917, -0.1347],\n",
      "         [ 0.5901, -0.3072, -0.1436],\n",
      "         [ 0.5865, -0.3082, -0.1417],\n",
      "         [ 0.4419, -0.1869, -0.0349],\n",
      "         [ 0.5509, -0.3044, -0.1306],\n",
      "         [ 0.5925, -0.3017, -0.1346],\n",
      "         [ 0.5719, -0.2843, -0.1369],\n",
      "         [ 0.5745, -0.2844, -0.0835],\n",
      "         [ 0.5971, -0.2908, -0.0933],\n",
      "         [ 0.5973, -0.2882, -0.0938],\n",
      "         [ 0.6044, -0.2958, -0.1386],\n",
      "         [ 0.5694, -0.2884, -0.1073],\n",
      "         [ 0.6013, -0.2956, -0.1376],\n",
      "         [ 0.6098, -0.2939, -0.1374],\n",
      "         [ 0.6009, -0.2986, -0.1365],\n",
      "         [ 0.5641, -0.2842, -0.1370],\n",
      "         [ 0.5880, -0.3008, -0.1452],\n",
      "         [ 0.5714, -0.2943, -0.1450],\n",
      "         [ 0.6031, -0.3062, -0.1349],\n",
      "         [ 0.5954, -0.3070, -0.1405],\n",
      "         [ 0.5225, -0.3082, -0.1452],\n",
      "         [ 0.5791, -0.2777, -0.1313],\n",
      "         [ 0.5706, -0.2858, -0.1326],\n",
      "         [ 0.5934, -0.3065, -0.1408],\n",
      "         [ 0.5954, -0.3034, -0.1379],\n",
      "         [ 0.5650, -0.2943, -0.1418],\n",
      "         [ 0.4437, -0.1896, -0.0371],\n",
      "         [ 0.5952, -0.2680, -0.0995],\n",
      "         [ 0.4559, -0.2141, -0.0548],\n",
      "         [ 0.5376, -0.3133, -0.1154]]], grad_fn=<StackBackward0>), tensor([[[ 1.1720, -0.6919, -0.2582],\n",
      "         [ 1.0647, -0.7222, -0.5316],\n",
      "         [ 1.1018, -0.7185, -0.5101],\n",
      "         [ 1.0656, -0.7413, -0.5228],\n",
      "         [ 1.0387, -0.7481, -0.5236],\n",
      "         [ 1.0939, -0.4291, -0.0680],\n",
      "         [ 1.0709, -0.7026, -0.3999],\n",
      "         [ 1.0549, -0.7233, -0.4927],\n",
      "         [ 0.9934, -0.6976, -0.4746],\n",
      "         [ 1.0581, -0.6412, -0.2629],\n",
      "         [ 1.0918, -0.6634, -0.3253],\n",
      "         [ 1.0918, -0.6554, -0.3270],\n",
      "         [ 1.0690, -0.7302, -0.5309],\n",
      "         [ 1.0660, -0.6778, -0.3377],\n",
      "         [ 1.0561, -0.7287, -0.5264],\n",
      "         [ 1.0902, -0.7248, -0.5243],\n",
      "         [ 1.0553, -0.7387, -0.5220],\n",
      "         [ 0.9675, -0.6953, -0.4759],\n",
      "         [ 1.0619, -0.7206, -0.5271],\n",
      "         [ 1.0419, -0.7219, -0.4798],\n",
      "         [ 1.1134, -0.7352, -0.5042],\n",
      "         [ 1.0882, -0.7413, -0.5088],\n",
      "         [ 1.1206, -0.7189, -0.4011],\n",
      "         [ 1.0201, -0.6756, -0.4509],\n",
      "         [ 1.0345, -0.6875, -0.4333],\n",
      "         [ 1.0795, -0.7389, -0.5103],\n",
      "         [ 1.0734, -0.7325, -0.5055],\n",
      "         [ 1.0199, -0.7193, -0.4683],\n",
      "         [ 1.1038, -0.4371, -0.0723],\n",
      "         [ 1.0746, -0.6306, -0.3345],\n",
      "         [ 1.1635, -0.5088, -0.1071],\n",
      "         [ 1.1636, -0.7216, -0.3159]]], grad_fn=<StackBackward0>))\n",
      "(tensor([[[ 0.5919, -0.3061, -0.1418],\n",
      "         [ 0.4458, -0.1699, -0.0318],\n",
      "         [ 0.5569, -0.2688, -0.1066],\n",
      "         [ 0.5504, -0.2841, -0.1376],\n",
      "         [ 0.5914, -0.2932, -0.1410],\n",
      "         [ 0.4418, -0.1854, -0.0287],\n",
      "         [ 0.5317, -0.2990, -0.0726],\n",
      "         [ 0.5867, -0.3091, -0.1430],\n",
      "         [ 0.5997, -0.3109, -0.1229],\n",
      "         [ 0.5885, -0.2927, -0.1162],\n",
      "         [ 0.5094, -0.2961, -0.1379],\n",
      "         [ 0.5848, -0.3020, -0.1386],\n",
      "         [ 0.5811, -0.2937, -0.1121],\n",
      "         [ 0.5843, -0.3067, -0.1426],\n",
      "         [ 0.5884, -0.2942, -0.1410],\n",
      "         [ 0.5519, -0.2867, -0.0920],\n",
      "         [ 0.5632, -0.2904, -0.1376],\n",
      "         [ 0.5732, -0.3094, -0.1427],\n",
      "         [ 0.5974, -0.2960, -0.1408],\n",
      "         [ 0.5144, -0.2803, -0.0743],\n",
      "         [ 0.5451, -0.2910, -0.1375],\n",
      "         [ 0.4668, -0.2692, -0.0647],\n",
      "         [ 0.4370, -0.1242, -0.0026],\n",
      "         [ 0.5775, -0.2983, -0.1351],\n",
      "         [ 0.5922, -0.2958, -0.1386],\n",
      "         [ 0.5981, -0.2954, -0.1399],\n",
      "         [ 0.5238, -0.2909, -0.1091],\n",
      "         [ 0.5761, -0.3026, -0.1466],\n",
      "         [ 0.5132, -0.2730, -0.0749],\n",
      "         [ 0.5494, -0.3041, -0.1201],\n",
      "         [ 0.5743, -0.3090, -0.1433],\n",
      "         [ 0.5512, -0.3145, -0.1396]]], grad_fn=<StackBackward0>), tensor([[[ 1.0666, -0.7330, -0.5292],\n",
      "         [ 1.0579, -0.3773, -0.0621],\n",
      "         [ 1.0179, -0.6160, -0.3325],\n",
      "         [ 0.9299, -0.6937, -0.4699],\n",
      "         [ 1.0277, -0.7211, -0.5314],\n",
      "         [ 1.0420, -0.4156, -0.0562],\n",
      "         [ 1.2226, -0.6701, -0.1878],\n",
      "         [ 1.0467, -0.7422, -0.5356],\n",
      "         [ 1.1085, -0.7468, -0.4479],\n",
      "         [ 1.0463, -0.6845, -0.4119],\n",
      "         [ 1.0227, -0.6844, -0.3812],\n",
      "         [ 1.0437, -0.7174, -0.5138],\n",
      "         [ 1.0530, -0.6875, -0.3866],\n",
      "         [ 1.0406, -0.7346, -0.5321],\n",
      "         [ 1.0176, -0.7238, -0.5321],\n",
      "         [ 1.0371, -0.6170, -0.2734],\n",
      "         [ 1.0118, -0.7036, -0.4464],\n",
      "         [ 0.9970, -0.7501, -0.5203],\n",
      "         [ 1.0469, -0.7309, -0.5321],\n",
      "         [ 1.1263, -0.6126, -0.1922],\n",
      "         [ 0.9493, -0.7005, -0.4479],\n",
      "         [ 1.1112, -0.5954, -0.1420],\n",
      "         [ 1.0459, -0.2647, -0.0050],\n",
      "         [ 1.0142, -0.7113, -0.4864],\n",
      "         [ 1.0267, -0.7279, -0.5230],\n",
      "         [ 1.0501, -0.7286, -0.5279],\n",
      "         [ 1.1057, -0.6680, -0.2951],\n",
      "         [ 1.0241, -0.7260, -0.5245],\n",
      "         [ 1.0727, -0.5810, -0.1947],\n",
      "         [ 1.0631, -0.6934, -0.3599],\n",
      "         [ 1.0145, -0.7452, -0.5134],\n",
      "         [ 1.0602, -0.7325, -0.4257]]], grad_fn=<StackBackward0>))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m dump(\u001b[38;5;28mvars\u001b[39m(parameters), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jldump\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#agent.gathering_data=False\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# --- Show results ---\u001b[39;00m\n\u001b[1;32m     10\u001b[0m basename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fname\n",
      "File \u001b[0;32m~/Code/deer/deer/agent.py:283\u001b[0m, in \u001b[0;36mNeuralAgent.run\u001b[0;34m(self, n_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03mThis function encapsulates the inference and the learning.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03mIf the agent is in train mode (mode = -1):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    maximum number of steps for a given epoch\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_non_train(n_epochs, epoch_length)\n",
      "File \u001b[0;32m~/Code/deer/deer/agent.py:308\u001b[0m, in \u001b[0;36mNeuralAgent._run_train\u001b[0;34m(self, n_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_loss_averages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m nbr_steps_left \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# run new episodes until the number of steps left for the epoch has reached 0\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m     nbr_steps_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runEpisode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbr_steps_left\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_controllers: c\u001b[38;5;241m.\u001b[39monEpochEnd(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Code/deer/deer/agent.py:396\u001b[0m, in \u001b[0;36mNeuralAgent._runEpisode\u001b[0;34m(self, maxSteps)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_addSample(obs, action, reward, \u001b[38;5;28;01mTrue\u001b[39;00m, hidden) \u001b[38;5;66;03m# If the episode ends because max number of steps is reached, mark the transition as terminal\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_controllers: \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monActionTaken\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_terminal:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/deer/deer/experiment/base_controllers.py:407\u001b[0m, in \u001b[0;36mTrainerController.onActionTaken\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_action:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/deer/deer/experiment/base_controllers.py:411\u001b[0m, in \u001b[0;36mTrainerController._update\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent):\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_periodicity \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_periodicity \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 411\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Code/deer/deer/agent.py:198\u001b[0m, in \u001b[0;36mNeuralAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nstep \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    194\u001b[0m     observations, actions, rewards, terminals, rndValidIndices, hidden_states \u001b[38;5;241m=\u001b[39m\\\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset\u001b[38;5;241m.\u001b[39mrandomBatch_nstep(\n\u001b[1;32m    196\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exp_priority\n\u001b[1;32m    197\u001b[0m             )\n\u001b[0;32m--> 198\u001b[0m     loss, loss_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learning_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecurrent_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#TODO\u001b[39;49;00m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     states, actions, rewards, next_states, terminals, rndValidIndices \u001b[38;5;241m=\u001b[39m\\\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset\u001b[38;5;241m.\u001b[39mrandomBatch(\n\u001b[1;32m    204\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exp_priority\n\u001b[1;32m    205\u001b[0m             )\n",
      "File \u001b[0;32m~/Code/deer/deer/learning_algos/CRAR_torch.py:335\u001b[0m, in \u001b[0;36mCRAR.recurrent_train\u001b[0;34m(self, observations, actions_val, rewards_val, terminals_val, hidden_states)\u001b[0m\n\u001b[1;32m    332\u001b[0m states_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(states_val, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    333\u001b[0m next_states_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(next_states_val, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m--> 335\u001b[0m Esp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_states_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m Es \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrar\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    339\u001b[0m     states_val, [h[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hidden_states], update_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    340\u001b[0m     )\n\u001b[1;32m    341\u001b[0m Es_and_actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([Es, onehot_actions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/deer/deer/learning_algos/NN_CRAR_torch.py:147\u001b[0m, in \u001b[0;36mNN.encoder_model.<locals>.EncoderRNN.forward\u001b[0;34m(self, x, hidden, update_hidden)\u001b[0m\n\u001b[1;32m    145\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(timesteps):\n\u001b[0;32m--> 147\u001b[0m     out, new_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m new_hidden\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_hidden:\n",
      "File \u001b[0;32m~/Code/deer/deer/learning_algos/NN_CRAR_torch.py:168\u001b[0m, in \u001b[0;36mNN.encoder_model.<locals>.EncoderRNN._forward_step\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mtype\u001b[39m(hidden) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (hidden[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m): \u001b[38;5;66;03m# hacky TODO\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m x, new_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, new_hidden\n",
      "File \u001b[0;32m~/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/nn/modules/rnn.py:752\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# If not PackedSequence input.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n\u001b[0;32m--> 752\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mhx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m    753\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor batched 3-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    754\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 3-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    755\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"params\")\n",
    "except Exception:\n",
    "    pass\n",
    "dump(vars(parameters), \"params/\" + fname + \".jldump\")\n",
    "#agent.gathering_data=False\n",
    "agent.run(parameters.epochs, parameters.steps_per_epoch)\n",
    "\n",
    "# --- Show results ---\n",
    "basename = \"scores/\" + fname\n",
    "scores = load(basename + \"_scores.jldump\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.gathering_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is the hidden state not in batch sizes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.setNetwork(fname, nEpoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._in_episode = True\n",
    "agent._mode = 0 # Testing mode with plan_depth=0\n",
    "initState = env.reset(agent._mode)\n",
    "inputDims = env.inputDimensions()\n",
    "\n",
    "for i in range(len(inputDims)):\n",
    "    if inputDims[i][0] > 1:\n",
    "        agent._state[i][1:] = initState[i][1:]\n",
    "agent._Vs_on_last_episode = []\n",
    "is_terminal = False\n",
    "reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for i in range(100):\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    plt.figure()\n",
    "    plt.imshow(np.flip(_obs.squeeze()))\n",
    "    plt.show()\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "    V, action, reward, _ = agent._step()\n",
    "    print(action)\n",
    "    agent._Vs_on_last_episode.append(V)\n",
    "    is_terminal = env.inTerminalState()\n",
    "    if is_terminal: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "obs = env.observe()\n",
    "_obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "_obs = np.flip(_obs.squeeze())\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "im = ax.imshow(np.zeros(_obs.shape))\n",
    "\n",
    "def init():\n",
    "    plt.cla()\n",
    "    im = ax.imshow(_obs)\n",
    "    return [im]\n",
    "\n",
    "def animate(i, *args, **kwargs):\n",
    "    plt.cla()\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    _obs = np.flip(_obs.squeeze())\n",
    "    im = ax.imshow(_obs)\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "        V, action, reward, _ = agent._step()\n",
    "        agent._Vs_on_last_episode.append(V)\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init, \n",
    "     frames=100, blit=False, repeat=True)\n",
    "ani.save('behavior.gif', writer=\"ffmpeg\", fps = 15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
