{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import hash, dump, load\n",
    "import os\n",
    "\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.CRAR_torch import CRAR\n",
    "from figure8_env import MyEnv as figure8_env\n",
    "import deer.experiment.base_controllers as bc\n",
    "\n",
    "from deer.policies import EpsilonGreedyPolicy, FixedFigure8Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure8_give_rewards = True\n",
    "nn_yaml = 'network_noconv.yaml'\n",
    "higher_dim_obs = False\n",
    "internal_dim = 10\n",
    "fname = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    steps_per_epoch = 5000\n",
    "    epochs = 50\n",
    "    steps_per_test = 1000\n",
    "    period_btw_summary_perfs = 1\n",
    "\n",
    "    # ----------------------\n",
    "    # Temporal Processing Parameters\n",
    "    # ----------------------\n",
    "    nstep = 20\n",
    "    nstep_decay = 0.8\n",
    "    encoder_type = 'variational'\n",
    "    \n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    frame_skip = 2\n",
    "    show_rewards = False\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    learning_rate = 1 * 1E-4\n",
    "    learning_rate_decay = 1.0\n",
    "    discount = 0.9\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 1.0\n",
    "    epsilon_decay = 1000\n",
    "    update_frequency = 1\n",
    "    replay_memory_size = 100000 #50000\n",
    "    batch_size = 64\n",
    "    freeze_interval = 1000\n",
    "    deterministic = False\n",
    "    \n",
    "    # ----------------------\n",
    "    # Learning algo parameters\n",
    "    # ----------------------\n",
    "    #loss_weights = [5E-3, 1E-3, 5E-3, 5E-3, 5E-3, 5E-3, 1.]\n",
    "    #loss_weights = [0, 0, 0, 0, 0, 0, 1.]\n",
    "    loss_weights = [5E-3, 5E-3, 5E-3, 0, 5E-3, 1E-3, 1., 1E-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end gathering data\n"
     ]
    }
   ],
   "source": [
    "parameters = Defaults()\n",
    "if parameters.deterministic:\n",
    "    rng = np.random.RandomState(123456)\n",
    "else:\n",
    "    rng = np.random.RandomState()\n",
    "\n",
    "# --- Instantiate environment ---\n",
    "env = figure8_env(\n",
    "    give_rewards=figure8_give_rewards,\n",
    "    intern_dim=internal_dim,\n",
    "    higher_dim_obs=higher_dim_obs,\n",
    "    show_rewards=parameters.show_rewards,\n",
    "    nstep=parameters.nstep, nstep_decay=parameters.nstep_decay\n",
    "    )\n",
    "\n",
    "# --- Instantiate learning_algo ---\n",
    "learning_algo = CRAR(\n",
    "    env,\n",
    "    parameters.freeze_interval,\n",
    "    parameters.batch_size,\n",
    "    rng,\n",
    "    high_int_dim=False,\n",
    "    internal_dim=internal_dim, lr=parameters.learning_rate,\n",
    "    nn_yaml=nn_yaml, double_Q=True,\n",
    "    loss_weights=parameters.loss_weights,\n",
    "    nstep=parameters.nstep, nstep_decay=parameters.nstep_decay,\n",
    "    encoder_type=parameters.encoder_type\n",
    "    )\n",
    "\n",
    "if figure8_give_rewards:\n",
    "    train_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.2,\n",
    "        consider_valid_transitions=False\n",
    "        )\n",
    "    test_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.\n",
    "        )\n",
    "else:\n",
    "    train_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng, epsilon=0.2,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "    test_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "\n",
    "# --- Instantiate agent ---\n",
    "agent = NeuralAgent(\n",
    "    env, learning_algo,\n",
    "    parameters.replay_memory_size,\n",
    "    1, parameters.batch_size, rng,\n",
    "    train_policy=train_policy, test_policy=test_policy)\n",
    "\n",
    "agent.run(10, 500)\n",
    "print(\"end gathering data\")\n",
    "\n",
    "# --- Bind controllers to the agent ---\n",
    "# Before every training epoch (periodicity=1), we want to print a summary of the agent's epsilon, discount and \n",
    "# learning rate as well as the training epoch number.\n",
    "agent.attach(bc.VerboseController(\n",
    "    evaluate_on='epoch', \n",
    "    periodicity=1))\n",
    "\n",
    "# Learning rate may follow a scheduler\n",
    "agent.attach(bc.LearningRateController(\n",
    "    initial_learning_rate=parameters.learning_rate, \n",
    "    learning_rate_decay=parameters.learning_rate_decay,\n",
    "    periodicity=1))\n",
    "\n",
    "# During training epochs, we want to train the agent after every [parameters.update_frequency] action it takes.\n",
    "# Plus, we also want to display after each training episode (!= than after every training) the average bellman\n",
    "# residual and the average of the V values obtained during the last episode, hence the two last arguments.\n",
    "agent.attach(bc.TrainerController(\n",
    "    evaluate_on='action', \n",
    "    periodicity=parameters.update_frequency, \n",
    "    show_episode_avg_V_value=True, \n",
    "    show_avg_Bellman_residual=True))\n",
    "\n",
    "# We wish to discover, among all versions of our neural network (i.e., after every training epoch), which one \n",
    "# has the highest validation score.\n",
    "# To achieve this goal, one can use the FindBestController along with an InterleavedTestEpochControllers. It is \n",
    "# important that the validationID is the same than the id argument of the InterleavedTestEpochController.\n",
    "# The FindBestController will dump on disk the validation scores for each and every network, as well as the \n",
    "# structure of the neural network having the best validation score. These dumps can then used to plot the evolution \n",
    "# of the validation and test scores (see below) or simply recover the resulting neural network for your \n",
    "# application.\n",
    "agent.attach(bc.FindBestController(\n",
    "    validationID=figure8_env.VALIDATION_MODE,\n",
    "    testID=None,\n",
    "    unique_fname=fname))\n",
    "\n",
    "# All previous controllers control the agent during the epochs it goes through. However, we want to interleave a \n",
    "# \"validation epoch\" between each training epoch. For each validation epoch, we want also to display the sum of all \n",
    "# rewards obtained, hence the showScore=True. Finally, we want to call the summarizePerformance method of ALE_env \n",
    "# every [parameters.period_btw_summary_perfs] *validation* epochs.\n",
    "agent.attach(bc.InterleavedTestEpochController(\n",
    "    id=figure8_env.VALIDATION_MODE, \n",
    "    epoch_length=parameters.steps_per_test,\n",
    "    periodicity=1,\n",
    "    show_score=True,\n",
    "    summarize_every=1,\n",
    "    unique_fname=fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.9788, -1.2043, -1.1370, -1.3863,  0.8605,  1.9189, -1.1895, -1.0534,\n",
      "        -0.1351,  0.6652]) tensor([-0.7933, -1.1102, -1.4306, -1.6623,  0.9538,  2.0401, -0.9203, -1.2591,\n",
      "         0.0874,  0.8875]) tensor([-0.9202, -0.5225,  0.6498,  0.9199, -1.9167,  0.5375,  0.9473,  0.4975,\n",
      "         0.7661, -0.2791])\n",
      "R[0]\n",
      "tensor([0.2181], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 1.2190736744403838; R = 0.007600422054587398;                 Gamma = 0.3480693909712136; Q = 0.025976787250488995;\n",
      "Entropy Neighbor = 1.1845786672424196e-05;                 Entropy Random = 1.0816577540484928e-05;                 Volume = 2.816436332702637; VAE = 47.36184683227539\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.8675,  1.4538, -0.6874, -0.3086, -1.1682,  0.6896,  1.1189, -0.0783,\n",
      "        -0.0470,  0.3011]) tensor([ 0.5029,  1.2150, -1.0129, -0.4578, -0.8791,  0.7719,  1.0039, -0.3767,\n",
      "        -0.0357,  0.7649]) tensor([ 2.4098,  0.5112, -0.1195, -1.1551,  0.1866,  1.7577,  0.9018, -1.3683,\n",
      "        -0.1280,  1.1484])\n",
      "R[0]\n",
      "tensor([0.0037], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.6363375975489617; R = 0.001249427962982736;                 Gamma = 0.03834969833772629; Q = 0.023184504618868233;\n",
      "Entropy Neighbor = 7.411889031436658e-05;                 Entropy Random = 5.759996774668252e-05;                 Volume = 2.4206156380176544; VAE = 47.24131485748291\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2445, -0.1347, -0.1451, -0.5464,  0.9851,  0.2352,  1.4841, -1.4791,\n",
      "        -0.3547,  1.4179]) tensor([ 0.6496, -0.0637, -0.1223, -0.6287,  0.6622,  0.0108,  1.5599, -1.3880,\n",
      "        -0.3984,  1.0493]) tensor([ 1.3680,  1.1138, -0.6826, -1.3308, -0.2938,  0.0701,  0.6810, -0.6745,\n",
      "         0.5273,  0.4081])\n",
      "R[0]\n",
      "tensor([-0.0031], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.42432801324129105; R = 0.001473685769826261;                 Gamma = 0.03015144306467846; Q = 0.002075991255871486;\n",
      "Entropy Neighbor = 0.00020357947718002833;                 Entropy Random = 0.00017939483674854272;                 Volume = 2.034242506980896; VAE = 43.90315911102295\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3044, -0.2473, -0.1589, -1.0888,  0.4030, -0.3148,  1.1629, -1.4884,\n",
      "         0.3591,  0.7320]) tensor([ 0.4308, -0.0309, -0.0206, -1.0487,  0.4861,  0.2813,  1.3068, -1.4042,\n",
      "         0.3572,  0.5859]) tensor([ 1.1053,  0.5043, -0.7912, -0.1441,  0.1593,  0.0537,  1.1699, -2.0061,\n",
      "         0.7868,  0.9847])\n",
      "R[0]\n",
      "tensor([0.0027], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.30451975256204605; R = 0.0011268576096008474;                 Gamma = 0.026707657114136963; Q = 0.0016622839902411216;\n",
      "Entropy Neighbor = 0.0005329030159336981;                 Entropy Random = 0.0004357066136144567;                 Volume = 2.0896171877384186; VAE = 46.357435958862304\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.3981, -0.2158, -0.7407, -0.5529,  0.8546,  1.2544,  1.5942, -1.6593,\n",
      "        -0.4331,  0.7532]) tensor([ 1.3147, -0.2373, -0.7652, -0.4357,  0.5896,  0.5725,  1.5826, -1.8751,\n",
      "        -0.3282,  0.9054]) tensor([ 0.6829,  0.5253, -0.3936, -0.3041, -0.9321, -0.0889,  1.4455, -1.9175,\n",
      "        -0.2242,  1.5860])\n",
      "R[0]\n",
      "tensor([-0.0053], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.2272224836051464; R = 0.0011350077513952784;                 Gamma = 0.022218888421775773; Q = 0.0012023234521911946;\n",
      "Entropy Neighbor = 0.001063233517401386;                 Entropy Random = 0.0009124663995753508;                 Volume = 2.1720598239898683; VAE = 47.98896701812744\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.0500,  0.2628, -0.8821, -0.2130,  0.5043,  0.3212,  1.4650, -1.5491,\n",
      "         0.1291,  0.7170]) tensor([ 0.9781,  0.1058, -0.9347, -0.3187,  0.5654,  0.3635,  1.3464, -1.4462,\n",
      "        -0.0375,  0.7143]) tensor([ 0.5835, -0.3045, -1.0105, -0.9186,  1.0103,  1.0150,  2.1399, -1.1470,\n",
      "        -0.1434,  0.4932])\n",
      "R[0]\n",
      "tensor([-0.0036], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.1726995325088501; R = 0.0011981547100585886;                 Gamma = 0.02099756339774467; Q = 0.0010699985830287914;\n",
      "Entropy Neighbor = 0.0019654380693100394;                 Entropy Random = 0.00171552166598849;                 Volume = 2.2828164286613464; VAE = 49.44131828308105\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.9376, -0.3612, -0.5602, -0.6968, -0.1948, -0.1051,  1.1990, -1.8724,\n",
      "         0.3208,  0.4358]) tensor([ 0.7657, -0.2285, -0.4328, -0.7303,  0.3589,  0.5093,  1.2089, -1.8980,\n",
      "         0.1341,  0.5657]) tensor([ 1.2973,  0.0172, -0.0611, -0.3191,  0.1968,  0.1740,  0.8147, -1.8494,\n",
      "        -0.1663,  0.7114])\n",
      "R[0]\n",
      "tensor([0.0006], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.13562497952580452; R = 0.0006646841784981916;                 Gamma = 0.018602348239626736; Q = 0.0007837380014680093;\n",
      "Entropy Neighbor = 0.00348246428091079;                 Entropy Random = 0.0028521506164688615;                 Volume = 2.4046123607158663; VAE = 50.21855724334717\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.8383, -0.7097, -0.4018, -0.4153,  0.5612,  0.4055,  1.5826, -2.0716,\n",
      "         0.1000,  0.6637]) tensor([ 1.5877, -0.5298, -0.6326, -0.6382,  0.5127,  0.4139,  1.4955, -2.1383,\n",
      "        -0.1499,  0.9374]) tensor([ 1.8330, -0.2048, -0.7467,  0.0940,  0.8753,  0.0236,  1.2663, -2.0671,\n",
      "        -0.0812,  0.9419])\n",
      "R[0]\n",
      "tensor([-0.0008], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.10990171325206756; R = 0.0011869541890878281;                 Gamma = 0.018150590563192962; Q = 0.0011099071067292242;\n",
      "Entropy Neighbor = 0.005464906467590481;                 Entropy Random = 0.004174027081346139;                 Volume = 2.4088090801239015; VAE = 49.75935292053223\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.1159, -0.1400, -0.7217, -0.7877,  0.4622,  0.3028,  1.2563, -1.4409,\n",
      "         0.2777,  0.7682]) tensor([ 0.9479, -0.1975, -0.6016, -0.5619,  0.3987,  0.2703,  1.1441, -1.4961,\n",
      "         0.0135,  0.6956]) tensor([ 1.2533, -0.1416, -1.0874, -0.7439,  0.4134,  0.3019,  1.3283, -1.2796,\n",
      "        -0.2450,  1.0019])\n",
      "R[0]\n",
      "tensor([-0.0025], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.09083176989853382; R = 0.0009365388209184857;                 Gamma = 0.017042753873043694; Q = 0.0007604862771258922;\n",
      "Entropy Neighbor = 0.00804441399127245;                 Entropy Random = 0.0057102555548772214;                 Volume = 2.3536625833511353; VAE = 48.72241876983642\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.2060, -0.5488, -0.7796, -0.5508,  0.8115,  0.3400,  1.3602, -1.9793,\n",
      "        -0.0310,  0.5453]) tensor([ 1.2037, -0.4135, -0.7569, -0.5914,  0.5389,  0.2706,  1.3473, -1.8930,\n",
      "        -0.0698,  0.6122]) tensor([ 1.6651, -0.5971, -0.8488, -0.8667,  0.5559,  0.3774,  1.0031, -1.3846,\n",
      "        -0.2809,  0.8404])\n",
      "R[0]\n",
      "tensor([-0.0042], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07714664278179408; R = 0.0006493004010103505;                 Gamma = 0.015803280257154256; Q = 0.0006071157163678436;\n",
      "Entropy Neighbor = 0.011027260340750217;                 Entropy Random = 0.007168279255274683;                 Volume = 2.273753823518753; VAE = 47.50910999298096\n",
      "Average (on the epoch) training loss: 0.005843313625238079\n",
      "Episode average V value: 0.07863877895212718\n",
      "epoch 1:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:275: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  abs_states[i:i+1], torch.as_tensor([action_encoding])\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:301: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  ax.scatter(\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.4236, -0.2033, -0.4562, -0.7022,  0.0916,  0.3257,  0.7770, -2.1177,\n",
      "        -0.0353,  0.9046]) tensor([ 1.4265, -0.4433, -0.3746, -0.6656,  0.3548,  0.3689,  0.7101, -2.0564,\n",
      "        -0.1040,  0.8071]) tensor([ 1.7488, -0.2470, -0.6419, -0.7277,  0.4187,  0.4013,  1.1190, -2.2989,\n",
      "        -0.1961,  1.0042])\n",
      "R[0]\n",
      "tensor([-4.6074e-05], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06847252438962459; R = 0.0009049177863053046;                 Gamma = 0.016333105246245396; Q = 0.0007320378847653046;\n",
      "Entropy Neighbor = 0.014011786185204983;                 Entropy Random = 0.008479813725687563;                 Volume = 2.2956529932022094; VAE = 47.107276153564456\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.3439, -0.5752, -1.0429, -0.0489,  0.2629,  0.3068,  1.2532, -1.5345,\n",
      "        -0.0173,  0.7552]) tensor([ 1.3571, -0.3580, -0.8951, -0.3424,  0.5774,  0.2192,  1.2208, -1.4647,\n",
      "        -0.2254,  0.6793]) tensor([ 1.5012, -0.2621, -0.7847, -0.8177,  0.6783,  0.5978,  0.9558, -1.7123,\n",
      "        -0.0285,  0.6667])\n",
      "R[0]\n",
      "tensor([-0.0025], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06190205855667591; R = 0.0009311585473287778;                 Gamma = 0.01626365638617426; Q = 0.0007014519151343848;\n",
      "Entropy Neighbor = 0.01660955430381;                 Entropy Random = 0.009430168138816952;                 Volume = 2.2396864347457885; VAE = 46.6660013961792\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.5708, -0.2382, -0.2355, -0.3167,  0.3683, -0.1648,  0.9499, -1.9706,\n",
      "        -0.0632,  0.9182]) tensor([ 1.6074, -0.5209, -0.5495, -0.6351,  0.3693,  0.2297,  0.8986, -1.9115,\n",
      "        -0.1531,  0.7360]) tensor([ 1.1730, -0.3263, -0.5015, -0.4013,  0.1660,  0.2657,  0.8413, -1.1505,\n",
      "         0.0535,  0.6230])\n",
      "R[0]\n",
      "tensor([-0.0023], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.057234470590949056; R = 0.0009711941316331832;                 Gamma = 0.016168534206110054; Q = 0.00078529919898574;\n",
      "Entropy Neighbor = 0.018568436071276666;                 Entropy Random = 0.010646108959801495;                 Volume = 2.251799651861191; VAE = 46.06133769226074\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.4475,  0.2256, -0.5825, -0.5501,  0.7877,  0.4200,  0.8139, -1.4407,\n",
      "        -0.3150,  0.7527]) tensor([ 1.3912, -0.3332, -0.7616, -0.3424,  0.5181,  0.3264,  0.7088, -1.4278,\n",
      "        -0.2706,  0.7778]) tensor([ 1.6598, -0.3106, -1.1165, -0.2993,  0.5652,  0.2756,  0.7427, -1.4820,\n",
      "        -0.2421,  0.8548])\n",
      "R[0]\n",
      "tensor([0.0012], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05541301850229502; R = 0.0010742600483483783;                 Gamma = 0.015266610846912955; Q = 0.000749797466552991;\n",
      "Entropy Neighbor = 0.01984668814577162;                 Entropy Random = 0.010262363040819764;                 Volume = 2.2462579805850984; VAE = 45.01613921356201\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.5431, -0.4046, -0.7983, -0.0940,  0.4420,  0.3445,  0.7703, -2.0540,\n",
      "        -0.0227,  0.6264]) tensor([ 1.6097, -0.4804, -0.7577, -0.5062,  0.5167,  0.2436,  0.6886, -1.8423,\n",
      "        -0.1019,  0.5610]) tensor([ 1.7275, -0.5339, -0.6285, -0.5555,  0.9001, -0.1195,  0.7541, -1.8701,\n",
      "        -0.3588,  0.7971])\n",
      "R[0]\n",
      "tensor([-0.0035], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05433796190470457; R = 0.0013634269199010304;                 Gamma = 0.01363391699141357; Q = 0.0010253141781140585;\n",
      "Entropy Neighbor = 0.0208227711096406;                 Entropy Random = 0.011036455354653299;                 Volume = 2.3683286275863646; VAE = 44.55486614227295\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 2.2017, -0.5736, -0.9211, -0.8100,  0.7378,  0.0156,  0.9341, -2.1267,\n",
      "         0.0466,  0.6132]) tensor([ 2.1688, -0.6942, -0.8817, -0.6055,  0.5530,  0.0079,  0.9011, -2.1238,\n",
      "        -0.0596,  0.5933]) tensor([ 2.0703, -0.4926, -1.0474, -0.5404,  0.7737,  0.0691,  1.0538, -2.0426,\n",
      "        -0.0838,  0.8211])\n",
      "R[0]\n",
      "tensor([-0.0003], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05240778163820505; R = 0.0010621045941406919;                 Gamma = 0.013636477726511657; Q = 0.0008580922574037686;\n",
      "Entropy Neighbor = 0.02119794061034918;                 Entropy Random = 0.010631846135482192;                 Volume = 2.451188819885254; VAE = 44.33899997711182\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 2.0705, -0.5105, -1.2237, -0.3867,  0.6377,  0.0873,  0.9546, -1.8737,\n",
      "        -0.5749,  0.4753]) tensor([ 2.1568, -0.5880, -1.0815, -0.3927,  0.6277,  0.1038,  0.9647, -1.7150,\n",
      "        -0.2944,  0.6779]) tensor([ 2.1732, -0.9956, -1.1388, -0.2936,  0.1939, -0.1803,  0.9026, -2.0536,\n",
      "        -0.3886,  0.5090])\n",
      "R[0]\n",
      "tensor([0.0005], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05228360096365214; R = 0.0009504296636305298;                 Gamma = 0.012295888548658694; Q = 0.0007287162679713219;\n",
      "Entropy Neighbor = 0.02127220499329269;                 Entropy Random = 0.011054743152111768;                 Volume = 2.588693051815033; VAE = 43.93545873260498\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.8031, -0.7429, -1.0136, -0.4844,  0.4519,  0.0716,  1.0720, -1.4136,\n",
      "        -0.4888,  0.9048]) tensor([ 1.8514, -0.5836, -0.9502, -0.3507,  0.5679,  0.2035,  1.1235, -1.4317,\n",
      "        -0.4266,  0.7612]) tensor([ 1.7305, -0.7357, -0.9812, -0.0680,  0.5390, -0.1181,  1.0139, -1.7277,\n",
      "        -0.2239,  0.7390])\n",
      "R[0]\n",
      "tensor([-0.0031], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05196144385635853; R = 0.0011880151092000233;                 Gamma = 0.012340774322918151; Q = 0.0007802068545861402;\n",
      "Entropy Neighbor = 0.021521116584539413;                 Entropy Random = 0.011087546899914742;                 Volume = 2.6797213811874387; VAE = 43.91422695159912\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.8261, -0.6622, -0.7447, -0.4134,  0.6717,  0.3954,  0.9799, -1.7048,\n",
      "         0.0545,  0.6061]) tensor([ 1.7745, -0.6823, -0.7945, -0.5504,  0.4921,  0.1476,  0.9282, -1.7215,\n",
      "        -0.1858,  0.5664]) tensor([ 1.7575, -0.8612, -0.6769, -0.6545,  0.6758,  0.4010,  0.6503, -1.4194,\n",
      "        -0.2646,  0.3499])\n",
      "R[0]\n",
      "tensor([-0.0014], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05273743768036365; R = 0.0016421283825911815;                 Gamma = 0.01192458222375717; Q = 0.0010011129871636514;\n",
      "Entropy Neighbor = 0.021469157122075557;                 Entropy Random = 0.011358617341145873;                 Volume = 2.8575559740066527; VAE = 43.966973220825196\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2060,  0.1244, -0.3927, -0.1651,  0.2122,  0.7265,  0.4916, -0.5734,\n",
      "         0.0625,  0.2872]) tensor([ 0.1488, -0.2508, -0.3986, -0.2040,  0.1958,  0.4468,  0.3923, -0.5554,\n",
      "        -0.1296,  0.3074]) tensor([ 0.3662, -0.4101, -0.4321, -0.3679,  0.2970,  0.2856,  0.2916, -0.6270,\n",
      "        -0.0375,  0.7714])\n",
      "R[0]\n",
      "tensor([0.0006], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.052983292482793334; R = 0.0015733332653308026;                 Gamma = 0.010750778779794927; Q = 0.0011201152688954607;\n",
      "Entropy Neighbor = 0.02143314445577562;                 Entropy Random = 0.011080607284791768;                 Volume = 2.8106035661697386; VAE = 42.61076789855957\n",
      "Average (on the epoch) training loss: 0.0008482144279572822\n",
      "Episode average V value: 0.06039278320118445\n",
      "epoch 2:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 0.2\n",
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:301: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  ax.scatter(\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 2.3349, -0.8924, -0.9946, -0.5227,  0.5999,  0.2913,  0.7322, -2.0869,\n",
      "        -0.4774,  0.6876]) tensor([ 2.4056, -0.7047, -0.9906, -0.5637,  0.5919,  0.1764,  0.7793, -2.0376,\n",
      "        -0.2415,  0.5821]) tensor([ 2.1987, -0.7631, -1.0270, -0.7809,  0.6533,  0.0892,  0.8639, -1.8870,\n",
      "        -0.1144,  0.6436])\n",
      "R[0]\n",
      "tensor([0.0003], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05364042358100414; R = 0.002068844107898258;                 Gamma = 0.0123453104988439; Q = 0.0015170180659624748;\n",
      "Entropy Neighbor = 0.021732441695407032;                 Entropy Random = 0.011199204393662513;                 Volume = 2.7656409177780152; VAE = 41.57756015014648\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 2.2990, -0.7966, -0.9446, -0.2989,  0.4256,  0.2478,  0.9587, -1.9114,\n",
      "        -0.2235,  0.8533]) tensor([ 2.3279, -0.7467, -1.0029, -0.5940,  0.5459,  0.1540,  0.9701, -1.8850,\n",
      "        -0.2328,  0.5890]) tensor([ 2.2265, -1.2579, -1.2546, -0.4078,  0.5233, -0.1470,  1.0179, -2.0106,\n",
      "        -0.1504,  0.9309])\n",
      "R[0]\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.052928408689796924; R = 0.001715752084241103;                 Gamma = 0.012312140138237736; Q = 0.001318240004206018;\n",
      "Entropy Neighbor = 0.02150806898623705;                 Entropy Random = 0.011318098199553787;                 Volume = 2.843746811389923; VAE = 41.7433150177002\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 2.0167, -0.4532, -0.9436, -0.5444,  0.6084, -0.1881,  0.8473, -1.6923,\n",
      "        -0.1396,  0.3072]) tensor([ 2.0209, -0.7074, -0.8832, -0.5473,  0.4588,  0.1447,  0.8218, -1.6585,\n",
      "        -0.1407,  0.5302]) tensor([ 2.1134, -0.4896, -1.0122, -0.2116,  0.4276,  0.2008,  0.9166, -1.6836,\n",
      "        -0.2292,  0.9382])\n",
      "R[0]\n",
      "tensor([-0.0036], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.052512406170368196; R = 0.0016506904614079759;                 Gamma = 0.010656857494468567; Q = 0.001344294401758816;\n",
      "Entropy Neighbor = 0.021879475427791475;                 Entropy Random = 0.011687005914747715;                 Volume = 2.823804681301117; VAE = 41.16387770843506\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.6544, -0.2953, -0.7383, -0.3042,  0.3616,  0.3184,  0.8282, -1.3733,\n",
      "        -0.1463,  0.4391]) tensor([ 1.6331, -0.6082, -0.8286, -0.4541,  0.4525,  0.2207,  0.7830, -1.3694,\n",
      "        -0.2229,  0.5033]) tensor([ 1.6864, -0.3667, -0.8524, -1.3371,  0.2508,  0.2363,  0.8605, -1.3400,\n",
      "        -0.2644,  1.0747])\n",
      "R[0]\n",
      "tensor([-0.0025], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05337259778380394; R = 0.0016930778606147215;                 Gamma = 0.011114127446111524; Q = 0.0011601026584467035;\n",
      "Entropy Neighbor = 0.02188787451386452;                 Entropy Random = 0.011663449748419225;                 Volume = 2.844163779735565; VAE = 41.101537353515624\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.9102, -0.5573, -0.9464, -0.3834,  0.2251,  0.2131,  0.9008, -1.7404,\n",
      "        -0.2364,  0.6675]) tensor([ 1.9328, -0.6917, -0.8662, -0.5558,  0.4936,  0.2254,  0.8882, -1.6805,\n",
      "        -0.2016,  0.4713]) tensor([ 1.7673, -0.8164, -0.9489, -0.2727,  0.6252,  0.0264,  0.9765, -1.6161,\n",
      "        -0.1193,  0.4640])\n",
      "R[0]\n",
      "tensor([0.0020], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05244986143708229; R = 0.001434693264197449;                 Gamma = 0.011007659879571292; Q = 0.0011242363920027855;\n",
      "Entropy Neighbor = 0.021600885113701226;                 Entropy Random = 0.011589385023340584;                 Volume = 2.805772312641144; VAE = 40.3371485748291\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.6755, -0.8907, -0.6418, -0.2573,  0.4237,  0.3658,  0.7985, -1.3024,\n",
      "        -0.1810,  0.6993]) tensor([ 1.6599, -0.6070, -0.8057, -0.4437,  0.4454,  0.2265,  0.8304, -1.3682,\n",
      "        -0.1984,  0.4554]) tensor([ 1.7643, -0.6675, -0.9060, -0.6823,  0.7043,  0.2273,  0.6765, -1.3967,\n",
      "        -0.0068,  0.5349])\n",
      "R[0]\n",
      "tensor([-0.0028], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05280858498811722; R = 0.001667917984597807;                 Gamma = 0.010132879114185925; Q = 0.0011464615997392683;\n",
      "Entropy Neighbor = 0.021814390173181892;                 Entropy Random = 0.011559961882419884;                 Volume = 2.749950825691223; VAE = 39.637720169067386\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.8663, -1.2212, -0.8340, -0.8231,  0.2351, -0.0280,  0.8382, -1.6941,\n",
      "         0.0280, -0.0937]) tensor([ 1.8584, -0.7905, -0.6740, -0.6896,  0.4006,  0.0921,  0.8578, -1.7255,\n",
      "         0.0478,  0.3696]) tensor([ 1.8753, -1.0605, -0.7315, -0.4635,  0.5107,  0.0896,  0.7393, -1.7143,\n",
      "         0.2380,  0.6669])\n",
      "R[0]\n",
      "tensor([2.9266e-05], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05157523067295551; R = 0.001146001729526688;                 Gamma = 0.01071585230110213; Q = 0.001083283753861906;\n",
      "Entropy Neighbor = 0.021724913029000162;                 Entropy Random = 0.011310887018218637;                 Volume = 2.7443572964668275; VAE = 39.13743424987793\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 2.2521, -0.9766, -1.0171, -0.7839,  0.3419, -0.1749,  1.0586, -1.8522,\n",
      "        -0.0358,  0.1006]) tensor([ 2.2500, -0.8812, -0.8570, -0.7073,  0.5056,  0.0442,  1.0519, -1.8495,\n",
      "        -0.0517,  0.4455]) tensor([ 2.3145, -0.7720, -0.7712, -0.5620,  0.9749,  0.4853,  0.9698, -1.8285,\n",
      "         0.1069,  0.6511])\n",
      "R[0]\n",
      "tensor([0.0054], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.052340561009943484; R = 0.0013631714690500302;                 Gamma = 0.010446521586331073; Q = 0.0012195957213116343;\n",
      "Entropy Neighbor = 0.021823602486401795;                 Entropy Random = 0.010918165510520338;                 Volume = 2.630811065673828; VAE = 37.75822129821778\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 2.0422, -0.4314, -0.9425,  0.3756,  0.4593,  0.0920,  0.8980, -1.7384,\n",
      "        -0.3131,  0.7449]) tensor([ 2.0581, -0.6503, -0.9965, -0.4452,  0.5193,  0.3078,  0.8847, -1.6771,\n",
      "        -0.2941,  0.4874]) tensor([ 1.8493, -0.6587, -0.9729, -0.3738,  0.5085,  0.4006,  0.8135, -1.4373,\n",
      "        -0.3078,  0.0861])\n",
      "R[0]\n",
      "tensor([-3.6478e-05], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05249262814968824; R = 0.0016000987797251582;                 Gamma = 0.008888268711511046; Q = 0.0012977804169931914;\n",
      "Entropy Neighbor = 0.021874838821589946;                 Entropy Random = 0.011038171164691447;                 Volume = 2.5907987275123596; VAE = 37.23068890380859\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2125, -0.4431,  0.1751,  0.0067,  0.0673,  0.1140, -0.0068, -0.1688,\n",
      "         0.2560, -0.0547]) tensor([-0.2463, -0.1755,  0.0207, -0.2681,  0.0610,  0.2437, -0.0038, -0.2347,\n",
      "         0.2183,  0.1092]) tensor([-0.1671, -0.0385, -0.2075, -0.5998, -0.2216,  0.2696,  0.2366, -0.1047,\n",
      "         0.3354,  0.3983])\n",
      "R[0]\n",
      "tensor([0.0329], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05234796877205372; R = 0.0018003094332625551;                 Gamma = 0.009847090982366353; Q = 0.0012340428101961151;\n",
      "Entropy Neighbor = 0.0223315050881356;                 Entropy Random = 0.010556887842714786;                 Volume = 2.440874917984009; VAE = 36.01728948974609\n",
      "Average (on the epoch) training loss: 0.0012445055824478913\n",
      "Episode average V value: 0.0489038581546127\n",
      "epoch 3:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 0.2\n",
      "Testing score per episode (id: 0) is 3.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 2.9997000299970003 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-1.0050,  0.1290,  0.8846, -0.1857,  0.0636,  0.1142, -0.3227,  0.6407,\n",
      "         0.5916, -0.6889]) tensor([-1.0725,  0.0012,  0.4073, -0.2177, -0.1438,  0.1506, -0.2959,  0.4257,\n",
      "         0.4665, -0.0062]) tensor([-1.2996, -0.3786,  1.0037, -0.2428, -0.3470, -0.2446, -0.5171,  0.6881,\n",
      "         0.5668, -0.0026])\n",
      "R[0]\n",
      "tensor([0.0544], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05209324613958597; R = 0.0018414062999654562;                 Gamma = 0.010011152709121235; Q = 0.0014038665456391755;\n",
      "Entropy Neighbor = 0.02246588676609099;                 Entropy Random = 0.010075798435136675;                 Volume = 2.2430645489692687; VAE = 34.34252880859375\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.6427, -0.6311, -1.1426, -0.6079,  0.3352,  0.3743,  0.8523, -1.3637,\n",
      "        -0.2632,  0.1563]) tensor([ 1.6596, -0.5720, -0.8170, -0.3845,  0.4563,  0.3109,  0.8656, -1.3583,\n",
      "        -0.2564,  0.4194]) tensor([ 1.6819, -0.5548, -0.6435, -0.4759,  0.4984,  0.2244,  0.7268, -1.3518,\n",
      "        -0.3218,  0.6116])\n",
      "R[0]\n",
      "tensor([0.0073], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.052464875675737856; R = 0.0025112801598115766;                 Gamma = 0.00936594497965416; Q = 0.001737644273089245;\n",
      "Entropy Neighbor = 0.02326077813282609;                 Entropy Random = 0.009908035995438696;                 Volume = 2.1261746745109558; VAE = 33.14216304016113\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.7694, -0.8234, -0.9043, -0.6457,  0.6401, -0.2086,  0.5212, -1.4645,\n",
      "         0.0952,  0.4369]) tensor([ 1.7639, -0.6558, -0.6799, -0.5354,  0.4336,  0.0368,  0.5169, -1.4536,\n",
      "         0.0556,  0.2736]) tensor([ 1.8040e+00, -6.1883e-01, -2.7193e-01, -8.2661e-01, -6.9290e-05,\n",
      "        -1.4078e-01,  4.9199e-01, -1.5307e+00,  3.1860e-01,  1.2864e-01])\n",
      "R[0]\n",
      "tensor([0.0013], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05253626874089241; R = 0.002610431149983924;                 Gamma = 0.010561678699596087; Q = 0.001708235740690725;\n",
      "Entropy Neighbor = 0.023607647260650993;                 Entropy Random = 0.009596253664232791;                 Volume = 2.0348674037456513; VAE = 32.058167503356934\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.6984, -0.8550, -0.7046, -0.3068,  0.8474,  0.4300,  0.6901, -1.3757,\n",
      "        -0.0723,  0.0265]) tensor([ 1.7000, -0.6092, -0.7599, -0.4287,  0.4190,  0.2064,  0.6951, -1.3820,\n",
      "        -0.1150,  0.3297]) tensor([ 1.7381, -0.6294, -1.1078,  0.0897,  0.3208,  0.5916,  0.8611, -1.3433,\n",
      "        -0.2241,  0.1850])\n",
      "R[0]\n",
      "tensor([-0.0044], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05411164175719023; R = 0.003715372800765181;                 Gamma = 0.009089334278556635; Q = 0.002027653260665829;\n",
      "Entropy Neighbor = 0.02388265163451433;                 Entropy Random = 0.009583186434581876;                 Volume = 1.9845018012523652; VAE = 31.460891918182373\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.7632, -0.8259, -0.6650, -0.3667,  0.3011,  0.2239,  0.6610, -1.4083,\n",
      "        -0.4229,  0.1314]) tensor([ 1.7664, -0.6007, -0.8000, -0.4019,  0.4654,  0.2408,  0.6807, -1.4108,\n",
      "        -0.1613,  0.3136]) tensor([ 1.8585, -0.1658, -0.7764, -0.5111,  0.3011,  0.0744,  0.7078, -1.4985,\n",
      "        -0.3087,  0.0950])\n",
      "R[0]\n",
      "tensor([-0.0016], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.054810908034443856; R = 0.0037782913826195;                 Gamma = 0.008534144507022574; Q = 0.0023706038087548224;\n",
      "Entropy Neighbor = 0.023799223341047764;                 Entropy Random = 0.00882973379921168;                 Volume = 1.8676403017044068; VAE = 30.587339328765868\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5733,  0.0357, -0.8475,  0.2049, -0.0755,  0.8377,  0.6755, -0.6767,\n",
      "        -0.9521,  0.3008]) tensor([ 0.6006, -0.1258, -0.8528,  0.0894,  0.3223,  0.8512,  0.7004, -0.6657,\n",
      "        -0.6211,  0.4063]) tensor([ 0.3990,  0.2878, -0.8621, -0.1450,  0.0675,  1.1358,  0.5595, -0.6219,\n",
      "        -0.6788,  0.3753])\n",
      "R[0]\n",
      "tensor([0.0424], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05563685931265354; R = 0.004257143339440518;                 Gamma = 0.008893710520438617; Q = 0.002548497076073545;\n",
      "Entropy Neighbor = 0.023988333959132434;                 Entropy Random = 0.008526501146145164;                 Volume = 1.7969376764297484; VAE = 30.016955879211427\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.5801, -0.1975, -1.1684, -0.3730,  0.4254,  0.6136,  0.6599, -1.1211,\n",
      "        -0.3563,  0.2680]) tensor([ 1.5986, -0.4484, -0.9403, -0.1646,  0.4558,  0.4701,  0.6691, -1.1402,\n",
      "        -0.4306,  0.3822]) tensor([ 1.6211, -0.2719, -0.9599, -0.0910, -0.0764,  0.5012,  0.4834, -1.0295,\n",
      "        -0.4257,  0.4332])\n",
      "R[0]\n",
      "tensor([-0.0025], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05650009451061487; R = 0.004502805832191371;                 Gamma = 0.008768092890590197; Q = 0.002854710192361381;\n",
      "Entropy Neighbor = 0.024158955121412872;                 Entropy Random = 0.008226486113388092;                 Volume = 1.6170412380695343; VAE = 28.456466327667236\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.7598, -0.7128, -0.9496, -0.5541,  0.0374,  0.0531,  0.7235, -1.2432,\n",
      "         0.2299,  0.4274]) tensor([ 1.7415, -0.6485, -0.6698, -0.5014,  0.3820,  0.0983,  0.7046, -1.2597,\n",
      "         0.0426,  0.2471]) tensor([ 1.7500, -0.8656, -0.7005, -0.5099,  0.1382,  0.2764,  0.8197, -1.3789,\n",
      "         0.2986,  0.2335])\n",
      "R[0]\n",
      "tensor([-0.0036], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05831018005311489; R = 0.0053902699697137;                 Gamma = 0.007993575396976666; Q = 0.0031186116904136723;\n",
      "Entropy Neighbor = 0.02460586017742753;                 Entropy Random = 0.008325071195140482;                 Volume = 1.693915629863739; VAE = 28.64032518005371\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.9384, -0.9404, -0.9531, -0.7761,  0.5134,  0.2276,  0.9211, -1.5683,\n",
      "         0.0972, -0.0344]) tensor([ 1.9273, -0.7353, -0.7459, -0.5739,  0.4132,  0.1247,  0.9163, -1.5492,\n",
      "         0.0330,  0.2098]) tensor([ 1.8841, -0.7375, -0.7767, -0.6859,  0.5553,  0.3147,  0.8788, -1.5124,\n",
      "        -0.3238, -0.2725])\n",
      "R[0]\n",
      "tensor([-9.6157e-05], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.0604130867049098; R = 0.005665208833568613;                 Gamma = 0.009233267280360452; Q = 0.0037761746841133573;\n",
      "Entropy Neighbor = 0.024185482282191514;                 Entropy Random = 0.007471709680277854;                 Volume = 1.5121439211368561; VAE = 27.22145429611206\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.6711, -0.6133, -0.5219, -1.1175,  0.1598, -0.1618,  0.6807, -1.3005,\n",
      "         0.1734,  0.1107]) tensor([ 1.6472, -0.7591, -0.4755, -0.6282,  0.3600, -0.0720,  0.6677, -1.3069,\n",
      "         0.2251,  0.1392]) tensor([ 1.6605, -0.4293, -0.3404, -0.3005,  0.2561, -0.2936,  0.6207, -1.2472,\n",
      "         0.3077,  0.2573])\n",
      "R[0]\n",
      "tensor([0.0080], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06136367944628; R = 0.006177391418452316;                 Gamma = 0.008338075824460247; Q = 0.0038663451611064374;\n",
      "Entropy Neighbor = 0.023701136767864228;                 Entropy Random = 0.0069580511758103964;                 Volume = 1.524687992811203; VAE = 27.172696853637696\n",
      "Average (on the epoch) training loss: 0.002541234243290819\n",
      "Episode average V value: 0.18217389872638576\n",
      "epoch 4:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 0.2\n",
      "Testing score per episode (id: 0) is 45.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 44.995500449955 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:301: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  ax.scatter(\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.6218, -0.4334, -0.8057, -0.4389,  0.6655,  0.8157,  0.4987, -1.1373,\n",
      "        -0.6643, -0.1418]) tensor([ 1.6375, -0.4137, -0.9278, -0.1197,  0.3727,  0.5465,  0.5390, -1.1454,\n",
      "        -0.4995,  0.2456]) tensor([ 1.5553, -0.5352, -0.5488, -0.2900,  0.5329,  0.6024,  0.2656, -1.0613,\n",
      "        -0.3348,  0.3639])\n",
      "R[0]\n",
      "tensor([0.0012], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06057524698227644; R = 0.006168485763802891;                 Gamma = 0.00861816825234564; Q = 0.004483690288790967;\n",
      "Entropy Neighbor = 0.024428422790020703;                 Entropy Random = 0.00637171443272382;                 Volume = 1.330993589282036; VAE = 25.792032245635987\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.4838, -0.4710,  0.0188, -1.0798,  0.0649,  0.0323,  0.5055, -1.1768,\n",
      "         0.3588,  0.4413]) tensor([ 1.4835, -0.7022, -0.3008, -0.7362,  0.3147, -0.1291,  0.5090, -1.1868,\n",
      "         0.3942,  0.1013]) tensor([ 1.5061, -0.7075,  0.0580, -0.4221,  0.3338, -0.6707,  0.5159, -1.1775,\n",
      "         0.3122, -0.1450])\n",
      "R[0]\n",
      "tensor([-0.0032], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06072048255056143; R = 0.006681455930833181;                 Gamma = 0.008162526002473896; Q = 0.005074221594724804;\n",
      "Entropy Neighbor = 0.024370462499558924;                 Entropy Random = 0.006334610620513558;                 Volume = 1.29212158203125; VAE = 25.686834491729737\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.7189, -0.6236, -0.5378, -0.5908,  0.5608, -0.0683,  0.6998, -1.2964,\n",
      "         0.1605,  0.6342]) tensor([ 1.7254, -0.6998, -0.5678, -0.5382,  0.3602,  0.0554,  0.6963, -1.2885,\n",
      "         0.1356,  0.1250]) tensor([ 1.5576, -0.7478, -0.6038, -0.2501,  0.0908,  0.0292,  0.5465, -1.2406,\n",
      "         0.2156,  0.2813])\n",
      "R[0]\n",
      "tensor([0.0018], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.059225192144513134; R = 0.007421800477182842;                 Gamma = 0.007925951511613676; Q = 0.004918167556985281;\n",
      "Entropy Neighbor = 0.025118432641029358;                 Entropy Random = 0.005809638301376254;                 Volume = 1.0589783529043197; VAE = 24.198776988983155\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.8943, -0.2820, -0.5147, -0.4486,  0.0521,  0.3038,  0.7575, -1.0843,\n",
      "         0.1614,  0.1782]) tensor([ 0.9154, -0.4523, -0.4244, -0.5047,  0.1677,  0.2831,  0.7550, -1.0514,\n",
      "         0.1382,  0.0729]) tensor([-0.7670, -0.0525,  0.0175, -0.4484, -0.2052,  1.0783,  0.4476, -0.1003,\n",
      "        -0.2101, -0.0394])\n",
      "R[0]\n",
      "tensor([0.0065], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05761737107485533; R = 0.006986625947916764;                 Gamma = 0.007997716929123272; Q = 0.004675280252587981;\n",
      "Entropy Neighbor = 0.025459669437259434;                 Entropy Random = 0.005837118328781798;                 Volume = 1.0705051172971725; VAE = 23.93805638885498\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.3555, -1.2613,  0.1199, -1.0561,  0.5147, -0.1563,  0.4350, -1.1375,\n",
      "         0.3226,  0.3817]) tensor([ 1.3173, -0.8115, -0.1193, -0.7773,  0.3181, -0.1933,  0.4338, -1.1176,\n",
      "         0.4522,  0.0370]) tensor([ 1.3459, -0.7311, -0.2342, -0.4460,  0.3286, -0.1731,  0.4296, -1.1575,\n",
      "         0.1487, -0.1286])\n",
      "R[0]\n",
      "tensor([0.0057], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.056988308317959306; R = 0.0072833413437329;                 Gamma = 0.008140243812682456; Q = 0.004643844009842723;\n",
      "Entropy Neighbor = 0.026476251255720853;                 Entropy Random = 0.005480456257006153;                 Volume = 0.895877214550972; VAE = 23.18322091293335\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.3965, -0.4510, -0.5772,  0.1581,  0.1511,  0.5288,  0.7726, -1.2278,\n",
      "         0.0152,  0.1195]) tensor([ 1.4354, -0.5467, -0.7277, -0.4444,  0.3018,  0.3417,  0.7627, -1.2197,\n",
      "        -0.0796,  0.1241]) tensor([ 1.4635, -0.7721, -0.6677, -0.6070,  0.1995,  0.1723,  0.6755, -1.0721,\n",
      "         0.0395, -0.0351])\n",
      "R[0]\n",
      "tensor([-0.0061], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.057840775690972804; R = 0.007474516659844084;                 Gamma = 0.007456496621438419; Q = 0.004524493106175214;\n",
      "Entropy Neighbor = 0.02687291793897748;                 Entropy Random = 0.005193014333955944;                 Volume = 0.8744788118600846; VAE = 23.30963306427002\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-1.1806,  0.6598, -0.5000,  0.0370, -0.3350,  1.5864,  0.2585,  0.1132,\n",
      "        -0.3090,  0.4555]) tensor([-1.0445,  0.3846, -0.5219,  0.0496, -0.2230,  1.1485,  0.3475,  0.0113,\n",
      "        -0.2549,  0.2456]) tensor([-1.3294,  0.5135, -0.3221,  0.3362, -0.0642,  0.9587,  0.1866,  0.2927,\n",
      "        -0.2993, -0.2488])\n",
      "R[0]\n",
      "tensor([0.3020], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05759765700995922; R = 0.008106689904889209;                 Gamma = 0.007835081501922105; Q = 0.004991439438657835;\n",
      "Entropy Neighbor = 0.026786327999085188;                 Entropy Random = 0.004868864669115283;                 Volume = 0.7920063823461533; VAE = 23.24419379425049\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.7776, -0.5182, -0.6984, -0.3735,  0.0590,  0.1613,  0.8659, -1.3020,\n",
      "        -0.0052,  0.0071]) tensor([ 1.7535, -0.6768, -0.7200, -0.5059,  0.3171,  0.2166,  0.8245, -1.2605,\n",
      "        -0.0197,  0.0985]) tensor([ 1.8152, -0.5711, -0.6852, -0.7616,  0.7136,  0.3506,  0.7742, -1.5096,\n",
      "        -0.1513,  0.3115])\n",
      "R[0]\n",
      "tensor([-0.0023], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05635456352680922; R = 0.007763601882004877;                 Gamma = 0.0077623445186764005; Q = 0.00471296895085834;\n",
      "Entropy Neighbor = 0.027725352436304092;                 Entropy Random = 0.0046647802595980465;                 Volume = 0.794114542722702; VAE = 23.316368183135985\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.1495, -0.8674, -0.8119,  0.2515,  0.0989,  0.4901,  0.2787, -0.8436,\n",
      "        -0.3497,  0.5735]) tensor([ 1.1717, -0.4286, -0.7219, -0.1669,  0.2819,  0.5527,  0.2963, -0.8571,\n",
      "        -0.3467,  0.1210]) tensor([ 1.1491, -0.4611, -0.8894,  0.3920,  0.6375,  0.6540,  0.3664, -0.6840,\n",
      "        -0.1814, -0.0698])\n",
      "R[0]\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05505352884531021; R = 0.008184864635688427;                 Gamma = 0.007814484192815144; Q = 0.004707108199829235;\n",
      "Entropy Neighbor = 0.02779377681389451;                 Entropy Random = 0.004717495006043464;                 Volume = 0.6539298443198204; VAE = 22.96476890182495\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.9745, -0.0347, -0.4004, -0.4201,  0.4082,  0.3465,  0.1585, -0.6814,\n",
      "        -0.2793, -0.1499]) tensor([ 0.9950, -0.4125, -0.5046, -0.2235,  0.1950,  0.3407,  0.1596, -0.6927,\n",
      "        -0.1938,  0.0470]) tensor([ 0.8575, -0.4561, -0.4303, -0.0029, -0.0339,  0.3307,  0.2338, -0.5099,\n",
      "        -0.0923,  0.0642])\n",
      "R[0]\n",
      "tensor([-0.0052], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05495658532530069; R = 0.008029042364723864;                 Gamma = 0.007308319446790847; Q = 0.004753925565746613;\n",
      "Entropy Neighbor = 0.028287278641015292;                 Entropy Random = 0.00458613310335204;                 Volume = 0.6650737398266793; VAE = 23.11414601135254\n",
      "Average (on the epoch) training loss: 0.0047485138964198995\n",
      "Episode average V value: 0.3329202134296298\n",
      "epoch 5:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 0.2\n",
      "Testing score per episode (id: 0) is 12.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 11.998800119988001 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:301: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  ax.scatter(\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.9327, -0.5543, -0.5210, -0.8013,  0.2044, -0.1255,  0.4879, -0.8737,\n",
      "         0.2501, -0.2630]) tensor([ 0.8294, -0.4900, -0.3220, -0.5482,  0.1582,  0.1249,  0.4607, -0.8232,\n",
      "         0.2744, -0.0280]) tensor([ 0.3297, -0.0586, -0.2524, -0.8946, -0.1654,  0.4131,  0.4970, -0.5956,\n",
      "         0.2355, -0.2224])\n",
      "R[0]\n",
      "tensor([0.0055], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05576662161201239; R = 0.009202222963387612;                 Gamma = 0.0071913140265533; Q = 0.005266884192358702;\n",
      "Entropy Neighbor = 0.028166944582015276;                 Entropy Random = 0.004596059827366844;                 Volume = 0.5961286343932152; VAE = 23.297647861480712\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.6204e+00, -6.4332e-01, -8.0853e-01, -1.3663e-03,  3.3086e-01,\n",
      "         1.0509e+00,  8.8226e-01, -1.3814e+00, -2.0351e-02, -2.8104e-01]) tensor([ 1.6552, -0.5434, -0.8768, -0.4236,  0.3352,  0.4592,  0.8371, -1.3199,\n",
      "        -0.1349,  0.0857]) tensor([ 1.6883, -0.7864, -0.4265, -0.4802,  0.2789,  0.4358,  0.9953, -1.3269,\n",
      "        -0.0648,  0.1520])\n",
      "R[0]\n",
      "tensor([-0.0053], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.056078258126974104; R = 0.00918829148230725;                 Gamma = 0.008218305083486484; Q = 0.005350723865209147;\n",
      "Entropy Neighbor = 0.027843236077576876;                 Entropy Random = 0.00434366651554592;                 Volume = 0.5830048957467079; VAE = 23.365242855072022\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4693, -0.2390, -0.6099, -0.5851,  0.1857,  0.6795,  1.0564, -1.1096,\n",
      "         0.3075,  0.6161]) tensor([ 0.5510, -0.3671, -0.5722, -0.6397,  0.0894,  0.4487,  1.0516, -1.0881,\n",
      "         0.2611, -0.0052]) tensor([ 0.5032, -0.2773, -0.8924, -0.6049, -0.4958,  0.3310,  1.0334, -1.0652,\n",
      "         0.3781, -0.0253])\n",
      "R[0]\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.0561352518722415; R = 0.00873969946584839;                 Gamma = 0.006967034771136241; Q = 0.005062895583803765;\n",
      "Entropy Neighbor = 0.02810487048327923;                 Entropy Random = 0.004273151688743383;                 Volume = 0.5221385203003883; VAE = 23.692956577301025\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.4038, -0.8602, -0.1970, -0.6053,  0.0353,  0.1746,  0.4271, -1.1122,\n",
      "         0.0972, -0.4300]) tensor([ 1.3250, -0.6991, -0.3738, -0.5781,  0.2642,  0.0626,  0.3948, -1.0514,\n",
      "         0.1672, -0.0797]) tensor([ 1.4480, -0.9560, -0.3560, -0.5735,  0.6141, -0.2248,  0.4432, -1.0593,\n",
      "         0.1779,  0.0663])\n",
      "R[0]\n",
      "tensor([0.0100], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05572115524113178; R = 0.008738099680151209;                 Gamma = 0.007637529421132058; Q = 0.004938669631956145;\n",
      "Entropy Neighbor = 0.02798914994671941;                 Entropy Random = 0.004307997960131616;                 Volume = 0.5264527060985565; VAE = 24.04406820297241\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.5352, -0.6464, -0.9470, -1.0959,  0.2343,  0.0545,  0.9278, -1.3349,\n",
      "        -0.1630, -0.0563]) tensor([ 1.3810, -0.5929, -0.7819, -0.6023,  0.3182,  0.3227,  0.8765, -1.2394,\n",
      "        -0.0439, -0.0061]) tensor([ 1.0867, -0.2105, -0.6764, -0.3239,  0.0643,  0.4992,  0.9997, -1.2029,\n",
      "         0.1059,  0.1735])\n",
      "R[0]\n",
      "tensor([0.0035], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05717877092957497; R = 0.009520123288006289;                 Gamma = 0.007317135668126866; Q = 0.00600438700558152;\n",
      "Entropy Neighbor = 0.028366243660449982;                 Entropy Random = 0.004202878596493974;                 Volume = 0.4524503368139267; VAE = 24.309691535949707\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.5289, -0.6790, -0.6763, -0.5964,  0.0727,  0.4567,  1.0435, -1.4579,\n",
      "        -0.0947,  0.1228]) tensor([ 1.4808, -0.6100, -0.8015, -0.6137,  0.3173,  0.3325,  0.9982, -1.3846,\n",
      "         0.0120,  0.0138]) tensor([ 1.5079, -0.4082, -0.8986, -0.6879, -0.0199, -0.0094,  0.9781, -1.3351,\n",
      "        -0.0131, -0.2399])\n",
      "R[0]\n",
      "tensor([-0.0012], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05748797369748354; R = 0.01008915714843897;                 Gamma = 0.007050008093938231; Q = 0.005874234935734421;\n",
      "Entropy Neighbor = 0.028301584873348475;                 Entropy Random = 0.004144183925818652;                 Volume = 0.4597660793662071; VAE = 24.573979259490965\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.9547, -0.3621, -0.6078, -0.2538,  0.1370,  0.2591,  0.0381, -0.7006,\n",
      "        -0.1792, -0.3503]) tensor([ 0.9367, -0.4466, -0.4307, -0.2959,  0.2325,  0.3503,  0.0388, -0.6789,\n",
      "        -0.1883, -0.0415]) tensor([ 0.9645, -0.3445, -0.3477, -0.3934,  0.3196,  0.6224,  0.1352, -0.7363,\n",
      "        -0.4772, -0.0686])\n",
      "R[0]\n",
      "tensor([-0.0095], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05951823277771473; R = 0.011404492499219486;                 Gamma = 0.006622092046120087; Q = 0.00688330748945009;\n",
      "Entropy Neighbor = 0.02867406166344881;                 Entropy Random = 0.004044133298797533;                 Volume = 0.4138811084628105; VAE = 24.929011005401613\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.4912, -1.3551, -0.2370, -0.6134,  0.8358,  0.2185,  0.5681, -1.2179,\n",
      "         0.3171, -0.1246]) tensor([ 1.4547, -0.9050, -0.3364, -0.6533,  0.3986, -0.0040,  0.5375, -1.1931,\n",
      "         0.2719, -0.1158]) tensor([ 1.5037, -1.0364, -0.2661, -1.1081,  0.6322, -0.4297,  0.6146, -1.2913,\n",
      "         0.3400,  0.1442])\n",
      "R[0]\n",
      "tensor([0.0131], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.05909603079408407; R = 0.010976889967889291;                 Gamma = 0.007189388159000373; Q = 0.006617127119097859;\n",
      "Entropy Neighbor = 0.028739917621016502;                 Entropy Random = 0.004075667929137126;                 Volume = 0.4115982359051704; VAE = 25.18727691268921\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6488, -0.2906, -0.5346, -0.1112, -0.0699,  0.0437,  0.0062, -0.5143,\n",
      "        -0.1477, -0.5896]) tensor([ 0.5945, -0.3433, -0.3703, -0.3053,  0.1869,  0.3582,  0.0073, -0.5042,\n",
      "        -0.0862, -0.0538]) tensor([ 0.4480, -0.1708,  0.1325, -0.1134,  0.1363,  0.1753,  0.1504, -0.5162,\n",
      "         0.5431, -0.0316])\n",
      "R[0]\n",
      "tensor([0.0041], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.060177648723125456; R = 0.011587064588355133;                 Gamma = 0.0073374654369763445; Q = 0.00669219739176333;\n",
      "Entropy Neighbor = 0.028934274159371852;                 Entropy Random = 0.004008484926307574;                 Volume = 0.38236746376752856; VAE = 25.472613067626952\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2768, -0.2161, -0.4865, -0.8074, -0.3556,  0.9764,  1.2076, -1.1253,\n",
      "         0.2923,  0.0496]) tensor([ 0.3618, -0.3478, -0.6410, -0.7241,  0.0540,  0.5184,  1.2125, -1.1616,\n",
      "         0.2976, -0.0115]) tensor([-0.0499,  0.0368, -0.4274, -0.9407,  0.1752,  0.3903,  1.0849, -0.9117,\n",
      "         0.2981, -0.1934])\n",
      "R[0]\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06059518551826477; R = 0.01185374337004032;                 Gamma = 0.006434705559964641; Q = 0.006904585927724838;\n",
      "Entropy Neighbor = 0.02901931209862232;                 Entropy Random = 0.0042032046362292024;                 Volume = 0.3900686423778534; VAE = 25.817847633361815\n",
      "Average (on the epoch) training loss: 0.005959501314267982\n",
      "Episode average V value: 0.38744240096360444\n",
      "epoch 6:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 0.2\n",
      "Testing score per episode (id: 0) is 130.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 129.98700129987 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:301: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  ax.scatter(\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 1.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.1335, -0.1494, -0.6911, -0.4957,  0.2605,  0.2681,  1.1427, -1.5386,\n",
      "         0.1149,  0.0680]) tensor([ 1.1380, -0.5528, -0.7574, -0.7880,  0.2662,  0.3450,  1.1295, -1.5075,\n",
      "         0.1610, -0.0225]) tensor([ 1.1511, -0.7489, -0.7893, -0.4589,  0.6625,  0.1569,  1.0143, -1.4248,\n",
      "         0.0048, -0.4520])\n",
      "R[0]\n",
      "tensor([-0.0040], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06303413390368223; R = 0.013452999947534409;                 Gamma = 0.006416240448481403; Q = 0.007785355653148145;\n",
      "Entropy Neighbor = 0.028900488559156657;                 Entropy Random = 0.003968085886212066;                 Volume = 0.38356553491950035; VAE = 26.023782001495363\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.8979, -0.1210, -1.0957, -0.0029,  0.4744,  0.8828,  0.3992, -0.7063,\n",
      "        -0.3921, -0.0868]) tensor([ 0.8969, -0.3132, -0.8016, -0.1678,  0.2746,  0.6875,  0.3539, -0.6701,\n",
      "        -0.4794,  0.1103]) tensor([ 0.8241, -0.3803, -0.7154,  0.0955,  0.5554,  0.5401,  0.1944, -0.5611,\n",
      "        -0.3611, -0.4444])\n",
      "R[0]\n",
      "tensor([-0.0155], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06245171099156141; R = 0.013169722339400324;                 Gamma = 0.005858374368952354; Q = 0.007649951166938991;\n",
      "Entropy Neighbor = 0.029430985186249016;                 Entropy Random = 0.004016787181142718;                 Volume = 0.38201290315389635; VAE = 26.514468437194825\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5820, -0.3632,  0.0373, -0.6608,  0.0335, -0.1227,  0.0555, -0.5497,\n",
      "         0.3882, -0.3553]) tensor([ 0.5933, -0.5092, -0.0382, -0.5064,  0.1645,  0.0175,  0.0953, -0.5872,\n",
      "         0.3586, -0.1372]) tensor([ 0.6124, -0.7953, -0.3799, -0.5890,  0.2520,  0.0476,  0.2126, -0.5757,\n",
      "         0.5377, -0.3451])\n",
      "R[0]\n",
      "tensor([0.0006], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06461181699484586; R = 0.013982220155361574;                 Gamma = 0.006590614795757574; Q = 0.008282350273570046;\n",
      "Entropy Neighbor = 0.029736813362687825;                 Entropy Random = 0.003943961651762948;                 Volume = 0.36618599101901056; VAE = 26.933316783905028\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.8202, -0.7290, -0.4093, -1.1485,  0.2754, -0.0773,  0.6855, -0.9951,\n",
      "         0.5595,  0.0031]) tensor([ 0.7666, -0.5471, -0.2589, -0.7206,  0.2294,  0.0189,  0.6678, -0.9961,\n",
      "         0.5117, -0.1657]) tensor([ 0.7344, -0.4520, -0.2600, -0.8047,  0.3752, -0.0360,  0.7419, -1.0249,\n",
      "         0.7110,  0.0240])\n",
      "R[0]\n",
      "tensor([0.0070], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06638155756145715; R = 0.014274190021795221;                 Gamma = 0.007341027842587209; Q = 0.007816086916718631;\n",
      "Entropy Neighbor = 0.029263582922518253;                 Entropy Random = 0.003961782035185024;                 Volume = 0.3654444263875484; VAE = 27.292231159210203\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.4876, -0.5807, -0.3404, -0.5743,  0.3606,  0.0670,  0.7459, -1.4046,\n",
      "         0.4721, -0.0314]) tensor([ 1.4269, -0.7390, -0.3951, -0.7965,  0.3215, -0.0246,  0.7130, -1.3486,\n",
      "         0.3918, -0.1001]) tensor([ 1.5071, -0.4982, -0.5270, -0.3987,  0.0396,  0.0396,  0.8377, -1.3473,\n",
      "         0.4714,  0.1138])\n",
      "R[0]\n",
      "tensor([0.0009], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06701325515657663; R = 0.014413517056789714;                 Gamma = 0.006439650492276996; Q = 0.00875825981539674;\n",
      "Entropy Neighbor = 0.029726083371788264;                 Entropy Random = 0.0037997009875252842;                 Volume = 0.3667319315671921; VAE = 27.40410915374756\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 1.1448, -0.4764, -0.5894, -0.9130,  0.2398,  0.2580,  0.8543, -1.2550,\n",
      "         0.4309, -0.0843]) tensor([ 1.1011, -0.6338, -0.4767, -0.7236,  0.2976,  0.1510,  0.8290, -1.2025,\n",
      "         0.3294, -0.1215]) tensor([ 1.1640, -0.5959, -0.6946, -1.1549,  0.1987,  0.3771,  0.9265, -1.2706,\n",
      "         0.2749, -0.0768])\n",
      "R[0]\n",
      "tensor([-0.0058], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06845980338007211; R = 0.015379900500294752;                 Gamma = 0.006044534272412421; Q = 0.008206813045195304;\n",
      "Entropy Neighbor = 0.02921950738877058;                 Entropy Random = 0.0037873771958984435;                 Volume = 0.36896111637353896; VAE = 27.748431774139405\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.7985, -0.1881, -0.8889, -0.3583,  0.6868,  0.7561,  0.5669, -0.8636,\n",
      "        -0.3958,  0.5166]) tensor([ 0.8310, -0.3718, -0.8076, -0.2581,  0.3095,  0.7143,  0.5490, -0.8412,\n",
      "        -0.3602,  0.1111]) tensor([ 0.8192, -0.3600, -1.0104,  0.2124,  0.5094,  1.0428,  0.6604, -0.9503,\n",
      "        -0.5670,  0.1545])\n",
      "R[0]\n",
      "tensor([-0.0193], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06765523075312375; R = 0.015422531504766085;                 Gamma = 0.007249024730306701; Q = 0.008952118965797127;\n",
      "Entropy Neighbor = 0.029086967557668687;                 Entropy Random = 0.0038118464718572796;                 Volume = 0.35074013155698774; VAE = 27.820413654327393\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5808, -0.4587, -0.5955, -1.1572,  0.3641,  0.9111,  1.1452, -1.2970,\n",
      "         0.2569, -0.4718]) tensor([ 0.6244, -0.5026, -0.6607, -0.8046,  0.3411,  0.5727,  1.1452, -1.2998,\n",
      "         0.2339, -0.1546]) tensor([ 0.5980, -0.8927, -0.6796, -1.4160,  0.5255,  0.8702,  1.1278, -1.1500,\n",
      "         0.0938, -0.0484])\n",
      "R[0]\n",
      "tensor([-0.0104], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06850024319440126; R = 0.01577787922177231;                 Gamma = 0.00739502883420937; Q = 0.008653581639751792;\n",
      "Entropy Neighbor = 0.029441023640334605;                 Entropy Random = 0.003798104806570336;                 Volume = 0.3446588896214962; VAE = 27.974567459106446\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.7922,  0.0529, -0.4475, -0.1792,  0.4769,  0.4825,  0.3130, -0.6050,\n",
      "        -0.1752,  0.6326]) tensor([ 0.7723, -0.2572, -0.4800, -0.2150,  0.2279,  0.4461,  0.2868, -0.5717,\n",
      "        -0.1521,  0.1071]) tensor([ 0.1085, -0.0611, -0.5059, -0.1013,  0.4102,  0.6510, -0.1299, -0.2850,\n",
      "        -0.4205, -0.0647])\n",
      "R[0]\n",
      "tensor([-0.0017], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06836428094655275; R = 0.01602185744664166;                 Gamma = 0.007292030093667563; Q = 0.009233860304113477;\n",
      "Entropy Neighbor = 0.02922062173858285;                 Entropy Random = 0.0037069960383232683;                 Volume = 0.32804354521632195; VAE = 27.92735779953003\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6207, -0.8090, -0.4909, -0.8544,  0.4830,  0.4471,  0.9117, -1.0948,\n",
      "         0.3683, -0.3129]) tensor([ 0.4800, -0.4868, -0.4669, -0.7055,  0.3073,  0.3472,  0.8472, -0.9976,\n",
      "         0.3757, -0.1745]) tensor([ 0.0886, -0.0711, -0.3683, -0.5419,  0.2686,  0.3052,  0.9192, -0.8802,\n",
      "         0.6209, -0.1778])\n",
      "R[0]\n",
      "tensor([-0.0028], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07137203009426593; R = 0.016814003595674875;                 Gamma = 0.006572501274917158; Q = 0.008877527209464461;\n",
      "Entropy Neighbor = 0.02881863633170724;                 Entropy Random = 0.003726089222240262;                 Volume = 0.3263211780488491; VAE = 28.19206212234497\n",
      "Average (on the epoch) training loss: 0.00842159049900947\n",
      "Episode average V value: 0.54564680544734\n",
      "epoch 7:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 0.2\n",
      "Testing score per episode (id: 0) is 142.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 141.985801419858 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:301: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  ax.scatter(\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6431, -0.5732, -0.9943, -0.6929,  0.1426, -0.0116,  0.9960, -1.0493,\n",
      "        -0.0505, -0.1440]) tensor([ 0.5656, -0.4009, -0.7366, -0.5923,  0.3180,  0.4997,  0.9684, -1.0056,\n",
      "         0.0408, -0.0319]) tensor([ 0.6463, -0.7345, -0.8394, -0.4414,  0.0818,  0.8340,  0.9772, -1.1124,\n",
      "         0.2979, -0.3921])\n",
      "R[0]\n",
      "tensor([-0.0145], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07087242698669434; R = 0.01668417024600785;                 Gamma = 0.006826686209096806; Q = 0.008937346798367798;\n",
      "Entropy Neighbor = 0.029215177085250617;                 Entropy Random = 0.0037984315818175673;                 Volume = 0.3210598703920841; VAE = 28.23548796081543\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3169, -0.3313,  0.0527, -1.1772,  0.3397, -0.0932,  0.5088, -0.9385,\n",
      "         0.8868, -0.0988]) tensor([ 0.3610, -0.5163,  0.0256, -0.8911,  0.2239, -0.1068,  0.5430, -0.9416,\n",
      "         0.7368, -0.2861]) tensor([ 0.3916, -0.5493,  0.3256, -0.7028, -0.0799, -0.1938,  0.6089, -0.9939,\n",
      "         0.8192, -0.6438])\n",
      "R[0]\n",
      "tensor([0.0314], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07034496284276247; R = 0.015854409827210474;                 Gamma = 0.0071288691043664585; Q = 0.00875996304419823;\n",
      "Entropy Neighbor = 0.02891059896722436;                 Entropy Random = 0.003676058829529211;                 Volume = 0.33222820529341696; VAE = 28.49049970245361\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.8117, -0.5883, -0.3087, -0.3603,  0.2028,  0.2440,  0.9733, -1.2403,\n",
      "         0.4477,  0.1429]) tensor([ 0.8522, -0.5632, -0.4417, -0.7545,  0.3199,  0.2258,  0.9997, -1.2436,\n",
      "         0.3648, -0.1194]) tensor([ 0.7615, -0.5433, -0.5503, -0.4112,  0.3465,  0.5044,  0.9840, -1.1947,\n",
      "         0.2654,  0.0245])\n",
      "R[0]\n",
      "tensor([0.0022], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07035851209610701; R = 0.015137358783977106;                 Gamma = 0.006358967438689433; Q = 0.008471756621263921;\n",
      "Entropy Neighbor = 0.028815625220537187;                 Entropy Random = 0.003679303982295096;                 Volume = 0.3265592003464699; VAE = 28.57419341659546\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 7.8453e-01, -2.8717e-01, -7.3288e-01, -4.5394e-01,  5.6169e-01,\n",
      "         9.0605e-02,  1.1504e-01, -5.9119e-01,  2.4312e-04,  3.7821e-02]) tensor([ 0.5800, -0.3068, -0.4324, -0.2735,  0.3392,  0.3223,  0.0357, -0.4803,\n",
      "        -0.0152,  0.0317]) tensor([ 0.8324, -0.6804, -0.7680, -0.4060,  0.2060,  0.5209,  0.2151, -0.6646,\n",
      "         0.0859, -0.0893])\n",
      "R[0]\n",
      "tensor([0.0044], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.0712495386376977; R = 0.01676801660275669;                 Gamma = 0.0065312140559981345; Q = 0.009131341801956296;\n",
      "Entropy Neighbor = 0.02873497035726905;                 Entropy Random = 0.0036220882572233675;                 Volume = 0.32598304185271265; VAE = 28.69007992172241\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4222, -0.3282, -0.8291, -0.3840,  0.4402,  0.5851,  0.9985, -1.0798,\n",
      "         0.0367,  0.3894]) tensor([ 0.3376, -0.2853, -0.6755, -0.5449,  0.2457,  0.6109,  0.9271, -0.9999,\n",
      "         0.0739, -0.0373]) tensor([ 0.4721, -0.2814, -0.7549, -0.5834,  0.3187,  0.7713,  1.0641, -1.0681,\n",
      "         0.2463, -0.4318])\n",
      "R[0]\n",
      "tensor([0.0048], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07164637595415116; R = 0.015693754813197302;                 Gamma = 0.006548486835919903; Q = 0.008748616329859942;\n",
      "Entropy Neighbor = 0.028432943269610406;                 Entropy Random = 0.0035381890680873766;                 Volume = 0.3215957399904728; VAE = 28.652628002166747\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5167,  0.1499, -0.9971, -0.8806,  0.1410,  0.4746,  1.1227, -1.1525,\n",
      "         0.2090, -0.3346]) tensor([ 0.3530, -0.2909, -0.6758, -0.6623,  0.2439,  0.4785,  1.0287, -1.0194,\n",
      "         0.2205, -0.0751]) tensor([ 0.5439, -0.3204, -0.6551, -0.5055,  0.5237,  0.5887,  1.1385, -1.1129,\n",
      "         0.0997, -0.2218])\n",
      "R[0]\n",
      "tensor([4.1604e-05], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07190385205298662; R = 0.015822254308033735;                 Gamma = 0.006401495707948925; Q = 0.008609232129063457;\n",
      "Entropy Neighbor = 0.02898299978300929;                 Entropy Random = 0.003648560998495668;                 Volume = 0.3249548853337765; VAE = 28.856758457183837\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.7561, -0.3595, -0.6137, -0.7454,  0.5611,  0.7351,  0.9974, -1.2510,\n",
      "         0.2253,  0.2534]) tensor([ 0.7631, -0.5008, -0.6478, -0.6285,  0.3587,  0.4478,  0.9797, -1.2296,\n",
      "         0.1241, -0.0548]) tensor([ 0.8591, -0.6193, -0.9226, -0.5619,  0.4928,  0.7059,  0.8750, -1.2066,\n",
      "         0.2718,  0.3968])\n",
      "R[0]\n",
      "tensor([-0.0009], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07134838277101517; R = 0.014620350679033436;                 Gamma = 0.00698351637583255; Q = 0.00876223584660329;\n",
      "Entropy Neighbor = 0.028713442236185074;                 Entropy Random = 0.0037922802738612517;                 Volume = 0.31317919087409973; VAE = 28.72352914047241\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1423,  0.0009,  0.3552, -0.2517, -0.3562, -0.2596, -0.4837,  0.1283,\n",
      "         0.7327, -0.3941]) tensor([-0.0173, -0.1660,  0.3134, -0.2982,  0.0527, -0.1169, -0.3841, -0.0071,\n",
      "         0.5396, -0.1479]) tensor([-0.8551,  0.4078,  0.8395, -0.5306, -0.0860, -0.0267, -0.6718,  0.5242,\n",
      "         0.7816, -0.2337])\n",
      "R[0]\n",
      "tensor([-0.0163], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07196296624839306; R = 0.013727469752571778;                 Gamma = 0.005767819931919803; Q = 0.0083332956971135;\n",
      "Entropy Neighbor = 0.029244517814368008;                 Entropy Random = 0.0036868517575785516;                 Volume = 0.31444117641448976; VAE = 28.78595862579346\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4716, -0.2470, -0.4023, -0.6956,  0.2568,  0.1905, -0.2514, -0.3357,\n",
      "        -0.0493, -0.1174]) tensor([ 0.2768, -0.2180, -0.1684, -0.1919,  0.1818,  0.2212, -0.3507, -0.2318,\n",
      "        -0.0178,  0.0043]) tensor([ 0.2055,  0.0997, -0.4940,  0.3399,  0.1724,  0.3884, -0.4262, -0.1180,\n",
      "        -0.3435, -0.2488])\n",
      "R[0]\n",
      "tensor([0.0065], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07269289556890726; R = 0.013648804939060937;                 Gamma = 0.005565681179476087; Q = 0.00866414252272807;\n",
      "Entropy Neighbor = 0.02890787719190121;                 Entropy Random = 0.003937675868743099;                 Volume = 0.3079897290468216; VAE = 28.776667278289796\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4581, -0.1967,  0.1261, -0.1174, -0.0051,  0.1284, -0.1579, -0.0171,\n",
      "         0.3575,  0.0499]) tensor([-0.2640, -0.1362,  0.0264, -0.2824,  0.0973,  0.2763, -0.0493, -0.1712,\n",
      "         0.2344, -0.1119]) tensor([-0.6946, -0.1838,  0.1925, -0.2787,  0.0172,  0.0189, -0.3633,  0.2904,\n",
      "         0.3548, -0.3349])\n",
      "R[0]\n",
      "tensor([0.0646], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.071939044483006; R = 0.013144414782873355;                 Gamma = 0.006110420651937602; Q = 0.008328041141387075;\n",
      "Entropy Neighbor = 0.029360224578529596;                 Entropy Random = 0.0038821282014250757;                 Volume = 0.30499238550662994; VAE = 28.904302993774415\n",
      "Average (on the epoch) training loss: 0.008674597193254158\n",
      "Episode average V value: 0.6834302735694721\n",
      "epoch 8:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 0.2\n",
      "Testing score per episode (id: 0) is 142.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 141.985801419858 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:301: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  ax.scatter(\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1988, -0.1979, -0.8650, -0.3323,  0.2832,  0.8897, -0.0675, -0.2724,\n",
      "        -0.5900,  0.3168]) tensor([ 9.0588e-02, -1.3278e-01, -5.6395e-01,  4.6578e-04,  1.9521e-01,\n",
      "         7.8648e-01, -1.6027e-01, -1.9417e-01, -5.1537e-01,  1.6347e-01]) tensor([-0.1656, -0.1033,  0.6473, -0.5688, -0.2317, -0.1887, -0.6097,  0.2379,\n",
      "         0.4154, -0.1783])\n",
      "R[0]\n",
      "tensor([-0.0024], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07201548499614001; R = 0.013117261775565566;                 Gamma = 0.006528518078048364; Q = 0.009033582995878532;\n",
      "Entropy Neighbor = 0.029129158757627012;                 Entropy Random = 0.004083574745571241;                 Volume = 0.2867070360481739; VAE = 28.595588989257813\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3461, -0.4493,  0.2226, -0.0428, -0.1728,  0.1856, -0.1697,  0.0894,\n",
      "         0.4631, -0.1356]) tensor([-0.1616, -0.1668,  0.0277, -0.2564,  0.1090,  0.2266, -0.0546, -0.0550,\n",
      "         0.3157, -0.1214]) tensor([-1.7386e-01, -3.6363e-01,  9.9877e-02, -4.8054e-01,  2.0528e-01,\n",
      "         5.1252e-02,  1.7196e-05, -1.6023e-01,  3.6633e-01,  1.6235e-01])\n",
      "R[0]\n",
      "tensor([-0.0627], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07260240712761878; R = 0.013622415614387137;                 Gamma = 0.006781671302604082; Q = 0.009307030418422074;\n",
      "Entropy Neighbor = 0.029327891714870928;                 Entropy Random = 0.003924766374169849;                 Volume = 0.2843035863339901; VAE = 28.61793042755127\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6666, -0.0557, -0.6990, -0.5105,  0.5934,  0.0175,  0.9803, -1.1392,\n",
      "         0.1041,  0.1350]) tensor([ 0.4589, -0.2992, -0.5915, -0.6068,  0.2996,  0.3581,  0.8885, -1.0082,\n",
      "         0.2218, -0.0224]) tensor([ 0.5842, -0.1872, -0.9090, -0.6620,  0.6962,  0.1085,  0.9887, -1.1601,\n",
      "         0.2383, -0.0808])\n",
      "R[0]\n",
      "tensor([-0.0049], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07296723870933056; R = 0.014098044197598938;                 Gamma = 0.006486238435973064; Q = 0.009106017100857572;\n",
      "Entropy Neighbor = 0.029520071994513272;                 Entropy Random = 0.004078791812062264;                 Volume = 0.27378300625085833; VAE = 28.582561721801756\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3332, -0.2604, -0.4507, -0.7933,  0.4242,  0.3103,  1.0166, -1.0762,\n",
      "         0.6196,  0.1895]) tensor([ 0.2243, -0.3621, -0.3683, -0.6995,  0.2574,  0.2286,  0.9884, -1.0046,\n",
      "         0.5548, -0.1687]) tensor([ 0.2602, -0.2449, -0.4871, -0.8566, -0.1670,  0.4619,  0.9326, -1.0028,\n",
      "         0.4593, -0.1494])\n",
      "R[0]\n",
      "tensor([-0.0032], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07260448345541955; R = 0.013409401063196128;                 Gamma = 0.006424414696011809; Q = 0.008748751510865986;\n",
      "Entropy Neighbor = 0.029986497178673743;                 Entropy Random = 0.004083604922168888;                 Volume = 0.2641364256739616; VAE = 28.58084884262085\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3403, -0.8396, -0.7063, -0.5939,  0.1819,  0.5445,  0.2619, -0.5923,\n",
      "        -0.3187, -0.2094]) tensor([ 0.1673, -0.3626, -0.5876, -0.3166,  0.2766,  0.6010,  0.2046, -0.5057,\n",
      "        -0.2352, -0.0248]) tensor([ 0.3969, -0.5604, -0.5193, -0.2660,  0.6032,  0.3544,  0.2503, -0.5768,\n",
      "        -0.1438,  0.1093])\n",
      "R[0]\n",
      "tensor([-0.0033], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07288922272622585; R = 0.014723441052949055;                 Gamma = 0.0058266340367190425; Q = 0.009456862593535334;\n",
      "Entropy Neighbor = 0.0300826406031847;                 Entropy Random = 0.0043265518865082415;                 Volume = 0.24779030764102936; VAE = 28.381550945281983\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4218, -0.5805, -0.6214, -0.2139, -0.2016,  0.6840,  0.2232, -0.5932,\n",
      "        -0.0701,  0.2406]) tensor([ 0.2924, -0.2752, -0.4474, -0.2985,  0.2232,  0.5270,  0.1809, -0.5223,\n",
      "        -0.0456,  0.0325]) tensor([ 0.4962, -0.1403, -0.5856, -0.2743,  0.0977,  0.2706,  0.3193, -0.5663,\n",
      "        -0.0767,  0.1306])\n",
      "R[0]\n",
      "tensor([0.0007], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07319437649101018; R = 0.013315530095424037;                 Gamma = 0.0055485296652987015; Q = 0.008384581050137058;\n",
      "Entropy Neighbor = 0.029425806209445;                 Entropy Random = 0.004327754028839991;                 Volume = 0.24535165801644326; VAE = 28.423119056701662\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2689,  0.2555,  0.6320, -0.4821, -0.1693, -0.3340, -0.7051,  0.2615,\n",
      "         0.5880, -0.3513]) tensor([-0.2009, -0.1186,  0.4858, -0.2554,  0.0160, -0.2552, -0.6495,  0.2202,\n",
      "         0.4644, -0.1850]) tensor([-0.0440,  0.0757,  0.2133, -0.0337,  0.0938, -0.1381, -0.4332,  0.0385,\n",
      "         0.3147, -0.1633])\n",
      "R[0]\n",
      "tensor([-0.1053], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07336522456258536; R = 0.013605110581091139;                 Gamma = 0.0053995828589613664; Q = 0.009145893118344248;\n",
      "Entropy Neighbor = 0.02979918684437871;                 Entropy Random = 0.004422970634419471;                 Volume = 0.22928910622000695; VAE = 28.17844338989258\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3617, -0.0699, -0.7256, -0.1955,  0.7735,  0.7567,  0.5117, -0.7350,\n",
      "        -0.3647,  0.2309]) tensor([ 0.2475, -0.2402, -0.6699, -0.2852,  0.2640,  0.6694,  0.4297, -0.6340,\n",
      "        -0.2694,  0.0813]) tensor([ 0.2078, -0.2562, -0.5138,  0.1847,  0.3350,  0.9531,  0.4143, -0.5922,\n",
      "        -0.6817,  0.1554])\n",
      "R[0]\n",
      "tensor([0.0009], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07373505184799432; R = 0.013628763662301936;                 Gamma = 0.006427855343419651; Q = 0.009511255576042458;\n",
      "Entropy Neighbor = 0.029407157495617866;                 Entropy Random = 0.004514700348721817;                 Volume = 0.22537111611664296; VAE = 28.285092082977293\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4415, -0.3892, -0.4944, -0.9929, -0.2737,  0.8612,  1.0409, -0.8468,\n",
      "         0.0942, -0.1658]) tensor([-0.5282, -0.2278, -0.5552, -0.5923,  0.1986,  0.7053,  0.9966, -0.7953,\n",
      "         0.1411, -0.1636]) tensor([-0.6943,  0.2270, -0.5685, -0.2078, -0.0958,  1.0978,  0.8251, -0.5857,\n",
      "        -0.0744,  0.0830])\n",
      "R[0]\n",
      "tensor([-0.0108], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.0732563905119896; R = 0.013666471647215076;                 Gamma = 0.005582869767880766; Q = 0.009149014309514314;\n",
      "Entropy Neighbor = 0.030142809350043535;                 Entropy Random = 0.004354450363432989;                 Volume = 0.21554773518443107; VAE = 28.095329330444336\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2827, -0.4428,  0.0238, -0.4376, -0.1730,  0.3019,  0.0132, -0.1255,\n",
      "         0.6236, -0.3198]) tensor([-0.1871, -0.1990, -0.0141, -0.3670,  0.1497,  0.1530,  0.0677, -0.2057,\n",
      "         0.4254, -0.1990]) tensor([-0.5360, -0.1730,  0.2567,  0.1842,  0.3928,  0.5026, -0.0370,  0.0646,\n",
      "         0.1999, -0.0684])\n",
      "R[0]\n",
      "tensor([0.0264], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07390272237360478; R = 0.01348000451902044;                 Gamma = 0.0055861445867631115; Q = 0.009179982119472697;\n",
      "Entropy Neighbor = 0.02962096006050706;                 Entropy Random = 0.004575707965064794;                 Volume = 0.21279190428555012; VAE = 28.081723041534424\n",
      "Average (on the epoch) training loss: 0.009102297079307027\n",
      "Episode average V value: 0.8070659877955914\n",
      "epoch 9:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score per episode (id: 0) is 142.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 141.985801419858 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:301: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  ax.scatter(\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6412, -0.2374, -0.6108, -0.5812,  1.0468, -0.0178,  0.9238, -1.0989,\n",
      "         0.1077, -0.1351]) tensor([ 0.5065, -0.3522, -0.6920, -0.6156,  0.5066,  0.3536,  0.9003, -1.0453,\n",
      "         0.1180, -0.0942]) tensor([ 0.6246, -0.6303, -0.1820, -1.0062,  0.5504,  0.3437,  0.9195, -1.0773,\n",
      "         0.1464,  0.3611])\n",
      "R[0]\n",
      "tensor([0.0245], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07043951278179884; R = 0.012693849506904371;                 Gamma = 0.0057332254391876634; Q = 0.00920137558504939;\n",
      "Entropy Neighbor = 0.030767518252134324;                 Entropy Random = 0.00447754874243401;                 Volume = 0.204677266061306; VAE = 28.087453285217286\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.9276,  0.0393, -0.7770, -0.1600,  0.2643,  1.3971,  0.5205, -0.3234,\n",
      "        -0.4833, -0.1506]) tensor([-0.5941, -0.1035, -0.8675, -0.1677,  0.2015,  1.2311,  0.6453, -0.5174,\n",
      "        -0.4446,  0.1511]) tensor([-1.2004, -0.1195, -0.6035, -0.0217, -0.4102,  1.3559,  0.3705, -0.1771,\n",
      "        -0.4729,  0.1460])\n",
      "R[0]\n",
      "tensor([0.1413], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07330246125161648; R = 0.013567793888098095;                 Gamma = 0.005816382831297233; Q = 0.009056040071183815;\n",
      "Entropy Neighbor = 0.03009915377944708;                 Entropy Random = 0.0041078932378441095;                 Volume = 0.21148993115127088; VAE = 28.226127014160156\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5069, -0.2314, -0.6909, -0.6266,  0.2082,  0.4116,  0.7443, -0.9351,\n",
      "         0.1705, -0.5769]) tensor([ 0.3476, -0.3394, -0.5625, -0.5442,  0.2760,  0.3618,  0.6848, -0.8442,\n",
      "         0.2019, -0.1143]) tensor([ 0.5169,  0.0212, -0.5784, -0.8241,  0.7097,  0.1280,  0.7989, -0.9722,\n",
      "         0.1794,  0.1112])\n",
      "R[0]\n",
      "tensor([-0.0147], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.0718355206772685; R = 0.013576443629164714;                 Gamma = 0.006573238926241174; Q = 0.009812477387720718;\n",
      "Entropy Neighbor = 0.030329072907567024;                 Entropy Random = 0.004266582454089075;                 Volume = 0.2082557387948036; VAE = 28.17550431060791\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7801,  0.1845,  0.8816, -0.1408, -0.0015, -0.4059, -0.5388,  0.3355,\n",
      "         0.8421, -0.2006]) tensor([-0.5867, -0.0498,  0.5542, -0.3763,  0.0256, -0.2302, -0.3855,  0.1503,\n",
      "         0.6199, -0.2319]) tensor([-1.1589, -0.0273,  0.6912,  0.2103, -0.2019, -0.2652, -0.5361,  0.4797,\n",
      "         0.6878, -0.1562])\n",
      "R[0]\n",
      "tensor([0.0313], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.07099133221060037; R = 0.013772883739147802;                 Gamma = 0.0053380245483713226; Q = 0.009145190591225401;\n",
      "Entropy Neighbor = 0.031059105660766362;                 Entropy Random = 0.004413107172003947;                 Volume = 0.205289271697402; VAE = 28.252975288391113\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5393, -0.8685, -0.4140, -0.2786,  0.3269,  0.5087,  0.8155, -0.9899,\n",
      "         0.4302,  0.3776]) tensor([ 0.5324, -0.4822, -0.4833, -0.5856,  0.4201,  0.2798,  0.8304, -0.9921,\n",
      "         0.3107, -0.1140]) tensor([ 0.5612, -0.5501, -0.3631, -1.0275,  0.5315,  0.2823,  0.7354, -1.0618,\n",
      "         0.2282, -0.0598])\n",
      "R[0]\n",
      "tensor([0.0107], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.0702154669240117; R = 0.014726672221906482;                 Gamma = 0.005357589847393683; Q = 0.009767361868871376;\n",
      "Entropy Neighbor = 0.03114342451840639;                 Entropy Random = 0.004433859343640506;                 Volume = 0.20712419824302197; VAE = 28.309043285369874\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 1.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-1.3645,  0.2736,  0.8305,  0.1612, -0.1068, -0.0251, -0.7212,  0.6969,\n",
      "         0.5387, -0.0530]) tensor([-0.6794,  0.0827,  0.1375, -0.0810,  0.0288,  0.3468, -0.2277,  0.0455,\n",
      "         0.3985,  0.0527]) tensor([ 0.2393, -0.3746, -0.8726, -1.2355,  0.2950,  0.5226,  1.0878, -1.0287,\n",
      "        -0.2026, -0.0526])\n",
      "R[0]\n",
      "tensor([0.8235], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06979078164696693; R = 0.014248976691509598;                 Gamma = 0.005754783089127159; Q = 0.009487103800289333;\n",
      "Entropy Neighbor = 0.03152650286257267;                 Entropy Random = 0.004124642339069396;                 Volume = 0.2094782783091068; VAE = 28.51725493621826\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3563, -0.2601,  0.3307, -0.3713,  0.2664, -0.0078, -0.0702, -0.0919,\n",
      "         0.5378, -0.0401]) tensor([-0.2833, -0.1654,  0.1525, -0.3882,  0.1573,  0.0367, -0.0132, -0.1492,\n",
      "         0.3689, -0.2132]) tensor([-0.4936, -0.2970,  0.3312, -0.1860,  0.5461,  0.1152, -0.1760,  0.0536,\n",
      "         0.4719, -0.3475])\n",
      "R[0]\n",
      "tensor([0.0541], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06854088456183673; R = 0.013295694661908783;                 Gamma = 0.005342235035423073; Q = 0.00959648553119041;\n",
      "Entropy Neighbor = 0.03161732993274927;                 Entropy Random = 0.004422702929703519;                 Volume = 0.20862546390295028; VAE = 28.659635860443114\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5235, -0.6274, -0.2899, -0.7485,  0.7777,  0.2340,  0.8379, -0.9986,\n",
      "         0.2145,  0.0158]) tensor([ 0.4678, -0.5083, -0.5492, -0.6431,  0.4643,  0.3611,  0.8703, -0.9918,\n",
      "         0.2203, -0.1546]) tensor([ 0.5267, -0.4794, -0.3272, -0.7417,  0.2896,  0.0834,  0.7857, -0.9780,\n",
      "         0.2867, -0.3244])\n",
      "R[0]\n",
      "tensor([-0.0018], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06862967711687087; R = 0.013786829797609243;                 Gamma = 0.005771291876444593; Q = 0.009558870861539618;\n",
      "Entropy Neighbor = 0.03205944455415011;                 Entropy Random = 0.004247850857209415;                 Volume = 0.20461414262652397; VAE = 28.49502898788452\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1374, -0.5555, -0.5721, -0.8236,  0.5768,  0.5391,  1.0844, -1.0061,\n",
      "         0.4659, -0.6895]) tensor([-0.1902, -0.4481, -0.4763, -0.7522,  0.3531,  0.4606,  1.0695, -0.9613,\n",
      "         0.4450, -0.3922]) tensor([-0.3918, -0.0889, -0.3556, -1.2032,  0.3596,  0.4556,  1.0972, -0.9089,\n",
      "         0.4782, -0.0565])\n",
      "R[0]\n",
      "tensor([-0.0029], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06742122191935777; R = 0.014173778629861772;                 Gamma = 0.0057400166863953925; Q = 0.009438522785203532;\n",
      "Entropy Neighbor = 0.03253534277528524;                 Entropy Random = 0.0044675053047249095;                 Volume = 0.20910041935741902; VAE = 28.699970573425293\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2449, -0.1239, -0.4014, -0.8037,  0.7192,  0.3549,  1.0190, -0.9599,\n",
      "         0.4430, -0.2871]) tensor([-0.3087, -0.3038, -0.3898, -0.7185,  0.3008,  0.3733,  1.0011, -0.9147,\n",
      "         0.4456, -0.2855]) tensor([-0.4509, -0.4433, -0.5832, -0.9714,  0.4693,  0.4103,  1.0021, -0.8046,\n",
      "         0.5171, -0.0075])\n",
      "R[0]\n",
      "tensor([0.0018], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06749786168336869; R = 0.015065125546592754;                 Gamma = 0.005549198489716218; Q = 0.009725968050770462;\n",
      "Entropy Neighbor = 0.03220769121870398;                 Entropy Random = 0.004480119952466339;                 Volume = 0.21582449416816235; VAE = 28.883775913238527\n",
      "Average (on the epoch) training loss: 0.009478939653304406\n",
      "Episode average V value: 0.8859745606490912\n",
      "epoch 10:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 0.2\n",
      "Testing score per episode (id: 0) is 142.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 141.985801419858 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:301: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n",
      "  ax.scatter(\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:333: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 1.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-1.3791,  0.2224,  1.0298,  0.2643, -0.1397, -0.1387, -0.8451,  0.8435,\n",
      "         0.6960, -0.0820]) tensor([-0.4569,  0.0057,  0.0988, -0.0783,  0.1059,  0.2838, -0.1656, -0.0409,\n",
      "         0.4625,  0.0502]) tensor([ 0.2162, -0.2376, -0.9401, -0.7658,  0.3711,  0.4971,  1.0527, -1.0136,\n",
      "        -0.0428,  0.2527])\n",
      "R[0]\n",
      "tensor([0.8819], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06779227401316167; R = 0.015141792837472167;                 Gamma = 0.0058837916517077244; Q = 0.010271765822079032;\n",
      "Entropy Neighbor = 0.032048573043197394;                 Entropy Random = 0.004483509021927602;                 Volume = 0.2181043034940958; VAE = 28.929636981964112\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4459, -0.4440, -0.6618, -0.7296, -0.1697,  0.7260,  0.9813, -0.8046,\n",
      "        -0.0307, -0.1789]) tensor([-0.5285, -0.2291, -0.6569, -0.5257,  0.2755,  0.7614,  0.9310, -0.7268,\n",
      "         0.0132, -0.1839]) tensor([-0.4173, -0.2953, -0.6008, -0.4711,  0.6313,  0.5219,  1.0598, -0.8009,\n",
      "         0.3453, -0.3799])\n",
      "R[0]\n",
      "tensor([-0.0106], grad_fn=<SelectBackward0>)\n",
      "LOSSES\n",
      "T = 0.06720255467295647; R = 0.014403939984389581;                 Gamma = 0.0057928456101653866; Q = 0.010098759327549488;\n",
      "Entropy Neighbor = 0.03316862442344427;                 Entropy Random = 0.004613911043736153;                 Volume = 0.2182750559002161; VAE = 29.039755687713622\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 1.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-1.7017,  0.0456, -0.5196, -0.0310,  0.1084,  1.2868,  0.1365,  0.0187,\n",
      "        -0.4245, -0.3177]) tensor([-0.9698, -0.0431, -0.8605, -0.1069,  0.1669,  1.3400,  0.5671, -0.5517,\n",
      "        -0.3725,  0.1887]) tensor([-0.0506, -0.4628, -0.3875, -0.6181,  0.3663,  0.4324,  0.9728, -1.0331,\n",
      "         0.4420, -0.0401])\n",
      "R[0]\n",
      "tensor([1.0361], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"params\")\n",
    "except Exception:\n",
    "    pass\n",
    "dump(vars(parameters), \"params/\" + fname + \".jldump\")\n",
    "#agent.gathering_data=False\n",
    "agent.run(parameters.epochs, parameters.steps_per_epoch)\n",
    "\n",
    "# --- Show results ---\n",
    "basename = \"scores/\" + fname\n",
    "scores = load(basename + \"_scores.jldump\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = learning_algo.get_losses()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.setNetwork(fname, nEpoch=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._in_episode = True\n",
    "agent._mode = 0 # Testing mode with plan_depth=0\n",
    "initState = env.reset(agent._mode)\n",
    "inputDims = env.inputDimensions()\n",
    "\n",
    "for i in range(len(inputDims)):\n",
    "    if inputDims[i][0] > 1:\n",
    "        agent._state[i][1:] = initState[i][1:]\n",
    "agent._Vs_on_last_episode = []\n",
    "is_terminal = False\n",
    "reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for i in range(100):\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    plt.figure()\n",
    "    plt.imshow(np.flip(_obs.squeeze()))\n",
    "    plt.show()\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "    V, action, reward, _ = agent._step()\n",
    "    print(action)\n",
    "    agent._Vs_on_last_episode.append(V)\n",
    "    is_terminal = env.inTerminalState()\n",
    "    if is_terminal: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "obs = env.observe()\n",
    "_obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "_obs = np.flip(_obs.squeeze())\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "im = ax.imshow(np.zeros(_obs.shape))\n",
    "\n",
    "def init():\n",
    "    plt.cla()\n",
    "    im = ax.imshow(_obs)\n",
    "    return [im]\n",
    "\n",
    "def animate(i, *args, **kwargs):\n",
    "    plt.cla()\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    _obs = np.flip(_obs.squeeze())\n",
    "    im = ax.imshow(_obs)\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "        V, action, reward, _ = agent._step()\n",
    "        agent._Vs_on_last_episode.append(V)\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init, \n",
    "     frames=100, blit=False, repeat=True)\n",
    "ani.save('behavior.gif', writer=\"ffmpeg\", fps = 15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
