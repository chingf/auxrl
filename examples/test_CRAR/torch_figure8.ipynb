{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import hash, dump, load\n",
    "import os\n",
    "\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.CRAR_torch import CRAR\n",
    "from figure8_env import MyEnv as figure8_env\n",
    "import deer.experiment.base_controllers as bc\n",
    "\n",
    "from deer.policies import EpsilonGreedyPolicy, FixedFigure8Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure8_give_rewards = True\n",
    "nn_yaml = 'network_noconv.yaml'\n",
    "higher_dim_obs = False\n",
    "internal_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    steps_per_epoch = 5000\n",
    "    epochs = 50\n",
    "    steps_per_test = 1000\n",
    "    period_btw_summary_perfs = 1\n",
    "\n",
    "    # ----------------------\n",
    "    # Temporal Processing Parameters\n",
    "    # ----------------------\n",
    "    nstep = 15\n",
    "    recurrent = False\n",
    "    \n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    frame_skip = 2\n",
    "    show_rewards = False\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    update_rule = 'rmsprop'\n",
    "    learning_rate = 1 * 1E-4 # 1E-4\n",
    "    learning_rate_decay = 0.9\n",
    "    discount = 0.9\n",
    "    discount_inc = 1\n",
    "    discount_max = 0.99\n",
    "    rms_decay = 0.9\n",
    "    rms_epsilon = 0.0001\n",
    "    momentum = 0\n",
    "    clip_norm = 1.0\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 1.0\n",
    "    epsilon_decay = 10000\n",
    "    update_frequency = 1\n",
    "    replay_memory_size = 50000 #1000000 #replacing with 200000 will works just fine (in case you dont have 18gb of memory)\n",
    "    batch_size = 32\n",
    "    freeze_interval = 1000\n",
    "    deterministic = False\n",
    "    \n",
    "    # ----------------------\n",
    "    # Learning algo parameters\n",
    "    # ----------------------\n",
    "    loss_weights = [5E-3, 1E-3, 5E-3, 5E-3, 5E-3, 5E-3, 1.] #[0, 0, 0, 0, 0, 0, 1.] #[1E-3, 1E-3, 1E-3, 1E-3, 1E-3, 1E-3, 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters hash is: 62977be8e45d8a56a5537c11dfd5d2fd8dda69e0\n",
      "The parameters are: <__main__.Defaults object at 0x127b732e0>\n",
      "end gathering data\n"
     ]
    }
   ],
   "source": [
    "parameters = Defaults()\n",
    "if parameters.deterministic:\n",
    "    rng = np.random.RandomState(123456)\n",
    "else:\n",
    "    rng = np.random.RandomState()\n",
    "\n",
    "# --- Instantiate environment ---\n",
    "env = figure8_env(\n",
    "    give_rewards=figure8_give_rewards,\n",
    "    intern_dim=internal_dim,\n",
    "    higher_dim_obs=higher_dim_obs,\n",
    "    show_rewards=parameters.show_rewards,\n",
    "    nstep=parameters.nstep\n",
    "    )\n",
    "\n",
    "# --- Instantiate learning_algo ---\n",
    "learning_algo = CRAR(\n",
    "    env,\n",
    "    parameters.rms_decay,\n",
    "    parameters.rms_epsilon,\n",
    "    parameters.momentum,\n",
    "    parameters.clip_norm,\n",
    "    parameters.freeze_interval,\n",
    "    parameters.batch_size,\n",
    "    parameters.update_rule,\n",
    "    rng,\n",
    "    high_int_dim=False,\n",
    "    internal_dim=internal_dim, lr=parameters.learning_rate,\n",
    "    nn_yaml=nn_yaml, double_Q=True,\n",
    "    loss_weights=parameters.loss_weights,\n",
    "    nstep=parameters.nstep,\n",
    "    recurrent=parameters.recurrent\n",
    "    )\n",
    "\n",
    "if figure8_give_rewards:\n",
    "    train_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.2,\n",
    "        consider_valid_transitions=False\n",
    "        )\n",
    "    test_policy = EpsilonGreedyPolicy(\n",
    "        learning_algo, env.nActions(), rng, 0.\n",
    "        )\n",
    "else:\n",
    "    train_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng, epsilon=0.2,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "    test_policy = FixedFigure8Policy.FixedFigure8Policy(\n",
    "        learning_algo, env.nActions(), rng,\n",
    "        height=env.HEIGHT, width=env.WIDTH\n",
    "        )\n",
    "\n",
    "# --- Instantiate agent ---\n",
    "agent = NeuralAgent(\n",
    "    env,\n",
    "    learning_algo,\n",
    "    parameters.replay_memory_size,\n",
    "    1,\n",
    "    parameters.batch_size,\n",
    "    rng,\n",
    "    train_policy=train_policy,\n",
    "    test_policy=test_policy)\n",
    "\n",
    "# --- Create unique filename for FindBestController ---\n",
    "h = hash(vars(parameters), hash_name=\"sha1\")\n",
    "fname = \"test_\" + h\n",
    "print(\"The parameters hash is: {}\".format(h))\n",
    "print(\"The parameters are: {}\".format(parameters))\n",
    "\n",
    "# As for the discount factor and the learning rate, one can update periodically the parameter of the epsilon-greedy\n",
    "# policy implemented by the agent. This controllers has a bit more capabilities, as it allows one to choose more\n",
    "# precisely when to update epsilon: after every X action, episode or epoch. This parameter can also be reset every\n",
    "# episode or epoch (or never, hence the resetEvery='none').\n",
    "agent.attach(bc.EpsilonController(\n",
    "    initial_e=parameters.epsilon_start,\n",
    "    e_decays=parameters.epsilon_decay,\n",
    "    e_min=parameters.epsilon_min,\n",
    "    evaluate_on='episode',\n",
    "    periodicity=1,\n",
    "    reset_every='none'))\n",
    "\n",
    "agent.run(10, 500)\n",
    "print(\"end gathering data\")\n",
    "\n",
    "# --- Bind controllers to the agent ---\n",
    "# Before every training epoch (periodicity=1), we want to print a summary of the agent's epsilon, discount and \n",
    "# learning rate as well as the training epoch number.\n",
    "agent.attach(bc.VerboseController(\n",
    "    evaluate_on='epoch', \n",
    "    periodicity=1))\n",
    "\n",
    "# Every epoch end, one has the possibility to modify the learning rate using a LearningRateController. Here we \n",
    "# wish to update the learning rate after every training epoch (periodicity=1), according to the parameters given.\n",
    "agent.attach(bc.LearningRateController(\n",
    "    initial_learning_rate=parameters.learning_rate, \n",
    "    learning_rate_decay=parameters.learning_rate_decay,\n",
    "    periodicity=1))\n",
    "\n",
    "# Same for the discount factor.\n",
    "agent.attach(bc.DiscountFactorController(\n",
    "    initial_discount_factor=parameters.discount, \n",
    "    discount_factor_growth=parameters.discount_inc, \n",
    "    discount_factor_max=parameters.discount_max,\n",
    "    periodicity=1))\n",
    "\n",
    "# During training epochs, we want to train the agent after every [parameters.update_frequency] action it takes.\n",
    "# Plus, we also want to display after each training episode (!= than after every training) the average bellman\n",
    "# residual and the average of the V values obtained during the last episode, hence the two last arguments.\n",
    "agent.attach(bc.TrainerController(\n",
    "    evaluate_on='action', \n",
    "    periodicity=parameters.update_frequency, \n",
    "    show_episode_avg_V_value=True, \n",
    "    show_avg_Bellman_residual=True))\n",
    "\n",
    "# We wish to discover, among all versions of our neural network (i.e., after every training epoch), which one \n",
    "# has the highest validation score.\n",
    "# To achieve this goal, one can use the FindBestController along with an InterleavedTestEpochControllers. It is \n",
    "# important that the validationID is the same than the id argument of the InterleavedTestEpochController.\n",
    "# The FindBestController will dump on disk the validation scores for each and every network, as well as the \n",
    "# structure of the neural network having the best validation score. These dumps can then used to plot the evolution \n",
    "# of the validation and test scores (see below) or simply recover the resulting neural network for your \n",
    "# application.\n",
    "agent.attach(bc.FindBestController(\n",
    "    validationID=figure8_env.VALIDATION_MODE,\n",
    "    testID=None,\n",
    "    unique_fname=fname))\n",
    "\n",
    "# All previous controllers control the agent during the epochs it goes through. However, we want to interleave a \n",
    "# \"validation epoch\" between each training epoch. For each validation epoch, we want also to display the sum of all \n",
    "# rewards obtained, hence the showScore=True. Finally, we want to call the summarizePerformance method of ALE_env \n",
    "# every [parameters.period_btw_summary_perfs] *validation* epochs.\n",
    "agent.attach(bc.InterleavedTestEpochController(\n",
    "    id=figure8_env.VALIDATION_MODE, \n",
    "    epoch_length=parameters.steps_per_test,\n",
    "    periodicity=1,\n",
    "    show_score=True,\n",
    "    summarize_every=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2475, -0.0807,  0.0903, -0.1572, -0.1948,  0.2499, -0.0362,  0.1996,\n",
      "        -0.1272, -0.0909]) tensor([ 0.2406,  0.0717,  0.2921, -0.1544, -0.3494,  0.5412, -0.2794,  0.3914,\n",
      "        -0.1751, -0.2439]) tensor([ 0.2336, -0.1109,  0.0662, -0.1431, -0.2142,  0.2458, -0.0254,  0.1770,\n",
      "        -0.1266, -0.0876])\n",
      "R[0]\n",
      "tensor([-0.1373], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.010113606852479279 0.006552207931585144 0.16744829382840545 0.01458500341582112 0.5067190005779266 0.0 0.08160869422927498\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2221, -0.1266, -0.0896, -0.2552, -0.6228,  0.1906, -0.1427,  0.1059,\n",
      "        -0.2430,  0.2194]) tensor([-0.2231, -0.1202, -0.0948, -0.2619, -0.6219,  0.1838, -0.1323,  0.0877,\n",
      "        -0.2391,  0.2046]) tensor([-0.3146, -0.1882, -0.1050, -0.2602, -0.7029,  0.0899,  0.0074,  0.0872,\n",
      "        -0.1071,  0.1557])\n",
      "R[0]\n",
      "tensor([0.0096], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.004719325388316065 0.005642420461408619 0.02173913764685858 0.012893575314432382 0.45532207334041597 0.0 0.04765357881225646\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3971,  0.5213,  0.7730, -0.4666,  0.1656,  0.2353,  0.0239,  0.1071,\n",
      "         0.3486, -0.3326]) tensor([ 0.3961,  0.5079,  0.7635, -0.4709,  0.1690,  0.2321,  0.0252,  0.1056,\n",
      "         0.3431, -0.3204]) tensor([ 0.4352,  0.5714,  0.8317, -0.4841,  0.2208,  0.2319,  0.0477,  0.1083,\n",
      "         0.4000, -0.3886])\n",
      "R[0]\n",
      "tensor([0.0085], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.00893712777364999 0.004809987015189109 0.01858618161274353 0.004903171565878437 0.35407414358854294 4.575744271278381e-06 0.024906829570885747\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2159, -0.1733,  0.2655, -0.2932, -0.7584, -0.4965,  0.1872,  0.6633,\n",
      "         0.2039, -0.0670]) tensor([-0.2165, -0.1841,  0.2552, -0.2891, -0.7516, -0.4628,  0.1773,  0.6536,\n",
      "         0.1933, -0.0607]) tensor([-0.1878, -0.0909,  0.3284, -0.3289, -0.6800, -0.4581,  0.2156,  0.6180,\n",
      "         0.2747, -0.1537])\n",
      "R[0]\n",
      "tensor([0.0011], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.009925658852793276 0.006417311360411986 0.01728775212576147 0.00577145037628361 0.32200604712963105 7.183989137411117e-05 0.02298785003926605\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-3.4536e-01, -3.9576e-02,  3.7190e-01, -4.2350e-01, -7.9132e-01,\n",
      "        -4.8554e-01,  2.0810e-01,  6.3384e-01,  2.6344e-01,  5.8830e-05]) tensor([-0.3155,  0.0180,  0.3958, -0.4321, -0.7573, -0.4461,  0.2227,  0.6017,\n",
      "         0.2929, -0.0416]) tensor([-0.3057,  0.0128,  0.4031, -0.4410, -0.7284, -0.4600,  0.2392,  0.6068,\n",
      "         0.3307, -0.1127])\n",
      "R[0]\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.010646485269069672 0.005517224847440957 0.017050587213714608 0.004536241655121558 0.3242333556413651 9.289471805095673e-05 0.025696958384476602\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1146,  0.3367,  0.6969, -0.6228, -0.4443, -0.2484,  0.2804,  0.5083,\n",
      "         0.5529, -0.3752]) tensor([-0.1612,  0.2925,  0.6781, -0.6195, -0.4948, -0.2536,  0.2723,  0.5222,\n",
      "         0.5177, -0.3213]) tensor([-0.2790,  0.1905,  0.6066, -0.5790, -0.6566, -0.3634,  0.2315,  0.5749,\n",
      "         0.4357, -0.1102])\n",
      "R[0]\n",
      "tensor([-0.0239], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.012941347070038318 0.004598552016301255 0.015922377230250275 0.003497918836583267 0.3046500509381294 0.00022049077600240708 0.02195630018832162\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0421,  0.1209,  0.4159, -0.4223, -0.3427, -0.1352,  0.4217,  0.3436,\n",
      "         0.4622, -0.5897]) tensor([-0.0830,  0.0748,  0.4047, -0.4101, -0.3998, -0.1460,  0.4067,  0.3708,\n",
      "         0.4247, -0.5338]) tensor([ 0.0850,  0.2542,  0.4062, -0.4403, -0.0854,  0.1389,  0.4219,  0.0585,\n",
      "         0.4984, -0.7820])\n",
      "R[0]\n",
      "tensor([-0.0049], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.014573996717110276 0.004563085370144109 0.015593870565469842 0.0033052686522714795 0.28827294427156447 0.00048672379553318023 0.022437378224683925\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5740,  0.8214,  0.9773, -0.7377,  0.3600,  0.6001, -0.0194, -0.1147,\n",
      "         0.5495, -0.3931]) tensor([ 0.5118,  0.7595,  0.9378, -0.7185,  0.2720,  0.5427, -0.0183, -0.0601,\n",
      "         0.5164, -0.3275]) tensor([ 0.3633,  0.6086,  0.9761, -0.7180,  0.0125,  0.2837,  0.0752,  0.2026,\n",
      "         0.5772, -0.2890])\n",
      "R[0]\n",
      "tensor([-0.0095], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.016011961466632783 0.005551464657670294 0.013686514484215878 0.0032659959899028765 0.2734865827858448 0.00048738881200551985 0.02097529976000078\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0170,  0.2605,  0.8308, -0.6063, -0.4566, -0.1550,  0.2234,  0.5288,\n",
      "         0.5657, -0.0894]) tensor([ 0.0253,  0.2634,  0.8209, -0.5957, -0.4348, -0.1361,  0.2206,  0.5043,\n",
      "         0.5551, -0.1065]) tensor([-0.1776,  0.1092,  0.6776, -0.5509, -0.6404, -0.3160,  0.3119,  0.5874,\n",
      "         0.5411, -0.0674])\n",
      "R[0]\n",
      "tensor([-0.0036], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.017152329549193383 0.005618312910890382 0.01200006495845446 0.003396374189236667 0.2629644624888897 0.00040274046361446383 0.018011017736280336\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4830, -0.1243,  0.4370, -0.4499, -0.8794, -0.4469,  0.2192,  0.6540,\n",
      "         0.1401,  0.4480]) tensor([-0.4262, -0.0784,  0.4777, -0.4810, -0.8040, -0.3849,  0.2146,  0.5996,\n",
      "         0.1604,  0.4159]) tensor([-0.5168, -0.1208,  0.4517, -0.4718, -0.9013, -0.4962,  0.3040,  0.6429,\n",
      "         0.2489,  0.3503])\n",
      "R[0]\n",
      "tensor([-0.0020], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.017656554443761706 0.005006631834366999 0.01279482565478247 0.0034659500454145018 0.25039485657215116 0.00029980672150850296 0.016914121892652473\n",
      "Average (on the epoch) training loss: 0.00596209500409459\n",
      "Episode average V value: 0\n",
      "epoch 1:\n",
      "Learning rate: 0.0001\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score per episode (id: 0) is 0.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 0.0 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1235,  0.2706,  0.6636, -0.5422, -0.3906,  0.0878, -0.1146,  0.1714,\n",
      "         0.0328,  0.6091]) tensor([-0.1721,  0.2198,  0.6275, -0.5283, -0.4505,  0.0300, -0.1098,  0.2076,\n",
      "         0.0047,  0.6564]) tensor([-0.0155,  0.4316,  0.6882, -0.5752, -0.2154,  0.2133, -0.2109,  0.0824,\n",
      "        -0.0292,  0.5623])\n",
      "R[0]\n",
      "tensor([-0.0159], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.01888653507269919 0.006016331738283043 0.013699142336408841 0.004048267596226651 0.24155641874670983 0.0003440786972641945 0.01581633604085073\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2068,  0.0041,  0.5866, -0.5133, -0.6962, -0.2230,  0.1066,  0.6979,\n",
      "         0.2347,  0.3424]) tensor([-0.1798,  0.0295,  0.5744, -0.5157, -0.6519, -0.1862,  0.0993,  0.6583,\n",
      "         0.2334,  0.3046]) tensor([-0.2068,  0.0041,  0.5866, -0.5133, -0.6962, -0.2230,  0.1066,  0.6979,\n",
      "         0.2347,  0.3424])\n",
      "R[0]\n",
      "tensor([0.0142], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.018654404079541564 0.0053847456850489835 0.012629834249622945 0.0032980082317371853 0.23654230508208274 0.0002972908318042755 0.014111495160032064\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2248, -0.0685,  0.3164, -0.3979, -0.5969, -0.2229,  0.5223,  0.3792,\n",
      "         0.5527, -0.4361]) tensor([-0.1637, -0.0386,  0.3669, -0.4205, -0.5412, -0.1512,  0.4928,  0.3422,\n",
      "         0.5611, -0.4531]) tensor([ 0.0586,  0.1645,  0.7091, -0.5548, -0.4466, -0.1447,  0.3687,  0.5496,\n",
      "         0.6569, -0.3291])\n",
      "R[0]\n",
      "tensor([-0.0035], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.019573038097470997 0.005314801357606484 0.010978701493237167 0.0036182954876858276 0.23048743984103204 0.0002461675927042961 0.014425819517695344\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2375,  0.3862,  0.1038, -0.3548,  0.3187,  0.7683,  0.2358, -0.7735,\n",
      "         0.2557, -0.6702]) tensor([ 0.2150,  0.3435,  0.0921, -0.3340,  0.2704,  0.7287,  0.2383, -0.7308,\n",
      "         0.2471, -0.6492]) tensor([ 0.2347,  0.3643,  0.0664, -0.3334,  0.3260,  0.7557,  0.2812, -0.7861,\n",
      "         0.2650, -0.7390])\n",
      "R[0]\n",
      "tensor([0.0058], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020250558180734516 0.006333859472331824 0.01016186454569106 0.0038355677915969865 0.22588032442331313 0.0004464311674237251 0.014419493089895695\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4611, -0.0463,  0.1493, -0.3307, -0.5257, -0.0200,  0.2117, -0.2064,\n",
      "        -0.0379,  0.3429]) tensor([-0.4145, -0.0642,  0.1522, -0.3290, -0.5058, -0.0148,  0.1923, -0.1943,\n",
      "        -0.0479,  0.3377]) tensor([-0.6213, -0.2540, -0.0399, -0.2288, -0.7181, -0.2092,  0.3937, -0.0950,\n",
      "        -0.0206,  0.1842])\n",
      "R[0]\n",
      "tensor([-0.0322], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02087732408940792 0.007102277354075341 0.011281356398030766 0.004227610859030392 0.2197746124267578 0.0002491383925080299 0.014304640095913783\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2006,  0.4665,  0.8922, -0.6580, -0.1231,  0.1880,  0.0159,  0.2939,\n",
      "         0.3253,  0.1365]) tensor([ 0.1858,  0.4540,  0.8526, -0.6385, -0.1256,  0.1799,  0.0336,  0.2748,\n",
      "         0.3171,  0.1226]) tensor([ 0.0831,  0.3371,  0.8481, -0.6250, -0.2889,  0.0307,  0.0758,  0.4390,\n",
      "         0.3249,  0.2066])\n",
      "R[0]\n",
      "tensor([-0.0010], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0207991695869714 0.00534655800260225 0.010126343180680123 0.004094100343470928 0.21732931607961656 0.0002970490828156471 0.013738935412373393\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4501, -0.3121, -0.1191, -0.1110, -0.7044, -0.3249,  0.5155,  0.1063,\n",
      "         0.2105, -0.3371]) tensor([-0.3740, -0.2829, -0.0548, -0.1337, -0.6516, -0.2706,  0.4695,  0.0973,\n",
      "         0.2171, -0.3399]) tensor([-0.1977, -0.1121,  0.3270, -0.3090, -0.6595, -0.2971,  0.3676,  0.4137,\n",
      "         0.3965, -0.1946])\n",
      "R[0]\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021286918153986336 0.005856414878151554 0.01097608718331321 0.004786422008473892 0.21292430263757706 0.0003032120764255524 0.012209031294216402\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4971,  0.0830,  0.0192,  0.1952,  0.1753,  0.4393, -0.2648, -0.6352,\n",
      "        -0.3802,  0.0815]) tensor([ 0.4945,  0.0921,  0.0521,  0.1685,  0.1612,  0.4198, -0.2451, -0.5911,\n",
      "        -0.3363,  0.0744]) tensor([ 0.4412,  0.2074,  0.1931,  0.0857,  0.1778,  0.4304, -0.2868, -0.6397,\n",
      "        -0.3507,  0.2472])\n",
      "R[0]\n",
      "tensor([0.0167], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021596621330827473 0.006078199082301581 0.009636872814895468 0.004759676925837994 0.21296984036266803 0.0003303435444831848 0.012531629657140001\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3052, -0.1658,  0.0791, -0.1362, -0.6464, -0.3010,  0.2343,  0.1369,\n",
      "         0.0483,  0.0205]) tensor([-0.2388, -0.1311,  0.1177, -0.1583, -0.5960, -0.2574,  0.2073,  0.1218,\n",
      "         0.0659, -0.0052]) tensor([-0.2141,  0.1360,  0.3324, -0.3686, -0.4481, -0.0533,  0.0372,  0.0668,\n",
      "         0.0279,  0.2268])\n",
      "R[0]\n",
      "tensor([-0.0015], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02259700531512499 0.006519037253397982 0.009215872753251461 0.0054617888235952705 0.2084083828777075 0.0004971953630447388 0.011495128761511295\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3772, -0.2040,  0.4583, -0.4329, -0.9052, -0.4185,  0.3441,  0.7248,\n",
      "         0.3945,  0.1766]) tensor([-0.3318, -0.1751,  0.4312, -0.4297, -0.8502, -0.3642,  0.3244,  0.6612,\n",
      "         0.3862,  0.1274]) tensor([-0.4333, -0.2477,  0.4560, -0.3982, -0.9436, -0.5116,  0.3158,  0.7765,\n",
      "         0.2736,  0.4108])\n",
      "R[0]\n",
      "tensor([-0.0002], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022487686099484563 0.0062282074199974885 0.009850112006490235 0.005302775581832975 0.20920099638402462 0.000296792097389698 0.010719514293130488\n",
      "Average (on the epoch) training loss: 0.00434325136494881\n",
      "Episode average V value: 0\n",
      "epoch 2:\n",
      "Learning rate: 9e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 1.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 0.9999000099990001 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3919,  0.6285,  0.4498, -0.5310,  0.4432,  0.8347,  0.0618, -0.6564,\n",
      "         0.2804, -0.5141]) tensor([ 0.3797,  0.5799,  0.4498, -0.5169,  0.3985,  0.8046,  0.0672, -0.5990,\n",
      "         0.2742, -0.4904]) tensor([ 0.2523,  0.3845,  0.3068, -0.3952,  0.2944,  0.7529,  0.1204, -0.6649,\n",
      "         0.0955, -0.2446])\n",
      "R[0]\n",
      "tensor([-0.0011], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022767915157601237 0.006401070468040416 0.008704128814002616 0.005574549739714712 0.20806913766264915 0.0002929259464144707 0.009912539151555393\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3322,  0.0296, -0.0415, -0.2586, -0.2073,  0.3244,  0.2495, -0.6391,\n",
      "        -0.0455,  0.0195]) tensor([-0.2779,  0.0025, -0.0461, -0.2439, -0.2043,  0.3156,  0.2205, -0.5971,\n",
      "        -0.0596,  0.0188]) tensor([-0.4882, -0.1431, -0.2010, -0.1770, -0.3707,  0.1167,  0.4603, -0.5348,\n",
      "         0.0134, -0.1969])\n",
      "R[0]\n",
      "tensor([-0.0269], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.023529467590153216 0.006686069076742569 0.011248474616062595 0.005623449149192311 0.2019209626019001 0.0003192952051758766 0.009566548952250741\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2989,  0.8742,  0.8159, -0.8154,  0.2668,  0.7382, -0.3655, -0.0709,\n",
      "        -0.0129,  0.2494]) tensor([ 0.2299,  0.7529,  0.7286, -0.7637,  0.1495,  0.6290, -0.3410, -0.0109,\n",
      "        -0.0564,  0.3104]) tensor([ 0.2756,  0.7470,  0.6694, -0.6595,  0.2136,  0.7398, -0.4216, -0.2205,\n",
      "        -0.1374,  0.3739])\n",
      "R[0]\n",
      "tensor([0.0112], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.023786982176825403 0.006998968115847674 0.008499412577286421 0.0056306878190953286 0.20002996085584163 0.0004809950664639473 0.010124017352121883\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3060, -0.7075, -0.1708, -0.0707, -0.9310,  0.1415,  0.0437,  0.3901,\n",
      "        -0.2759,  0.6672]) tensor([-0.2305, -0.6377, -0.1155, -0.1129, -0.8615,  0.1935,  0.0307,  0.3675,\n",
      "        -0.2352,  0.6208]) tensor([-0.1698, -0.5776, -0.0792, -0.1083, -0.7176,  0.3908,  0.0260,  0.1004,\n",
      "        -0.2014,  0.6492])\n",
      "R[0]\n",
      "tensor([0.0219], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.023083324752748012 0.005549895272080903 0.008985647990528377 0.005467976486193947 0.20262991625070573 0.0004267629235982895 0.009668559088400797\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0930, -0.5000, -0.0179, -0.0417, -0.3576,  0.5691,  0.2581, -0.2954,\n",
      "         0.0285,  0.2668]) tensor([ 0.0651, -0.4899,  0.0046, -0.0408, -0.3704,  0.5760,  0.2468, -0.2661,\n",
      "         0.0136,  0.3048]) tensor([-0.1131, -0.5072,  0.0380, -0.1132, -0.4984,  0.3043,  0.4245, -0.1137,\n",
      "         0.1098,  0.2523])\n",
      "R[0]\n",
      "tensor([0.0006], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.023608560472726824 0.0067673272998654286 0.008408176853568875 0.005447227349271997 0.20182065673172475 0.0004712064415216446 0.010010695178469177\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0792, -0.0708, -0.0241, -0.2740, -0.0371,  0.8100,  0.1310, -0.5786,\n",
      "        -0.3017,  0.2929]) tensor([-0.0409, -0.1019, -0.0303, -0.2627, -0.0551,  0.7818,  0.1188, -0.5217,\n",
      "        -0.3007,  0.2939]) tensor([-0.0606, -0.1203, -0.2012, -0.1205,  0.0100,  0.8102,  0.1472, -0.7866,\n",
      "        -0.3492,  0.1628])\n",
      "R[0]\n",
      "tensor([0.0076], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.023460553759709 0.005604371029010508 0.008392913475428941 0.00534348350064829 0.19769278091192247 0.000573120191693306 0.009801729052414885\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4176,  0.1415,  0.5721, -0.5786, -0.6273, -0.1075, -0.0651,  0.5212,\n",
      "        -0.0878,  0.8386]) tensor([-0.3922,  0.1413,  0.5185, -0.5486, -0.6074, -0.0970, -0.0602,  0.4820,\n",
      "        -0.0971,  0.7869]) tensor([-0.4858,  0.0083,  0.5269, -0.5268, -0.7431, -0.2534,  0.0650,  0.5993,\n",
      "        -0.0053,  0.7894])\n",
      "R[0]\n",
      "tensor([-0.0153], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02362204523012042 0.006786742828524438 0.007810909737236216 0.005361344115808606 0.19642932349443434 0.0006691584885120392 0.009206353611138184\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3704, -0.6297, -0.5426,  0.1102, -0.7402,  0.1939,  0.2940, -0.2411,\n",
      "        -0.0370,  0.0013]) tensor([-0.3350, -0.5610, -0.4810,  0.0712, -0.6845,  0.2240,  0.2948, -0.2534,\n",
      "         0.0033, -0.0238]) tensor([-0.1573, -0.3076, -0.4747,  0.1100, -0.3791,  0.3924,  0.1270, -0.6159,\n",
      "        -0.1226, -0.1532])\n",
      "R[0]\n",
      "tensor([0.0166], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.023223211538046597 0.005813267898280174 0.006841131141576625 0.00519788998819422 0.19972878813743591 0.000620740719139576 0.009660779025522061\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3617, -0.1347, -0.3007, -0.1322, -0.2539,  0.2475,  0.5272, -0.5921,\n",
      "         0.0821, -0.5840]) tensor([-0.3538, -0.1523, -0.2784, -0.1300, -0.2840,  0.2016,  0.5111, -0.5387,\n",
      "         0.0804, -0.5429]) tensor([-0.1487,  0.0505, -0.1278, -0.2332, -0.0012,  0.4858,  0.4703, -0.6884,\n",
      "         0.1096, -0.6484])\n",
      "R[0]\n",
      "tensor([0.0011], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.023084256663918494 0.006480994552650373 0.006799405686120735 0.005073141319444403 0.2011963502317667 0.0007328400984406471 0.009575686289230362\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2001,  0.2547,  0.5177, -0.5555, -0.5854, -0.0853, -0.2304,  0.7159,\n",
      "        -0.0570,  0.3853]) tensor([-0.2084,  0.2598,  0.4807, -0.5497, -0.5808, -0.1197, -0.1751,  0.6687,\n",
      "        -0.0214,  0.3260]) tensor([-0.2946,  0.1415,  0.5604, -0.5361, -0.6676, -0.2041, -0.1154,  0.7641,\n",
      "        -0.0222,  0.5700])\n",
      "R[0]\n",
      "tensor([0.0176], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02274365857988596 0.005743898256070679 0.0072636862741492225 0.004987505116965622 0.2029978407919407 0.0005111686810851097 0.01008682625851361\n",
      "Average (on the epoch) training loss: 0.005370725458452944\n",
      "Episode average V value: 0\n",
      "epoch 3:\n",
      "Learning rate: 8.1e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score per episode (id: 0) is 102.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 101.98980101989801 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3315,  0.5105,  0.8647, -0.6727, -0.1465,  0.0479,  0.2394,  0.5562,\n",
      "         0.7680, -0.8919]) tensor([ 0.3663,  0.5730,  0.9178, -0.7015, -0.0498,  0.1964,  0.2066,  0.4849,\n",
      "         0.7672, -0.9238]) tensor([ 0.3994,  0.6111,  0.9069, -0.7121, -0.0239,  0.1647,  0.1944,  0.4440,\n",
      "         0.7552, -0.9270])\n",
      "R[0]\n",
      "tensor([0.0051], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022073978301137685 0.006662368233824964 0.0084300954264545 0.005489678370533511 0.2002567837834358 0.00047574901580810545 0.009394837500411085\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0644,  0.1885,  0.5491, -0.4945, -0.4568, -0.2347,  0.3812,  0.6785,\n",
      "         0.7062, -0.8309]) tensor([ 0.0699,  0.2391,  0.5595, -0.5275, -0.3933, -0.1710,  0.3833,  0.5909,\n",
      "         0.7224, -0.8803]) tensor([ 0.2087,  0.3589,  0.7511, -0.6050, -0.3225, -0.1106,  0.2943,  0.6770,\n",
      "         0.7522, -0.8343])\n",
      "R[0]\n",
      "tensor([0.0026], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02256841181963682 0.006824966395244701 0.007926363430226046 0.005374940296751447 0.20050785836577414 0.0004257114753127098 0.009723065633210353\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0137,  0.3400,  0.2149, -0.4308,  0.0033,  0.8515, -0.3423, -0.5130,\n",
      "        -0.3736,  0.4321]) tensor([ 0.0171,  0.3462,  0.2152, -0.4379,  0.0055,  0.8558, -0.3468, -0.5017,\n",
      "        -0.3734,  0.4137]) tensor([ 0.0759,  0.1270,  0.2224, -0.3441, -0.1015,  0.8180, -0.3201, -0.3220,\n",
      "        -0.4078,  0.6111])\n",
      "R[0]\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0224879932962358 0.006462075524512329 0.0064089401242599705 0.005385369017836638 0.20211074694991113 0.0004414603263139725 0.010345269855461083\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5544, -0.4686, -0.2278, -0.1078, -0.8080, -0.4323,  0.6245,  0.3600,\n",
      "         0.1862, -0.3815]) tensor([-0.5213, -0.4402, -0.2102, -0.1166, -0.7753, -0.4116,  0.5968,  0.3367,\n",
      "         0.1803, -0.3693]) tensor([-0.5458, -0.4446, -0.3013, -0.0956, -0.7393, -0.3683,  0.6780,  0.2479,\n",
      "         0.2210, -0.5661])\n",
      "R[0]\n",
      "tensor([-0.0063], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021931939035654067 0.006009314886119682 0.0076527334280544895 0.004628188585629686 0.20235993191599846 0.0004201081842184067 0.010254068209964317\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5868, -0.3765, -0.1524, -0.1292, -0.6184, -0.0989,  0.5543, -0.2171,\n",
      "         0.0946, -0.0106]) tensor([-0.5137, -0.3393, -0.1588, -0.1197, -0.5486, -0.0291,  0.5035, -0.2590,\n",
      "         0.0707, -0.0467]) tensor([-0.4736, -0.2564, -0.0427, -0.1987, -0.4899,  0.0678,  0.4453, -0.3531,\n",
      "         0.1082,  0.0474])\n",
      "R[0]\n",
      "tensor([-0.0251], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02238506678491831 0.006422631015928346 0.006505928041267907 0.0053472728529013695 0.19769695533812046 0.0004854844957590103 0.009426121518947185\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0315,  0.0093,  0.5511, -0.4348, -0.6706, -0.3708,  0.2993,  0.8522,\n",
      "         0.5875, -0.4467]) tensor([-0.1279, -0.1020,  0.5052, -0.3826, -0.7929, -0.4811,  0.3054,  0.9265,\n",
      "         0.5244, -0.2951]) tensor([-0.1918, -0.1790,  0.2517, -0.2997, -0.7301, -0.4101,  0.4291,  0.7085,\n",
      "         0.4770, -0.5034])\n",
      "R[0]\n",
      "tensor([0.0025], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02277247170172632 0.007237872758560115 0.00641247422775632 0.004945589233539067 0.19993742060661315 0.00041072530299425124 0.009712106855469756\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2683, -0.0648, -0.0613, -0.0256, -0.1097,  0.3382, -0.0099, -0.1344,\n",
      "        -0.1503, -0.1354]) tensor([ 0.2311, -0.0595, -0.0403, -0.0462, -0.1378,  0.2800,  0.0335, -0.1037,\n",
      "        -0.0967, -0.1535]) tensor([ 0.4671, -0.3676, -0.3536,  0.2860, -0.2442,  0.2925, -0.3652,  0.1335,\n",
      "        -0.6495,  0.1553])\n",
      "R[0]\n",
      "tensor([0.0519], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0219886307567358 0.005191001311890432 0.006802659611486888 0.004799077101808507 0.19891874021291733 0.0002424060255289078 0.009254574703169056\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5198, -0.2450,  0.1947, -0.4441, -0.7448,  0.3007, -0.0476,  0.2138,\n",
      "        -0.1643,  0.9655]) tensor([-0.4116, -0.1363,  0.2115, -0.4635, -0.5882,  0.4866, -0.1521,  0.0875,\n",
      "        -0.2214,  0.9008]) tensor([-0.5812, -0.5361, -0.2225, -0.2226, -0.8678,  0.4261, -0.0285,  0.0503,\n",
      "        -0.3505,  0.9309])\n",
      "R[0]\n",
      "tensor([-0.0156], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021733107957988978 0.006441222770859895 0.006406092426404939 0.005314751675934531 0.20037748090922833 0.0002764130309224129 0.009642798453569412\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2213,  0.3662,  0.7964, -0.6139, -0.3227, -0.0774,  0.2359,  0.6724,\n",
      "         0.7031, -0.7476]) tensor([ 0.0493,  0.1383,  0.7014, -0.5213, -0.5712, -0.2989,  0.2878,  0.8471,\n",
      "         0.6455, -0.5329]) tensor([ 0.0116,  0.1034,  0.7025, -0.5020, -0.6323, -0.3073,  0.2077,  0.8528,\n",
      "         0.5916, -0.3295])\n",
      "R[0]\n",
      "tensor([0.0018], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021387723887339234 0.0052243312674836485 0.006494783064670628 0.005202213360462338 0.19864229659736157 0.0005707580670714378 0.009217183152330107\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1938,  0.1838,  0.3866, -0.4800, -0.2234,  0.2458,  0.1767, -0.2990,\n",
      "         0.2412,  0.0311]) tensor([-0.1872,  0.1526,  0.3484, -0.4486, -0.2508,  0.2060,  0.1804, -0.2729,\n",
      "         0.2354,  0.0157]) tensor([-0.1766,  0.2112,  0.4032, -0.4937, -0.2020,  0.2599,  0.1644, -0.3018,\n",
      "         0.2437,  0.0196])\n",
      "R[0]\n",
      "tensor([-0.0010], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02204501098021865 0.0064739052651639215 0.0064967899746479815 0.0053260002568131315 0.1996255764067173 0.0002720034718513489 0.009825392651255243\n",
      "Average (on the epoch) training loss: 0.005181308075221023\n",
      "Episode average V value: 0\n",
      "epoch 4:\n",
      "Learning rate: 7.290000000000001e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 91.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 90.99090090990902 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3381, -0.1300, -0.4213, -0.0687, -0.1598,  0.5157,  0.2164, -0.6635,\n",
      "        -0.4072,  0.0177]) tensor([-0.3018, -0.0691, -0.3624, -0.1135, -0.1118,  0.5373,  0.1844, -0.6761,\n",
      "        -0.3980,  0.0149]) tensor([-0.2518, -0.1321, -0.4952, -0.0195, -0.0923,  0.5284,  0.2535, -0.7246,\n",
      "        -0.3711, -0.1737])\n",
      "R[0]\n",
      "tensor([-0.0141], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021590746676549314 0.005959771543704847 0.00662613893240632 0.005014899510890246 0.1929120756983757 0.00029295361787080767 0.010077033950132317\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1545, -0.3308,  0.3961, -0.1890, -0.5856,  0.1336, -0.2669,  0.7997,\n",
      "        -0.3654,  0.8544]) tensor([ 0.1522, -0.2789,  0.4055, -0.2092, -0.5453,  0.1551, -0.2194,  0.7459,\n",
      "        -0.3002,  0.7612]) tensor([ 0.2570, -0.1415,  0.4852, -0.2630, -0.4301,  0.4489, -0.4340,  0.5240,\n",
      "        -0.3563,  0.8820])\n",
      "R[0]\n",
      "tensor([0.0715], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022111363392323256 0.005675530300926766 0.006253645813652838 0.004747642024653032 0.19297979955375194 0.0002273087278008461 0.009894327974179759\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1294,  0.1414,  0.7590, -0.6159, -0.3475,  0.2453,  0.0829,  0.2500,\n",
      "         0.1696,  0.4718]) tensor([-0.1264,  0.1146,  0.6914, -0.5806, -0.3510,  0.2498,  0.0990,  0.2250,\n",
      "         0.1701,  0.4072]) tensor([-0.1949,  0.0581,  0.7921, -0.5767, -0.4558, -0.0671,  0.2876,  0.4165,\n",
      "         0.3441,  0.3388])\n",
      "R[0]\n",
      "tensor([-0.0134], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02196570071205497 0.005378305134945549 0.006714399611439149 0.004896311430202332 0.18998087866604327 0.0002630635350942612 0.009012028035300318\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4189, -0.5547, -0.5027,  0.0705, -0.7206,  0.2762,  0.1418, -0.1916,\n",
      "        -0.2631,  0.2683]) tensor([-0.3804, -0.5667, -0.4688,  0.0725, -0.7489,  0.2439,  0.1400, -0.1171,\n",
      "        -0.2392,  0.2875]) tensor([-0.4605, -0.6304, -0.5391,  0.0897, -0.7859,  0.1887,  0.2036, -0.0917,\n",
      "        -0.2485,  0.2469])\n",
      "R[0]\n",
      "tensor([-0.0138], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022319595698267223 0.006034308667753067 0.006371497531217756 0.004996053754002788 0.19233917072415352 0.0002702092230319977 0.009359586227452383\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2888, -0.1021, -0.1299, -0.1574, -0.1715,  0.5160,  0.1629, -0.5199,\n",
      "        -0.4086,  0.2796]) tensor([-0.2820, -0.1704, -0.1430, -0.1134, -0.2255,  0.4812,  0.1656, -0.4401,\n",
      "        -0.4306,  0.3385]) tensor([-0.4673, -0.2569, -0.2769, -0.0938, -0.3726,  0.2361,  0.3695, -0.4037,\n",
      "        -0.2673,  0.0622])\n",
      "R[0]\n",
      "tensor([-0.0119], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02300846706517041 0.006036115343325946 0.006864072526157543 0.005897838344913907 0.1869794534146786 0.0004919731840491295 0.009399232079507783\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5584, -0.2418,  0.4634, -0.4153, -0.8505, -0.3216,  0.2345,  0.5828,\n",
      "         0.1129,  0.7786]) tensor([-4.5466e-01, -1.4521e-01,  4.5859e-01, -4.1313e-01, -6.9727e-01,\n",
      "        -1.0687e-01,  9.7540e-02,  4.3588e-01,  1.6401e-04,  7.7179e-01]) tensor([-0.3524,  0.1138,  0.6265, -0.5774, -0.4980,  0.1118, -0.0710,  0.2935,\n",
      "        -0.0686,  0.9024])\n",
      "R[0]\n",
      "tensor([-0.0015], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022112711507827044 0.005583573307369079 0.005972399172336737 0.005154481611796655 0.1899237257540226 0.0004026264697313309 0.009180606996873394\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6373, -0.4524, -0.1530, -0.3382, -0.7708,  0.1619,  0.3647,  0.2525,\n",
      "        -0.0822,  0.3154]) tensor([-0.5728, -0.4184, -0.1451, -0.3294, -0.7279,  0.1807,  0.3524,  0.2442,\n",
      "        -0.0691,  0.2643]) tensor([-0.5707, -0.3173,  0.0539, -0.3431, -0.7499, -0.0210,  0.2591,  0.3961,\n",
      "        -0.1342,  0.4618])\n",
      "R[0]\n",
      "tensor([0.0209], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022424593122676015 0.005337689407228027 0.005347935803314613 0.005239677771111019 0.1862899717837572 0.00047725804895162584 0.00923802631662693\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0784,  0.4323,  0.8173, -0.6708, -0.2149,  0.1355, -0.0548,  0.4947,\n",
      "        -0.0736,  0.4012]) tensor([-0.0521,  0.4170,  0.7720, -0.6478, -0.1941,  0.1594, -0.0394,  0.4560,\n",
      "        -0.0810,  0.3429]) tensor([ 0.0945,  0.5543,  0.8870, -0.7046, -0.0950,  0.2743, -0.1701,  0.4764,\n",
      "        -0.0458,  0.3124])\n",
      "R[0]\n",
      "tensor([0.0008], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022299531077966093 0.0059402906883260585 0.006221039591735462 0.0056325162590947 0.18775017474591732 0.00040937594324350356 0.00893400922161527\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1237,  0.4777,  0.1665, -0.3957,  0.0682,  0.7777, -0.3218, -0.6773,\n",
      "        -0.0344, -0.1574]) tensor([ 0.0965,  0.4260,  0.1607, -0.3986, -0.0075,  0.6789, -0.2982, -0.6093,\n",
      "         0.0014, -0.1568]) tensor([ 0.1924,  0.5228,  0.1828, -0.4033,  0.0890,  0.7975, -0.3664, -0.6572,\n",
      "         0.0119, -0.2593])\n",
      "R[0]\n",
      "tensor([0.0032], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0228429018240422 0.007527352788762073 0.005828297488009412 0.006014610836631618 0.19082295206189157 0.0004760046228766441 0.008379749714396895\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1681, -0.2720, -0.4246,  0.0343, -0.2449,  0.6900, -0.5030, -0.0360,\n",
      "        -0.9666,  0.4676]) tensor([ 0.1788, -0.1852, -0.3361, -0.0424, -0.1890,  0.6978, -0.4893, -0.0592,\n",
      "        -0.9113,  0.4350]) tensor([ 0.1260, -0.2666, -0.3158, -0.0472, -0.3320,  0.6960, -0.5472,  0.0608,\n",
      "        -0.9352,  0.6126])\n",
      "R[0]\n",
      "tensor([-0.0040], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022606240522116423 0.006541818903846433 0.006696444875939051 0.005847236030152999 0.18948786398768425 0.0005984043627977371 0.008992304867657367\n",
      "Average (on the epoch) training loss: 0.005344126757344929\n",
      "Episode average V value: 0\n",
      "epoch 5:\n",
      "Learning rate: 6.561000000000002e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2507,  0.3150,  0.7500, -0.5169, -0.3224, -0.1308,  0.2707,  0.6674,\n",
      "         0.6872, -0.7926]) tensor([ 0.2368,  0.3632,  0.7436, -0.5436, -0.2682, -0.0744,  0.2598,  0.5751,\n",
      "         0.6756, -0.8155]) tensor([ 0.3265,  0.4700,  0.8515, -0.6059, -0.1882, -0.0102,  0.2041,  0.5900,\n",
      "         0.6903, -0.8349])\n",
      "R[0]\n",
      "tensor([0.0027], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022223267963156105 0.005571522228849062 0.006605032914798357 0.005899808969348669 0.18985865093767643 0.00038509376347064975 0.008870028614881448\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 1.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1615, -0.3767, -0.4542,  0.0459, -0.1506,  0.5047,  0.5420, -0.6977,\n",
      "        -0.0334, -0.3534]) tensor([-0.1689, -0.3925, -0.4232,  0.0337, -0.1925,  0.4387,  0.5230, -0.6305,\n",
      "        -0.0293, -0.2907]) tensor([ 0.0630, -0.0333, -0.1312, -0.1764, -0.0055,  0.6212,  0.4342, -0.5661,\n",
      "         0.2187, -0.7460])\n",
      "R[0]\n",
      "tensor([0.0115], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022282721208408476 0.006208260268147569 0.0066450675143642 0.006047277091769502 0.1912193034887314 0.0003179152086377144 0.009418862279097084\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4807, -0.6514, -0.2600,  0.1470, -0.2315,  0.5088,  0.2635, -0.0411,\n",
      "         0.0723, -0.3530]) tensor([ 0.4677, -0.5887, -0.1867,  0.0899, -0.2250,  0.5046,  0.2419, -0.0185,\n",
      "         0.1018, -0.3341]) tensor([ 0.4933, -0.6258, -0.2198,  0.1182, -0.2321,  0.5079,  0.2511, -0.0148,\n",
      "         0.0995, -0.3632])\n",
      "R[0]\n",
      "tensor([0.0424], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022009045472368597 0.005609944302035728 0.006359188236972841 0.006075515415752307 0.1904583415389061 0.00028554681688547134 0.009044379910628777\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2903,  0.3489,  0.7694, -0.5182, -0.2799, -0.1048,  0.2404,  0.6411,\n",
      "         0.6671, -0.7933]) tensor([ 0.2868,  0.3965,  0.7689, -0.5404, -0.2338, -0.0661,  0.2268,  0.5695,\n",
      "         0.6618, -0.8142]) tensor([ 0.3631,  0.5025,  0.8598, -0.6003, -0.1414,  0.0134,  0.1767,  0.5512,\n",
      "         0.6651, -0.8379])\n",
      "R[0]\n",
      "tensor([-0.0005], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022085155956447125 0.005706490863776708 0.0044729490070967584 0.005427161452593282 0.18875555273890496 0.0002724999785423279 0.00884559010565863\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2992,  0.6458,  0.4207, -0.5257,  0.4180,  0.8677, -0.1409, -0.4458,\n",
      "        -0.1742, -0.1315]) tensor([ 0.2781,  0.5778,  0.3798, -0.4794,  0.4065,  0.8864, -0.1480, -0.4314,\n",
      "        -0.2604, -0.0322]) tensor([ 0.1034,  0.4121,  0.0630, -0.3530,  0.2918,  0.8837, -0.0559, -0.6804,\n",
      "        -0.2849, -0.1514])\n",
      "R[0]\n",
      "tensor([-0.0076], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021528366250917315 0.0053323953510480354 0.0056164085770651585 0.005411328869406134 0.18968657276034356 0.0003505149036645889 0.0094393346052384\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1591, -0.8604, -0.4488,  0.1324, -0.7979,  0.0683,  0.2157,  0.4232,\n",
      "        -0.2883,  0.2181]) tensor([-0.1188, -0.8105, -0.3863,  0.0974, -0.7727,  0.0916,  0.2008,  0.4428,\n",
      "        -0.2535,  0.2100]) tensor([-0.2136, -0.7198, -0.3080,  0.0372, -0.8337,  0.1347, -0.0593,  0.5637,\n",
      "        -0.5597,  0.6364])\n",
      "R[0]\n",
      "tensor([-0.0018], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021361836906522512 0.005929673983409885 0.005272901359781827 0.005473677243688144 0.19237655751407146 0.00027328108251094816 0.008924872331728693\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2709,  0.7321,  0.5949, -0.6471,  0.3372,  0.8284, -0.2135, -0.2157,\n",
      "        -0.1627, -0.0605]) tensor([ 0.2404,  0.7006,  0.5680, -0.6296,  0.3176,  0.8064, -0.1660, -0.2130,\n",
      "        -0.1478, -0.0949]) tensor([ 0.2766,  0.7652,  0.5353, -0.6548,  0.3765,  0.8748, -0.1979, -0.2843,\n",
      "        -0.1174, -0.2080])\n",
      "R[0]\n",
      "tensor([0.0006], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021709983313456176 0.005589706755097723 0.005588842159631895 0.005337059563258663 0.1907420673072338 0.0002159677892923355 0.009009984536154661\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1300, -0.1043,  0.6559, -0.3253, -0.5138, -0.0235, -0.0243,  0.2853,\n",
      "        -0.0243,  0.9745]) tensor([-1.2852e-01, -1.6818e-01,  5.8092e-01, -2.8153e-01, -5.5053e-01,\n",
      "        -3.9439e-02,  8.2105e-05,  2.9398e-01, -2.9397e-02,  9.2241e-01]) tensor([ 4.3096e-02,  2.2733e-04,  7.7753e-01, -3.7632e-01, -4.1864e-01,\n",
      "         9.0897e-02, -1.8561e-01,  4.6972e-01, -1.1221e-01,  1.0382e+00])\n",
      "R[0]\n",
      "tensor([-0.0055], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021870178855955602 0.006301054842959275 0.005662509099885938 0.005806985383853316 0.1915911886394024 0.00021211876720190047 0.009168085474404506\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2588,  0.7592,  0.4763, -0.6488,  0.3918,  0.8890, -0.1555, -0.3210,\n",
      "        -0.1007, -0.3043]) tensor([ 0.2460,  0.7447,  0.4696, -0.6416,  0.3898,  0.8854, -0.1296, -0.3175,\n",
      "        -0.0987, -0.3173]) tensor([ 0.2783,  0.7818,  0.4597, -0.6533,  0.4166,  0.9031, -0.1422, -0.3562,\n",
      "        -0.0489, -0.4000])\n",
      "R[0]\n",
      "tensor([0.0013], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02163879088498652 0.006159404128113237 0.005258955524135672 0.0056914789326838215 0.19274147160351277 0.00022370342165231705 0.008995153579395265\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1889, -0.5593,  0.1082, -0.0885, -0.7799, -0.3281,  0.4267,  0.6200,\n",
      "         0.1807,  0.1078]) tensor([-0.1079, -0.4667,  0.1517, -0.1368, -0.7016, -0.2645,  0.3864,  0.5823,\n",
      "         0.2190,  0.0170]) tensor([ 0.0681, -0.4971,  0.0791, -0.0623, -0.5372, -0.0698,  0.4311,  0.4039,\n",
      "         0.2710, -0.1894])\n",
      "R[0]\n",
      "tensor([0.0064], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02149592290818691 0.005791292033449281 0.006731654127754155 0.0056751579091651365 0.19036405818164348 0.0002428887188434601 0.0095694468904112\n",
      "Average (on the epoch) training loss: 0.005684545083151898\n",
      "Episode average V value: 0\n",
      "epoch 6:\n",
      "Learning rate: 5.904900000000002e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3732,  0.1252,  0.1274, -0.0852,  0.0199,  0.6329, -0.1493, -0.4243,\n",
      "        -0.0545, -0.1179]) tensor([ 0.3494,  0.0809,  0.1122, -0.0679, -0.0056,  0.6308, -0.1546, -0.3973,\n",
      "        -0.1005, -0.0443]) tensor([ 0.3497,  0.1896,  0.2015, -0.1628,  0.0274,  0.6764, -0.1597, -0.4467,\n",
      "        -0.0035, -0.1081])\n",
      "R[0]\n",
      "tensor([-0.0186], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021833924777805803 0.005643741392181255 0.004951068176633271 0.0056184358574682845 0.18896091303229332 0.000394541896879673 0.009096604125399609\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0754,  0.0256,  0.7323, -0.4625, -0.4717, -0.1431,  0.2847,  0.4867,\n",
      "         0.4128,  0.1724]) tensor([-0.1006, -0.0187,  0.6578, -0.4279, -0.4879, -0.1527,  0.3062,  0.4577,\n",
      "         0.3960,  0.1451]) tensor([-0.1096,  0.0015,  0.7252, -0.4549, -0.5180, -0.1936,  0.2917,  0.5241,\n",
      "         0.4140,  0.2006])\n",
      "R[0]\n",
      "tensor([-0.0137], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02186776161380112 0.005589064396219328 0.005612608265313611 0.0054233705763472246 0.18674381935596465 0.0003305906355381012 0.008994809032534249\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3390,  0.2644,  0.4433, -0.2765,  0.1321,  0.6205, -0.1662, -0.0220,\n",
      "        -0.3715,  0.3280]) tensor([ 0.3125,  0.2548,  0.4480, -0.2919,  0.1065,  0.5831, -0.1535, -0.0132,\n",
      "        -0.3445,  0.3181]) tensor([ 0.2077,  0.4274,  0.7262, -0.5229,  0.0405,  0.4579, -0.1132,  0.2266,\n",
      "        -0.1758,  0.3246])\n",
      "R[0]\n",
      "tensor([-0.0258], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021901178615167737 0.005739159509983437 0.005024403540184721 0.005623668601387181 0.18997935558855533 0.00033552514016628264 0.010000462360680104\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1621, -0.3264,  0.2266, -0.1433, -0.8032, -0.5123,  0.4281,  0.7688,\n",
      "         0.3543, -0.3211]) tensor([-0.1898, -0.3030,  0.2261, -0.1553, -0.7849, -0.5063,  0.4435,  0.7210,\n",
      "         0.3644, -0.3146]) tensor([-0.2851, -0.3712,  0.3199, -0.1827, -0.9179, -0.5793,  0.3746,  0.8511,\n",
      "         0.2741,  0.0793])\n",
      "R[0]\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02219684205763042 0.006183444288617466 0.0056132196133039545 0.005631209431798198 0.18493324691057206 0.000429846778512001 0.00913374339195434\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4779, -0.4088, -0.1482, -0.0398, -0.5650,  0.0869,  0.3267, -0.3113,\n",
      "        -0.1935,  0.3663]) tensor([-0.4458, -0.4653, -0.1721,  0.0009, -0.6051,  0.0472,  0.3220, -0.2272,\n",
      "        -0.2111,  0.3781]) tensor([-0.6340, -0.5966, -0.2907,  0.0426, -0.7768, -0.1743,  0.5386, -0.1102,\n",
      "        -0.0787,  0.1861])\n",
      "R[0]\n",
      "tensor([-0.0139], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022569666780531406 0.00519143690639612 0.00541849255359557 0.0054978380118263885 0.18456494545936583 0.0003474007099866867 0.009027473911293783\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4306,  0.5813,  0.8970, -0.6177, -0.0380,  0.1292,  0.1014,  0.4683,\n",
      "         0.5886, -0.7955]) tensor([ 0.4381,  0.6211,  0.8873, -0.6330,  0.0185,  0.1809,  0.0822,  0.3906,\n",
      "         0.5677, -0.8081]) tensor([ 0.2225,  0.1701,  0.6485, -0.4046, -0.3895, -0.1809,  0.3178,  0.6757,\n",
      "         0.6025, -0.7131])\n",
      "R[0]\n",
      "tensor([-0.0093], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022527744760736823 0.006085000221559312 0.005563874135950755 0.006049124186509289 0.18605626240372658 0.0004478321000933647 0.00901623120950535\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3063, -0.4424, -0.6509,  0.1225, -0.5350,  0.4114,  0.1161, -0.3940,\n",
      "        -0.2595, -0.0584]) tensor([-0.2644, -0.3863, -0.5732,  0.0846, -0.5031,  0.3968,  0.1195, -0.3649,\n",
      "        -0.2065, -0.0787]) tensor([-0.0833, -0.2229, -0.6275,  0.1525, -0.2985,  0.5554, -0.0845, -0.6597,\n",
      "        -0.3240, -0.2531])\n",
      "R[0]\n",
      "tensor([0.0079], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021979021064937116 0.005151409667734697 0.005844709759767284 0.005580039127729833 0.18775696501135827 0.00037212048470973967 0.008820360980811529\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 5.9941e-01,  9.3594e-01,  9.0639e-01, -7.6848e-01,  3.9293e-01,\n",
      "         6.2079e-01, -1.1133e-01,  3.1218e-04,  4.6411e-01, -8.3004e-01]) tensor([ 0.5914,  0.9717,  0.8959, -0.7883,  0.4564,  0.6739, -0.1135, -0.0838,\n",
      "         0.4415, -0.8424]) tensor([ 0.6164,  0.9545,  0.8843, -0.7687,  0.4320,  0.6706, -0.1210, -0.0596,\n",
      "         0.4605, -0.8480])\n",
      "R[0]\n",
      "tensor([-0.0018], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021886953480541708 0.005202902201439428 0.006131819506968896 0.005600144039606675 0.18861957456171513 0.00037901046872138976 0.009120987124333624\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2280,  0.1681,  0.4590, -0.5010, -0.2795,  0.2320,  0.0592, -0.1978,\n",
      "         0.0734,  0.3819]) tensor([-0.1849,  0.1942,  0.4494, -0.5059, -0.2438,  0.2692,  0.0215, -0.2355,\n",
      "         0.0753,  0.3255]) tensor([-0.2024,  0.2011,  0.4874, -0.5214, -0.2596,  0.2393,  0.0474, -0.1866,\n",
      "         0.0919,  0.3566])\n",
      "R[0]\n",
      "tensor([0.0063], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02215255917236209 0.005864102313142211 0.004821810416171502 0.005620275754947215 0.1894876434803009 0.00042349842935800555 0.009196549570886418\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2849,  0.0291,  0.0164, -0.1433, -0.1345,  0.8293, -0.4138, -0.3468,\n",
      "        -0.2250,  0.1632]) tensor([ 0.2609,  0.0251,  0.0453, -0.1716, -0.1561,  0.7695, -0.3787, -0.3273,\n",
      "        -0.1726,  0.1491]) tensor([-0.0616, -0.1571, -0.1061, -0.2023, -0.4141,  0.8717, -0.4063, -0.2811,\n",
      "        -0.3812,  0.5695])\n",
      "R[0]\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02179282274097204 0.004579743578513444 0.0041552534609072605 0.005341381406818982 0.18605021965503693 0.0005015524327754975 0.009029296346125194\n",
      "Average (on the epoch) training loss: 0.005598548699443927\n",
      "Episode average V value: 0\n",
      "epoch 7:\n",
      "Learning rate: 5.314410000000002e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0964,  0.3251,  0.3107, -0.4246,  0.1082,  0.6482, -0.0262, -0.3493,\n",
      "        -0.1183,  0.0455]) tensor([ 0.0955,  0.2962,  0.3101, -0.4087,  0.0711,  0.5980, -0.0027, -0.2921,\n",
      "        -0.0944,  0.0242]) tensor([ 0.0027,  0.2540,  0.0190, -0.3233,  0.1536,  0.7541,  0.0118, -0.5790,\n",
      "        -0.2817, -0.0488])\n",
      "R[0]\n",
      "tensor([-0.0048], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02170901247113943 0.006195935702649876 0.003874226060357614 0.005374644065392203 0.18644451463222503 0.0005340586453676224 0.008721186243521516\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0259, -0.4782, -0.2462, -0.0870, -0.5843,  0.6752, -0.4969,  0.3189,\n",
      "        -0.8252,  0.8383]) tensor([ 0.0730, -0.3514, -0.1377, -0.1873, -0.4891,  0.7216, -0.5021,  0.2654,\n",
      "        -0.7491,  0.7625]) tensor([-0.0215, -0.3726, -0.1811, -0.1608, -0.5401,  0.6937, -0.4819,  0.2549,\n",
      "        -0.7915,  0.8240])\n",
      "R[0]\n",
      "tensor([-0.0077], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021858539490029215 0.006165886446840886 0.0046679621413059065 0.005586716216756031 0.18842878840863705 0.0004142814055085182 0.009590987765113823\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1097,  0.3456,  0.1489, -0.4253,  0.0776,  0.5993, -0.0258, -0.3291,\n",
      "        -0.3278,  0.1978]) tensor([-0.0784,  0.3711,  0.1730, -0.4470,  0.1040,  0.5917, -0.0279, -0.3375,\n",
      "        -0.2869,  0.1467]) tensor([-0.1723,  0.2618, -0.1721, -0.3041,  0.0600,  0.6265,  0.0776, -0.5117,\n",
      "        -0.2994, -0.1758])\n",
      "R[0]\n",
      "tensor([-0.0026], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021927108265459537 0.0054713312925232455 0.004565844896183989 0.005692984951194376 0.18659919571876527 0.00040948191285133365 0.009145016235066578\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3835,  0.7910,  0.7194, -0.7173,  0.4387,  0.8809, -0.2500, -0.2867,\n",
      "        -0.0693,  0.0413]) tensor([ 3.7341e-01,  7.8497e-01,  7.1046e-01, -7.1611e-01,  4.3505e-01,\n",
      "         8.7862e-01, -2.2892e-01, -2.8793e-01, -5.3740e-02, -2.0742e-05]) tensor([ 0.4814,  0.8867,  0.6822, -0.7298,  0.5614,  0.9537, -0.2263, -0.3642,\n",
      "         0.0334, -0.2484])\n",
      "R[0]\n",
      "tensor([-0.0040], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022014084039255977 0.006105660899556824 0.004865014441529638 0.005984574298723601 0.1869717490375042 0.0003305516168475151 0.00903960007423302\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0541, -0.1371,  0.4723, -0.2432, -0.7001, -0.4307,  0.3029,  0.8817,\n",
      "         0.4482, -0.3799]) tensor([ 0.0187, -0.1337,  0.4647, -0.2449, -0.6992, -0.4496,  0.3356,  0.8452,\n",
      "         0.4641, -0.3693]) tensor([-0.1306, -0.2262,  0.4612, -0.2535, -0.8419, -0.4905,  0.2584,  0.9299,\n",
      "         0.3217,  0.0043])\n",
      "R[0]\n",
      "tensor([0.0124], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02222368222847581 0.005596001391561003 0.005198689341112186 0.0057815017020329835 0.18627261421084404 0.00026986756175756453 0.009321871997788549\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5055, -0.5548,  0.0905, -0.0532, -0.9599, -0.5776,  0.5742,  0.5878,\n",
      "         0.1845,  0.1586]) tensor([-0.3925, -0.4739,  0.1214, -0.0771, -0.8736, -0.5205,  0.5307,  0.5700,\n",
      "         0.2139,  0.0506]) tensor([-0.5082, -0.5887, -0.0905,  0.0216, -0.8881, -0.5096,  0.6650,  0.4207,\n",
      "         0.1958, -0.1184])\n",
      "R[0]\n",
      "tensor([-0.0008], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021739903660491108 0.006094461845495971 0.004851554947330442 0.006042952962568961 0.1873814621269703 0.0002394353523850441 0.009288912956428249\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5787, -0.5993, -0.4526,  0.1201, -0.7526, -0.1595,  0.5719, -0.1597,\n",
      "        -0.0016, -0.1510]) tensor([-0.5368, -0.5576, -0.4247,  0.1095, -0.7107, -0.1291,  0.5352, -0.1644,\n",
      "        -0.0123, -0.1357]) tensor([-0.3113, -0.1513, -0.3406, -0.0849, -0.2595,  0.3700,  0.3248, -0.5119,\n",
      "        -0.1464, -0.2758])\n",
      "R[0]\n",
      "tensor([0.0136], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02171926626935601 0.005581615691087791 0.004517534124355734 0.005601845358032733 0.18585672433674336 0.00032996931672096255 0.009313450129935518\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4622, -0.2365,  0.6023, -0.3657, -0.8651, -0.5308,  0.3345,  0.7304,\n",
      "         0.2551,  0.7272]) tensor([-0.4530, -0.2707,  0.5477, -0.3250, -0.8747, -0.5366,  0.3495,  0.7257,\n",
      "         0.2386,  0.6961]) tensor([-0.4607, -0.2432,  0.6011, -0.3628, -0.8688, -0.5375,  0.3420,  0.7390,\n",
      "         0.2603,  0.7219])\n",
      "R[0]\n",
      "tensor([-0.0028], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021645026944577695 0.004879609849776898 0.0036997971216806034 0.0052745526985963805 0.18639102387428283 0.000298476442694664 0.00945931573503185\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3769,  0.0207,  0.4458, -0.4251, -0.6280, -0.1516,  0.0521,  0.4859,\n",
      "        -0.0939,  0.5752]) tensor([-0.3665,  0.0308,  0.4182, -0.4090, -0.6000, -0.1133,  0.0685,  0.4401,\n",
      "        -0.0875,  0.5164]) tensor([-0.3390,  0.1810,  0.5277, -0.5439, -0.4791,  0.0206, -0.0073,  0.2910,\n",
      "        -0.0551,  0.5528])\n",
      "R[0]\n",
      "tensor([-0.0034], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021576126465573907 0.0056064180365283395 0.00453623386298932 0.0052346561200683935 0.18861067456007002 0.00030919121205806734 0.009263665545731783\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6216,  0.3576,  0.6563, -0.3497, -0.0521,  0.6187, -0.8222,  0.3827,\n",
      "        -0.5088,  0.7054]) tensor([ 0.5721,  0.3447,  0.6575, -0.3568, -0.0544,  0.6003, -0.7218,  0.3483,\n",
      "        -0.4405,  0.6402]) tensor([ 0.6136,  0.3787,  0.5917, -0.3569,  0.0093,  0.6409, -0.7830,  0.3397,\n",
      "        -0.5700,  0.5913])\n",
      "R[0]\n",
      "tensor([0.0285], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021847043927758932 0.005379075591568835 0.0045715678861233755 0.005400181534118019 0.18675452288985253 0.00033167414367198946 0.009543666276731528\n",
      "Average (on the epoch) training loss: 0.005597460990748368\n",
      "Episode average V value: 0\n",
      "epoch 8:\n",
      "Learning rate: 4.782969000000002e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 1.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4069, -0.3916, -0.5042,  0.0998, -0.2394,  0.5343,  0.0141,  0.0510,\n",
      "        -0.1718, -0.6849]) tensor([ 0.3402, -0.3262, -0.3790,  0.0187, -0.2757,  0.4975,  0.0447,  0.0864,\n",
      "        -0.0996, -0.6345]) tensor([-0.0291, -0.3362, -0.3606,  0.1217, -0.3141, -0.0366,  0.4303, -0.1330,\n",
      "        -0.0094, -0.6136])\n",
      "R[0]\n",
      "tensor([0.1039], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02234029170498252 0.005519960496530985 0.004221850222464127 0.005840006146812812 0.1848248127102852 0.00035403257608413694 0.009443174437968991\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0190,  0.0294, -0.1031, -0.1568,  0.0965,  0.7213,  0.0260, -0.5683,\n",
      "        -0.4222,  0.1649]) tensor([ 0.0384,  0.0496, -0.0645, -0.1751,  0.0928,  0.7032,  0.0260, -0.5304,\n",
      "        -0.3851,  0.1298]) tensor([-0.1008,  0.0235, -0.0330, -0.2337,  0.0343,  0.7577,  0.0040, -0.5087,\n",
      "        -0.4996,  0.4542])\n",
      "R[0]\n",
      "tensor([0.0144], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022170925721526145 0.006258204517172999 0.004106544648740965 0.005881819603149779 0.1874848427325487 0.0003486556261777878 0.008690702384512406\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3517,  0.3343,  0.6851, -0.4094,  0.0222,  0.5679, -0.4212,  0.0024,\n",
      "        -0.2346,  0.7613]) tensor([ 0.3002,  0.3251,  0.6670, -0.4070, -0.0012,  0.5588, -0.3756, -0.0130,\n",
      "        -0.1937,  0.7065]) tensor([ 0.0804,  0.0897,  0.2193, -0.2528, -0.1866,  0.6588, -0.4263, -0.1550,\n",
      "        -0.4628,  0.7627])\n",
      "R[0]\n",
      "tensor([0.0192], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02206291963905096 0.0058749278174873324 0.005120636644809565 0.005582197597599588 0.187660452991724 0.00036274956911802294 0.009418122292961925\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5646, -0.6312, -0.0650,  0.0493, -0.9021, -0.5392,  0.6741,  0.3928,\n",
      "         0.1023,  0.1293]) tensor([-0.4407, -0.5541, -0.0329,  0.0302, -0.8189, -0.4858,  0.6191,  0.3999,\n",
      "         0.1223,  0.0250]) tensor([-0.3180, -0.4267,  0.1564, -0.0747, -0.8510, -0.5108,  0.4855,  0.6582,\n",
      "         0.1936, -0.0440])\n",
      "R[0]\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02216142615862191 0.005512613053833775 0.004832299717916612 0.006030333052738569 0.1843801345974207 0.00030880236625671387 0.009349934098310768\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1202,  0.0805, -0.2623, -0.1469, -0.1103,  0.4083,  0.2987, -0.4678,\n",
      "         0.0267, -0.7391]) tensor([-0.0756,  0.0688, -0.2481, -0.1333, -0.1076,  0.4205,  0.2533, -0.4042,\n",
      "        -0.0184, -0.6853]) tensor([ 0.0810,  0.3341, -0.0576, -0.2787,  0.1404,  0.6077,  0.1692, -0.5212,\n",
      "         0.0252, -0.7636])\n",
      "R[0]\n",
      "tensor([0.0215], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02206965964846313 0.0054728582369425564 0.0038856743913929675 0.005654781173099764 0.18473796182870864 0.00032865258306264875 0.009686948797781952\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5107,  0.7832,  0.7244, -0.6404,  0.4046,  0.7771, -0.1776, -0.1131,\n",
      "         0.1532, -0.3520]) tensor([ 0.5141,  0.7796,  0.7251, -0.6423,  0.4096,  0.7676, -0.1635, -0.1077,\n",
      "         0.1641, -0.3742]) tensor([ 0.4818,  0.8841,  0.6293, -0.6718,  0.4903,  0.8372, -0.1791, -0.2281,\n",
      "         0.1271, -0.5398])\n",
      "R[0]\n",
      "tensor([0.0042], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022672663401812315 0.006454825936169073 0.0042104805866329115 0.006069307209807448 0.18081291835010052 0.0002689584121108055 0.008700699275708757\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2160, -0.0778, -0.3249, -0.0713, -0.1802,  0.3984,  0.3008, -0.5245,\n",
      "        -0.1251, -0.4004]) tensor([-0.1848, -0.0771, -0.2884, -0.0730, -0.1861,  0.3723,  0.2893, -0.4641,\n",
      "        -0.1172, -0.3786]) tensor([-0.3314, -0.2846, -0.4882,  0.0594, -0.3961,  0.1723,  0.4232, -0.4308,\n",
      "        -0.0379, -0.5057])\n",
      "R[0]\n",
      "tensor([-0.0148], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022260655561462044 0.0055030352361500265 0.0047436382679043165 0.006081540846382268 0.1810706291794777 0.00019729264080524443 0.008940240595664364\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3722, -0.1694, -0.5716, -0.0116, -0.2417,  0.3004,  0.4064, -0.6339,\n",
      "        -0.1387, -0.5021]) tensor([-0.3398, -0.1668, -0.5234, -0.0164, -0.2562,  0.2714,  0.3873, -0.5608,\n",
      "        -0.1349, -0.4613]) tensor([-0.2641, -0.1445, -0.5920,  0.0030, -0.1686,  0.3168,  0.4314, -0.6510,\n",
      "        -0.0868, -0.7025])\n",
      "R[0]\n",
      "tensor([-0.0050], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02227956986054778 0.006180797358269046 0.004775074960525672 0.0061834329644916575 0.1818090504705906 0.00024005982279777527 0.008919278410612606\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5252, -0.3674,  0.1395, -0.1599, -0.7287, -0.1650,  0.3210,  0.0156,\n",
      "        -0.0375,  0.5529]) tensor([-0.4507, -0.3051,  0.1479, -0.1619, -0.6582, -0.1024,  0.2823, -0.0205,\n",
      "        -0.0346,  0.4757]) tensor([-0.4334, -0.2303,  0.2477, -0.2529, -0.6109, -0.0541,  0.2387, -0.0422,\n",
      "         0.0023,  0.5483])\n",
      "R[0]\n",
      "tensor([-0.0095], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021918102413415907 0.004909504784925957 0.004226823575947492 0.005868490140652284 0.1848947515040636 0.0002827616035938263 0.008438178402604536\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4806, -0.3418,  0.2119, -0.1864, -0.7951, -0.2626,  0.2744,  0.4132,\n",
      "        -0.0975,  0.6082]) tensor([-0.4249, -0.2947,  0.2132, -0.1851, -0.7311, -0.2117,  0.2627,  0.3687,\n",
      "        -0.0887,  0.5379]) tensor([-0.4050, -0.0751,  0.3462, -0.3605, -0.6079, -0.0334,  0.0940,  0.2672,\n",
      "        -0.1209,  0.6294])\n",
      "R[0]\n",
      "tensor([-0.0049], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022305172495543958 0.005544680182632874 0.004037659159308532 0.0056509103458374736 0.18062706452608107 0.0003082744553685188 0.008419200864620507\n",
      "Average (on the epoch) training loss: 0.005884281908057164\n",
      "Episode average V value: 0\n",
      "epoch 9:\n",
      "Learning rate: 4.304672100000002e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5414, -0.6121,  0.0626, -0.0043, -0.9960, -0.5895,  0.6000,  0.5635,\n",
      "         0.1415,  0.2283]) tensor([-0.4277, -0.5397,  0.0845, -0.0187, -0.9091, -0.5435,  0.5666,  0.5552,\n",
      "         0.1675,  0.1072]) tensor([-0.5273, -0.6427, -0.1112,  0.0709, -0.9263, -0.5301,  0.6747,  0.4198,\n",
      "         0.1557, -0.0544])\n",
      "R[0]\n",
      "tensor([-0.0073], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022466423155739903 0.005509516130579868 0.004688504290203128 0.006181547747575678 0.1788155740350485 0.00027895911782979964 0.009086083060479724\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0348, -0.1896,  0.4734, -0.2344, -0.7758, -0.4241,  0.1825,  0.9914,\n",
      "         0.3020, -0.1377]) tensor([-0.0116, -0.1976,  0.4708, -0.2342, -0.7821, -0.4647,  0.2513,  0.9599,\n",
      "         0.3396, -0.1359]) tensor([ 0.1901, -0.0138,  0.6205, -0.3224, -0.6107, -0.2886,  0.1405,  0.9300,\n",
      "         0.3962, -0.3038])\n",
      "R[0]\n",
      "tensor([0.0201], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022405951289460064 0.006045982388022821 0.004733266137929604 0.006009355611400678 0.18020336082577706 0.0003825014010071754 0.009320722579257562\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0668, -0.1671,  0.4094, -0.1981, -0.6320, -0.4019,  0.3960,  0.8113,\n",
      "         0.4219, -0.4613]) tensor([ 0.0429, -0.1577,  0.4017, -0.2009, -0.6271, -0.4144,  0.4116,  0.7829,\n",
      "         0.4267, -0.4480]) tensor([ 0.2440,  0.0485,  0.6155, -0.3242, -0.4578, -0.2592,  0.3087,  0.8039,\n",
      "         0.5051, -0.5685])\n",
      "R[0]\n",
      "tensor([0.0026], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022330968217924237 0.005569848752515099 0.0040031251638174585 0.006231500435620547 0.1807981346398592 0.0004002843201160431 0.008399492092779837\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0456,  0.3494,  0.1958, -0.4262,  0.1963,  0.8230, -0.0967, -0.4938,\n",
      "        -0.3406,  0.1670]) tensor([ 0.0477,  0.3335,  0.2109, -0.4259,  0.1661,  0.7893, -0.0785, -0.4402,\n",
      "        -0.3106,  0.1376]) tensor([-0.0790,  0.2337, -0.1423, -0.2560,  0.1609,  0.7642,  0.0543, -0.7102,\n",
      "        -0.3380, -0.1652])\n",
      "R[0]\n",
      "tensor([0.0039], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02243798414990306 0.005913067697147199 0.00337313159657424 0.005898566811345518 0.18083135280013085 0.0002845601961016655 0.008110659276833758\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0603, -0.0315, -0.4022,  0.0092, -0.2501,  0.6858, -0.2536, -0.7565,\n",
      "        -0.2737, -0.1698]) tensor([-0.0324,  0.0097, -0.3332, -0.0337, -0.2134,  0.6808, -0.2535, -0.7604,\n",
      "        -0.2311, -0.1621]) tensor([-0.0598, -0.2037, -0.6208,  0.1895, -0.2882,  0.5143, -0.1137, -0.7077,\n",
      "        -0.3183, -0.2891])\n",
      "R[0]\n",
      "tensor([-0.0092], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022131394855678083 0.004410553122987039 0.004058281389687181 0.006339529334101826 0.18150783583521843 0.0003668022453784943 0.008650138364057057\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3365, -0.5109, -0.0176,  0.0187, -0.8102, -0.4748,  0.6314,  0.4953,\n",
      "         0.2485, -0.3094]) tensor([-0.3602, -0.5232, -0.0267,  0.0261, -0.8332, -0.4346,  0.5606,  0.4867,\n",
      "         0.1550, -0.1595]) tensor([-0.4232, -0.5349,  0.1460, -0.0471, -0.9432, -0.5643,  0.5515,  0.6481,\n",
      "         0.1876,  0.1303])\n",
      "R[0]\n",
      "tensor([-0.0069], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022512297289445996 0.005341712750603619 0.0032337797520194727 0.005972964157117531 0.1805753808170557 0.0002607810348272324 0.00885645305493381\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4619, -0.2928,  0.2133, -0.2095, -0.6778, -0.1441,  0.2914,  0.0250,\n",
      "         0.0245,  0.4989]) tensor([-0.3771, -0.2015,  0.2249, -0.2361, -0.5665, -0.0061,  0.1874, -0.0793,\n",
      "        -0.0207,  0.4792]) tensor([-0.3709, -0.1619,  0.2995, -0.2906, -0.5606, -0.0314,  0.2109, -0.0430,\n",
      "         0.0526,  0.4742])\n",
      "R[0]\n",
      "tensor([-0.0006], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02218209351040423 0.005288392540838686 0.0036223738485314243 0.0061889115599915384 0.1817661351263523 0.00047423071414232255 0.008632024559541606\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4326, -0.4881,  0.3140, -0.1251, -0.9673, -0.6275,  0.4651,  0.7790,\n",
      "         0.1551,  0.4635]) tensor([-0.3399, -0.4437,  0.3196, -0.1307, -0.9027, -0.6017,  0.4633,  0.7705,\n",
      "         0.1916,  0.3312]) tensor([-0.4879, -0.5943,  0.0970, -0.0053, -0.9724, -0.5956,  0.5806,  0.6076,\n",
      "         0.1350,  0.2255])\n",
      "R[0]\n",
      "tensor([-0.0006], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02214343515969813 0.006043027222905948 0.0031089672768684976 0.006091138598392718 0.18355653497576713 0.00038787014782428744 0.008245376003324055\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2233,  0.0762,  0.3862, -0.4014, -0.3223,  0.1971,  0.0780, -0.2481,\n",
      "         0.0636,  0.3802]) tensor([-0.2287,  0.0376,  0.3398, -0.3666, -0.3488,  0.1824,  0.0913, -0.2308,\n",
      "         0.0592,  0.3429]) tensor([-0.1524,  0.1659,  0.4475, -0.4519, -0.2474,  0.2513,  0.0346, -0.2516,\n",
      "         0.0836,  0.3420])\n",
      "R[0]\n",
      "tensor([-0.0064], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02254354008100927 0.0060603794342459875 0.0028387623710696063 0.006408818590804003 0.18243385256826877 0.0004078816846013069 0.008723897539428436\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3455, -0.4946,  0.2054, -0.0595, -0.9449, -0.5790,  0.4660,  0.7467,\n",
      "         0.1761,  0.1401]) tensor([-0.3610, -0.4811,  0.1941, -0.0565, -0.9236, -0.5710,  0.4856,  0.7017,\n",
      "         0.1760,  0.1448]) tensor([-0.3872, -0.4566,  0.3150, -0.1321, -0.9656, -0.6075,  0.4189,  0.8139,\n",
      "         0.1666,  0.3753])\n",
      "R[0]\n",
      "tensor([-0.0025], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022751835653558372 0.005797649406340497 0.001699993287678808 0.0061252061650156975 0.18234160673618316 0.0004930963143706322 0.008779835117049516\n",
      "Average (on the epoch) training loss: 0.0061447539011365735\n",
      "Episode average V value: 0\n",
      "epoch 10:\n",
      "Learning rate: 3.874204890000002e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0247,  0.3588,  0.6015, -0.5753, -0.1206,  0.3244, -0.0485, -0.1904,\n",
      "         0.1303,  0.2586]) tensor([-0.0444,  0.3050,  0.5473, -0.5369, -0.1592,  0.3004, -0.0214, -0.1737,\n",
      "         0.1257,  0.2274]) tensor([-0.0217,  0.3653,  0.6042, -0.5792, -0.1156,  0.3279, -0.0504, -0.1910,\n",
      "         0.1311,  0.2537])\n",
      "R[0]\n",
      "tensor([0.0044], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022345048636198045 0.006034122462602681 0.001977261825448295 0.00613837550021708 0.18270260606706143 0.00047258005291223525 0.008300914993626066\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1469,  0.2778,  0.7953, -0.5657, -0.2341, -0.0536,  0.1659,  0.7337,\n",
      "         0.2407,  0.0458]) tensor([ 0.0767,  0.1648,  0.6935, -0.5005, -0.3287, -0.1131,  0.1793,  0.7563,\n",
      "         0.1784,  0.1065]) tensor([ 0.3654,  0.4424,  0.8025, -0.5649,  0.0530,  0.2602,  0.0424,  0.4909,\n",
      "         0.1605, -0.0313])\n",
      "R[0]\n",
      "tensor([0.0115], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022363340916112064 0.005849359467618342 0.0018901833964737307 0.0057706893233116716 0.18202364078164102 0.0004655326306819916 0.008759664053621236\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2751,  0.0058,  0.6055, -0.2999, -0.4674, -0.2565,  0.2992,  0.8326,\n",
      "         0.4697, -0.5498]) tensor([ 0.3725,  0.1758,  0.6784, -0.3807, -0.3302, -0.0964,  0.2033,  0.7274,\n",
      "         0.4864, -0.6500]) tensor([ 0.3867,  0.2124,  0.7635, -0.4106, -0.2947, -0.1026,  0.2231,  0.7317,\n",
      "         0.5287, -0.6297])\n",
      "R[0]\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02185862536355853 0.005503887795690389 0.002127011708520513 0.005720746549544856 0.185478382691741 0.0005209253430366516 0.008877143937395885\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1237,  0.1970,  0.0520, -0.2741, -0.1626,  0.6157, -0.2626, -0.5959,\n",
      "        -0.2705,  0.2609]) tensor([-0.0703,  0.2396,  0.0928, -0.3037, -0.1088,  0.6248, -0.2696, -0.6179,\n",
      "        -0.2221,  0.2031]) tensor([-0.0543,  0.1706,  0.0179, -0.2515, -0.2066,  0.6145, -0.2522, -0.6489,\n",
      "        -0.0908,  0.0133])\n",
      "R[0]\n",
      "tensor([-0.0046], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02218764516711235 0.005904159878948122 0.0017884960357023374 0.005842952723382041 0.18371930703520775 0.0005360570847988129 0.008870866678480525\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2261,  0.3340,  0.6734, -0.4534, -0.0259,  0.3833, -0.0260,  0.2087,\n",
      "         0.0116,  0.1204]) tensor([ 0.1839,  0.2444,  0.6178, -0.4091, -0.1029,  0.3054,  0.0055,  0.2554,\n",
      "         0.0094,  0.1331]) tensor([ 0.4155,  0.4019,  0.5608, -0.3738,  0.1760,  0.5814, -0.0781, -0.0147,\n",
      "        -0.0112, -0.0909])\n",
      "R[0]\n",
      "tensor([-0.0064], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022020296847447753 0.005498325591353932 0.001842780498584034 0.005370365533977747 0.18460029700398445 0.0003977776318788528 0.008516368009732106\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5389,  0.8902,  0.5569, -0.6453,  0.5945,  0.9300, -0.1872, -0.4189,\n",
      "         0.1054, -0.5907]) tensor([ 0.5366,  0.8939,  0.5699, -0.6579,  0.6018,  0.9360, -0.1913, -0.4101,\n",
      "         0.1000, -0.5792]) tensor([ 0.5971,  0.9463,  0.5653, -0.6616,  0.6477,  0.9506, -0.1839, -0.4420,\n",
      "         0.1759, -0.7345])\n",
      "R[0]\n",
      "tensor([-0.0115], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02229327053390443 0.005519758550261031 0.00262236245924214 0.006115251609240658 0.18216812618076803 0.00039498021453619004 0.009381163052050397\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3905,  0.7903,  0.4078, -0.6129,  0.5404,  0.8646, -0.0258, -0.3826,\n",
      "         0.0814, -0.7154]) tensor([ 0.4056,  0.8066,  0.4351, -0.6266,  0.5554,  0.8743, -0.0423, -0.3638,\n",
      "         0.0743, -0.7018]) tensor([ 0.4557,  0.8130,  0.4085, -0.5913,  0.5765,  0.8751, -0.0430, -0.4222,\n",
      "         0.1153, -0.7867])\n",
      "R[0]\n",
      "tensor([-0.0078], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02182792764902115 0.006051859314902685 0.0015554665673225827 0.005573937638779171 0.18476127327978611 0.0003651963397860527 0.009151101028546691\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4544, -0.6249, -0.6718,  0.4128, -0.2779, -0.1034,  0.1762,  0.4151,\n",
      "        -0.3999, -0.6481]) tensor([ 0.3712, -0.5013, -0.5084,  0.2878, -0.2811, -0.0864,  0.1847,  0.3820,\n",
      "        -0.3175, -0.5594]) tensor([ 0.5078, -0.4854, -0.6758,  0.3764, -0.1901,  0.1694, -0.0432,  0.1712,\n",
      "        -0.4825, -0.5475])\n",
      "R[0]\n",
      "tensor([0.1192], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022263082202523946 0.005680856181614217 0.002135708124918892 0.005456877100979909 0.18372823248803616 0.0004291839376091957 0.008500984491896816\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3626, -0.5448, -0.4920,  0.2591, -0.3276, -0.2360,  0.3317,  0.4471,\n",
      "        -0.0666, -0.7666]) tensor([ 0.3132, -0.4610, -0.3832,  0.1742, -0.3284, -0.2283,  0.3185,  0.4336,\n",
      "        -0.0253, -0.6767]) tensor([ 0.4701, -0.5187, -0.4064,  0.2973, -0.3014, -0.3743,  0.3457,  0.5785,\n",
      "        -0.0115, -0.8862])\n",
      "R[0]\n",
      "tensor([0.0704], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.022356057612225415 0.005656869382000877 0.0019337003274158633 0.005753824687912129 0.18192069140076636 0.00046454115957021713 0.00872380521701416\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5447, -0.6563, -0.0360,  0.0555, -0.9640, -0.5578,  0.6588,  0.4628,\n",
      "         0.1381,  0.1213]) tensor([-0.4881, -0.5865, -0.0116,  0.0413, -0.8901, -0.5042,  0.6250,  0.4215,\n",
      "         0.1326,  0.0976]) tensor([-0.5200, -0.6036,  0.1433, -0.0270, -1.0010, -0.6214,  0.5772,  0.6173,\n",
      "         0.1503,  0.3599])\n",
      "R[0]\n",
      "tensor([-0.0090], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021693703554570676 0.006105075049152219 0.002086439870885442 0.005981158529408276 0.1861105090379715 0.000420895129442215 0.008952664405107499\n",
      "Average (on the epoch) training loss: 0.005772417919675354\n",
      "Episode average V value: 0\n",
      "epoch 11:\n",
      "Learning rate: 3.4867844010000016e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1574,  0.3067,  0.9060, -0.5970, -0.1681,  0.2490, -0.0076,  0.2805,\n",
      "         0.2209,  0.3653]) tensor([ 0.1482,  0.2934,  0.8527, -0.5787, -0.1662,  0.2715, -0.0031,  0.2482,\n",
      "         0.2115,  0.3128]) tensor([ 0.0347,  0.2371,  0.9424, -0.6163, -0.2647,  0.1921,  0.0069,  0.3845,\n",
      "         0.1675,  0.6012])\n",
      "R[0]\n",
      "tensor([-0.0066], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0218908657040447 0.005852792206183949 0.0017513459711899486 0.005730856928159483 0.18376985137164592 0.00052733463793993 0.009365263532148674\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4200, -0.4195, -0.3347,  0.0297, -0.2393,  0.5667,  0.1046,  0.0317,\n",
      "        -0.0176, -0.5697]) tensor([ 0.2967, -0.5661, -0.3408,  0.0647, -0.4311,  0.3851,  0.1655,  0.1992,\n",
      "        -0.0273, -0.4052]) tensor([-0.0737, -0.5876, -0.1470, -0.0850, -0.6899,  0.1780,  0.1745,  0.4581,\n",
      "        -0.1394,  0.1615])\n",
      "R[0]\n",
      "tensor([0.0023], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02191573273204267 0.006093495589419035 0.0016450714206257544 0.006112192308646627 0.185931425511837 0.0004235384911298752 0.008899306290026289\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0885, -0.0709,  0.7232, -0.4068, -0.5803, -0.3101,  0.3182,  0.6583,\n",
      "         0.4195,  0.2843]) tensor([-0.1148, -0.1040,  0.6453, -0.3714, -0.5996, -0.2815,  0.2752,  0.6163,\n",
      "         0.3358,  0.3287]) tensor([-0.0940, -0.0746,  0.7196, -0.4047, -0.5865, -0.3164,  0.3195,  0.6610,\n",
      "         0.4193,  0.2864])\n",
      "R[0]\n",
      "tensor([0.0077], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02177228886447847 0.006214575522426458 0.0022302098828386077 0.005807342440006323 0.18448656333982943 0.00037462939321994783 0.009033214173221496\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4398, -0.4715,  0.3895, -0.1732, -0.9436, -0.6547,  0.4791,  0.7906,\n",
      "         0.1939,  0.5952]) tensor([-0.3786, -0.3566,  0.3864, -0.2155, -0.8206, -0.4574,  0.3321,  0.6333,\n",
      "         0.0924,  0.6349]) tensor([-0.4308, -0.4254,  0.4465, -0.2177, -0.9271, -0.6371,  0.4476,  0.8037,\n",
      "         0.2125,  0.6382])\n",
      "R[0]\n",
      "tensor([0.0091], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0217072766572237 0.005283404762405553 0.0020265293177399142 0.0052438874712679535 0.18527887201309204 0.0003264310508966446 0.009549032189883292\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5654, -0.4758, -0.0905, -0.0288, -0.7463, -0.1621,  0.4007, -0.1010,\n",
      "        -0.0834,  0.3498]) tensor([-0.4858, -0.3663, -0.0372, -0.0780, -0.6356, -0.0287,  0.3009, -0.1938,\n",
      "        -0.1088,  0.3599]) tensor([-0.4605, -0.2745,  0.1436, -0.1892, -0.6157, -0.0543,  0.2594, -0.1050,\n",
      "        -0.0571,  0.4768])\n",
      "R[0]\n",
      "tensor([-0.0033], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02186597149260342 0.005631397858865967 0.0025055182397336465 0.005698152047814801 0.18573443979024887 0.0003298133388161659 0.009425244179903529\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2248,  0.0098,  0.6017, -0.3087, -0.4443, -0.1493,  0.3136,  0.6830,\n",
      "         0.4449, -0.5077]) tensor([ 0.2152,  0.0300,  0.5886, -0.3142, -0.4202, -0.1357,  0.3143,  0.6344,\n",
      "         0.4416, -0.5084]) tensor([-0.0370, -0.2773,  0.2867, -0.1334, -0.6236, -0.3160,  0.5332,  0.6139,\n",
      "         0.3603, -0.4707])\n",
      "R[0]\n",
      "tensor([0.0009], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02160957539640367 0.0059586288872596925 0.0019364857518412463 0.005740100670722313 0.18475335228443146 0.00034643459320068357 0.00948915195534937\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2945, -0.4938, -0.1630,  0.0855, -0.6821, -0.3712,  0.7172,  0.2926,\n",
      "         0.2779, -0.6398]) tensor([-0.2849, -0.4655, -0.1425,  0.0758, -0.6748, -0.3684,  0.6819,  0.3043,\n",
      "         0.2621, -0.5941]) tensor([-0.3410, -0.5302, -0.2845,  0.1343, -0.6743, -0.3559,  0.7509,  0.1911,\n",
      "         0.2587, -0.7247])\n",
      "R[0]\n",
      "tensor([-0.0175], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021337063996121286 0.005709584830088716 0.0015893086975574987 0.005177212753565982 0.1873982683867216 0.0002651502564549446 0.009351121000479906\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5564,  0.7432,  0.9799, -0.6764,  0.1360,  0.4445, -0.2257,  0.3215,\n",
      "         0.2600, -0.4263]) tensor([ 0.3657,  0.4341,  0.8307, -0.5395, -0.1485,  0.1630, -0.0552,  0.5087,\n",
      "         0.2618, -0.3207]) tensor([ 0.2239,  0.1190,  0.7296, -0.3883, -0.4547, -0.0934,  0.1319,  0.6925,\n",
      "         0.3896, -0.3125])\n",
      "R[0]\n",
      "tensor([-0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021276146946474908 0.005270919267983117 0.002757952925901918 0.0052998851460870355 0.18810846370458603 0.00028383032232522966 0.009045697015943006\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0522,  0.3307,  0.5176, -0.5554, -0.1171,  0.3511, -0.0442, -0.2394,\n",
      "         0.0824,  0.2550]) tensor([-0.0662,  0.2851,  0.4800, -0.5255, -0.1575,  0.3233, -0.0244, -0.2071,\n",
      "         0.0804,  0.2304]) tensor([-0.0308,  0.3642,  0.5384, -0.5759, -0.0910,  0.3699, -0.0570, -0.2358,\n",
      "         0.0874,  0.2358])\n",
      "R[0]\n",
      "tensor([-0.0035], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021588075598701836 0.005691500112705398 0.002106150152274495 0.00495116210100241 0.1845114116370678 0.00017420339584350587 0.009228949067532084\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1418,  0.1908,  0.4606, -0.4835, -0.2341,  0.2563,  0.0235, -0.2090,\n",
      "         0.0699,  0.3336]) tensor([-0.1605,  0.0923,  0.3729, -0.4160, -0.2851,  0.2279,  0.0430, -0.1809,\n",
      "         0.0077,  0.3570]) tensor([-0.4372, -0.2454,  0.2166, -0.2363, -0.6140, -0.0749,  0.2617, -0.0484,\n",
      "         0.0095,  0.4770])\n",
      "R[0]\n",
      "tensor([0.0033], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02158628677390516 0.005780145091666782 0.0018669238433794816 0.005177302374620922 0.18478624206781388 0.00024025345593690873 0.009832984638342169\n",
      "Average (on the epoch) training loss: 0.005493809424189385\n",
      "Episode average V value: 0\n",
      "epoch 12:\n",
      "Learning rate: 3.138105960900002e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2523,  0.1817, -0.2144, -0.2584, -0.0750,  0.6134, -0.0022, -0.6192,\n",
      "        -0.2925, -0.1041]) tensor([-0.2075,  0.1983, -0.1630, -0.2723, -0.0700,  0.5789,  0.0048, -0.5616,\n",
      "        -0.2506, -0.1405]) tensor([-0.2760,  0.1897,  0.0007, -0.3526, -0.1411,  0.5438, -0.0418, -0.4438,\n",
      "        -0.3055,  0.2006])\n",
      "R[0]\n",
      "tensor([-0.0053], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021780310628935696 0.006276743900350994 0.0016605839549993107 0.005136372101609595 0.18546766990423202 0.00035242697596549985 0.00900325253570918\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4355, -0.6887, -0.5692,  0.1924, -0.8582,  0.0563,  0.2581, -0.0125,\n",
      "        -0.1708,  0.0572]) tensor([-0.3957, -0.7009, -0.5343,  0.2007, -0.8588,  0.0195,  0.2891,  0.0531,\n",
      "        -0.1392,  0.0415]) tensor([-0.4414, -0.6942, -0.5819,  0.2154, -0.8610, -0.0063,  0.3012, -0.0078,\n",
      "        -0.1404, -0.0247])\n",
      "R[0]\n",
      "tensor([0.0050], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021213988048955797 0.005746817196508346 0.0018017177201800224 0.005050672311335802 0.18765893957018853 0.00039548126608133317 0.009261335529619828\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5434,  0.5276,  0.8856, -0.5331,  0.0149,  0.2182,  0.0319,  0.4444,\n",
      "         0.4643, -0.6778]) tensor([ 0.5269,  0.5996,  0.8791, -0.5700,  0.0871,  0.3176, -0.0095,  0.3103,\n",
      "         0.4331, -0.6642]) tensor([ 0.6218,  0.7055,  0.9218, -0.6067,  0.2000,  0.4070, -0.0801,  0.2782,\n",
      "         0.4001, -0.6832])\n",
      "R[0]\n",
      "tensor([-0.0077], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02112666442990303 0.0055425917984757685 0.0015050019319060085 0.005252295060898178 0.1886162827461958 0.00044293516874313355 0.009633484225952997\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4033, -0.2256, -0.5412,  0.0193, -0.4330,  0.4627,  0.0032, -0.5652,\n",
      "        -0.4127,  0.1194]) tensor([-0.3224, -0.1305, -0.4476, -0.0470, -0.3411,  0.5165, -0.0351, -0.5962,\n",
      "        -0.3642,  0.0886]) tensor([-0.4242, -0.3461, -0.6156,  0.0843, -0.5266,  0.3721,  0.0716, -0.4702,\n",
      "        -0.3767,  0.0812])\n",
      "R[0]\n",
      "tensor([-0.0040], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021366325033828616 0.005174554879493371 0.0020923039675435576 0.0053282966479891915 0.18549456740915776 0.0002809349149465561 0.0093420388745144\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0806,  0.0135, -0.5033, -0.0754, -0.0129,  0.4672,  0.3317, -0.6733,\n",
      "        -0.0408, -0.8863]) tensor([-0.0358,  0.0161, -0.4746, -0.0689, -0.0069,  0.4979,  0.2644, -0.6156,\n",
      "        -0.0963, -0.8055]) tensor([-0.2409,  0.0680, -0.4379, -0.1172, -0.1117,  0.5200,  0.1289, -0.7072,\n",
      "        -0.1940, -0.4518])\n",
      "R[0]\n",
      "tensor([0.0304], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021477319693192838 0.004900714633360621 0.001686411170534484 0.004808071103645489 0.185499085649848 0.0003744555115699768 0.009025649691233411\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4685,  0.6421,  0.8268, -0.6070,  0.1914,  0.7116, -0.3585,  0.0212,\n",
      "         0.0062,  0.1403]) tensor([ 0.4571,  0.6636,  0.8261, -0.6232,  0.2091,  0.7308, -0.3447, -0.0095,\n",
      "         0.0316,  0.0845]) tensor([ 0.4556,  0.6277,  0.8966, -0.6356,  0.1535,  0.5998, -0.2763,  0.1689,\n",
      "         0.0919,  0.0944])\n",
      "R[0]\n",
      "tensor([-0.0041], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02138007198832929 0.005510412016759801 0.001953167746502004 0.005082161356753204 0.18413823679089547 0.00031058170646429065 0.008988950910512357\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4432, -0.3918, -0.3565,  0.0340, -0.5756,  0.0197,  0.3188, -0.2692,\n",
      "        -0.1535,  0.0447]) tensor([-0.3952, -0.4301, -0.3586,  0.0622, -0.5888, -0.0197,  0.3288, -0.1881,\n",
      "        -0.1561,  0.0268]) tensor([-0.3813, -0.5950, -0.5850,  0.2497, -0.6003, -0.0533,  0.4024, -0.2044,\n",
      "        -0.2168, -0.0736])\n",
      "R[0]\n",
      "tensor([0.0118], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021209186751395465 0.005656192632573948 0.0018255766255188064 0.0051239277302520345 0.18598189191520215 0.00037408605217933657 0.008758248631260358\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4228, -0.1524,  0.6443, -0.3969, -0.7958, -0.4328,  0.2354,  0.6736,\n",
      "         0.1913,  0.7955]) tensor([-0.3819, -0.2029,  0.5858, -0.3542, -0.7960, -0.4484,  0.2805,  0.6825,\n",
      "         0.1971,  0.6997]) tensor([-0.4545, -0.3624,  0.4476, -0.2421, -0.9105, -0.5570,  0.3706,  0.7152,\n",
      "         0.1828,  0.6713])\n",
      "R[0]\n",
      "tensor([0.0122], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021502731591463087 0.006421437085060461 0.0018180146455815701 0.005451828079647385 0.18236105193197727 0.00015216206759214402 0.00974346619390417\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5611,  0.5431,  0.8777, -0.5271,  0.0594,  0.2006,  0.0774,  0.4203,\n",
      "         0.5026, -0.7626]) tensor([ 0.6141,  0.6610,  0.8893, -0.5706,  0.1813,  0.3666, -0.0273,  0.2733,\n",
      "         0.4404, -0.7653]) tensor([ 0.6235,  0.6976,  0.9100, -0.5907,  0.2166,  0.3558, -0.0162,  0.2766,\n",
      "         0.4483, -0.7717])\n",
      "R[0]\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021529822846874595 0.004919186163941049 0.0016525345860809466 0.004842474410892464 0.1839534868746996 0.0003223529160022736 0.009393233473005238\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1508, -0.0791,  0.7062, -0.4169, -0.6012, -0.3342,  0.3301,  0.6714,\n",
      "         0.3766,  0.3457]) tensor([-0.1630, -0.0799,  0.6356, -0.3979, -0.5871, -0.2692,  0.2691,  0.5956,\n",
      "         0.2935,  0.3765]) tensor([-0.1395, -0.0870,  0.7060, -0.4092, -0.6026, -0.3494,  0.3414,  0.6898,\n",
      "         0.3877,  0.3269])\n",
      "R[0]\n",
      "tensor([0.0079], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02137697228230536 0.005240341456221358 0.0016542610289670848 0.0049791058372939 0.1823432827591896 0.00029506920278072354 0.008696720472769811\n",
      "Average (on the epoch) training loss: 0.005105520464031724\n",
      "Episode average V value: 0\n",
      "epoch 13:\n",
      "Learning rate: 2.8242953648100018e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2809, -0.1139, -0.0990, -0.1271, -0.1690,  0.5988, -0.0758, -0.0459,\n",
      "        -0.2276, -0.1939]) tensor([ 0.3046, -0.0983, -0.0647, -0.1380, -0.1459,  0.6027, -0.0676, -0.0307,\n",
      "        -0.2097, -0.2036]) tensor([ 0.1398, -0.0481,  0.0039, -0.2254, -0.2452,  0.6135, -0.1585,  0.0019,\n",
      "        -0.3096,  0.0715])\n",
      "R[0]\n",
      "tensor([-0.0026], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021719031885266304 0.005762523027602583 0.001302655806799521 0.005170955971814692 0.18361528761684895 0.0004826270490884781 0.009481437228503637\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4457, -0.4125, -0.1902, -0.0346, -0.7600,  0.1126,  0.1237, -0.1906,\n",
      "        -0.1113,  0.2622]) tensor([-0.3870, -0.3404, -0.1481, -0.0614, -0.6854,  0.1476,  0.1116, -0.2181,\n",
      "        -0.0830,  0.2001]) tensor([-0.2403, -0.2230,  0.2129, -0.0839, -0.6044,  0.0541, -0.0356, -0.0201,\n",
      "        -0.1960,  0.6272])\n",
      "R[0]\n",
      "tensor([-0.0066], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021677688406780363 0.005449834841048869 0.0013722100333761773 0.0047821551150409505 0.18387444858253002 0.0003763125240802765 0.008132908566272818\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 1.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3366, -0.3941, -0.4599,  0.0031, -0.6056,  0.2062,  0.0149, -0.0893,\n",
      "        -0.4383,  0.1873]) tensor([-0.3071, -0.3236, -0.3850, -0.0460, -0.5453,  0.2221,  0.0312, -0.1068,\n",
      "        -0.3885,  0.1575]) tensor([-0.5404, -0.1466,  0.0320, -0.2194, -0.5085, -0.0698,  0.3199, -0.2560,\n",
      "        -0.0824,  0.2441])\n",
      "R[0]\n",
      "tensor([0.0606], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021799072539433837 0.005707321339752525 0.001221237766660124 0.004935502267675474 0.18211629593372344 0.0003797412887215614 0.008492643610865344\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2963,  0.6322,  0.1963, -0.4644,  0.4058,  0.8667, -0.1071, -0.5663,\n",
      "        -0.0494, -0.5583]) tensor([ 0.2597,  0.5675,  0.1437, -0.4187,  0.3779,  0.8727, -0.1233, -0.5603,\n",
      "        -0.1360, -0.4523]) tensor([ 0.4741,  0.8091,  0.3559, -0.5606,  0.5469,  0.9162, -0.1465, -0.5135,\n",
      "         0.0868, -0.7362])\n",
      "R[0]\n",
      "tensor([0.0026], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021623572766780855 0.005430409107771993 0.0013334987346788694 0.005126655739266426 0.1838944270014763 0.00037527942657470705 0.008503162434557452\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6062,  0.6199,  0.8976, -0.5480,  0.1512,  0.2820,  0.0336,  0.3286,\n",
      "         0.4892, -0.7677]) tensor([ 0.3548,  0.2485,  0.7353, -0.3859, -0.2167, -0.0628,  0.2258,  0.5737,\n",
      "         0.4767, -0.5845]) tensor([ 0.3541,  0.1334,  0.6574, -0.3304, -0.2850, -0.0995,  0.3230,  0.6182,\n",
      "         0.5476, -0.6888])\n",
      "R[0]\n",
      "tensor([-0.0035], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021787040572613478 0.006381235693617782 0.0009401713527495304 0.005156080872868188 0.18350471268594265 0.0004412887692451477 0.008961731589166447\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0734,  0.3393, -0.1836, -0.2453,  0.2126,  0.7251,  0.0917, -0.7827,\n",
      "        -0.0476, -0.7200]) tensor([ 0.0728,  0.3079, -0.2050, -0.2153,  0.2075,  0.7499,  0.0484, -0.7654,\n",
      "        -0.1256, -0.6239]) tensor([-0.0596,  0.2447, -0.2803, -0.1702,  0.1013,  0.7068,  0.0159, -0.8062,\n",
      "        -0.2204, -0.4319])\n",
      "R[0]\n",
      "tensor([0.0123], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02138148252852261 0.005519672052287206 0.0018099640456275665 0.004705350701638963 0.185111144348979 0.0003296026661992073 0.008993090850533918\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1315,  0.2372,  0.2511, -0.3775, -0.0563,  0.5448, -0.1495, -0.2798,\n",
      "        -0.4447,  0.6091]) tensor([-0.0692,  0.2801,  0.2798, -0.4013, -0.0046,  0.5540, -0.1568, -0.2948,\n",
      "        -0.3858,  0.5072]) tensor([-0.1645,  0.2025, -0.0088, -0.2892, -0.0007,  0.6168, -0.0423, -0.5307,\n",
      "        -0.3827,  0.2338])\n",
      "R[0]\n",
      "tensor([-0.0152], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02194583386182785 0.006111793526593828 0.001263508871790691 0.005186557136010379 0.17933215440809727 0.0002647895365953445 0.008805250713659916\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4446, -0.4198,  0.3487, -0.1714, -0.9413, -0.6057,  0.4882,  0.7235,\n",
      "         0.2601,  0.3717]) tensor([-0.4215, -0.4038,  0.3372, -0.1614, -0.9055, -0.5969,  0.5051,  0.6971,\n",
      "         0.2651,  0.3334]) tensor([-0.4955, -0.5315,  0.1055, -0.0294, -0.9174, -0.5620,  0.6304,  0.5087,\n",
      "         0.2502,  0.0954])\n",
      "R[0]\n",
      "tensor([0.0007], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021379217313602566 0.004378075435954088 0.001167065120587722 0.004631329633877613 0.1816442330777645 0.0003136982619762421 0.008939698207657784\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3492, -0.2203, -0.2300,  0.0906, -0.1554,  0.0717,  0.3395,  0.0237,\n",
      "         0.1359, -0.9568]) tensor([ 0.2713, -0.2374, -0.1996,  0.0689, -0.2229,  0.0179,  0.3386,  0.0666,\n",
      "         0.1254, -0.8263]) tensor([ 0.1711, -0.3553, -0.3697,  0.1230, -0.3229,  0.1818,  0.2785, -0.0781,\n",
      "         0.0311, -0.6857])\n",
      "R[0]\n",
      "tensor([-0.0014], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02165902374126017 0.006083513767451223 0.0015734563745245396 0.0052121798961889 0.18273621398210527 0.00038441197574138643 0.009142738405265846\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1320,  0.0766,  0.6083, -0.4014, -0.2365,  0.0587,  0.2553,  0.3447,\n",
      "         0.3175,  0.0059]) tensor([ 0.1258,  0.0760,  0.5604, -0.3908, -0.2194,  0.0846,  0.2470,  0.2966,\n",
      "         0.2949, -0.0216]) tensor([ 0.0485,  0.0126,  0.6862, -0.4122, -0.3737, -0.1143,  0.3068,  0.5183,\n",
      "         0.3690,  0.1216])\n",
      "R[0]\n",
      "tensor([-0.0063], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02159141733497381 0.0062332509146508525 0.0013741090564653858 0.005050387311610393 0.18011061860620975 0.0003683145120739937 0.008723569430992938\n",
      "Average (on the epoch) training loss: 0.004995715464599198\n",
      "Episode average V value: 0\n",
      "epoch 14:\n",
      "Learning rate: 2.5418658283290016e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6473,  0.9562,  0.5974, -0.6771,  0.6534,  0.9350, -0.2071, -0.3911,\n",
      "         0.2030, -0.7258]) tensor([ 0.5003,  0.6769,  0.5055, -0.5622,  0.3815,  0.6449, -0.0393, -0.1632,\n",
      "         0.2577, -0.6625]) tensor([ 0.5171,  0.7261,  0.7156, -0.6440,  0.4106,  0.7104, -0.1104, -0.1226,\n",
      "         0.1831, -0.3532])\n",
      "R[0]\n",
      "tensor([0.0008], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02175390463322401 0.006388009273498028 0.0008948371018996113 0.004943962254677899 0.18079099641740323 0.0003729144260287285 0.009107013491040561\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1625, -0.0815,  0.4597, -0.2189, -0.4340, -0.2514,  0.5047,  0.6233,\n",
      "         0.5247, -0.6480]) tensor([ 0.0094, -0.2417,  0.3787, -0.1524, -0.6277, -0.3887,  0.5410,  0.7227,\n",
      "         0.4506, -0.4643]) tensor([ 0.1761, -0.1331,  0.5143, -0.2318, -0.5594, -0.4266,  0.4118,  0.8879,\n",
      "         0.5032, -0.4643])\n",
      "R[0]\n",
      "tensor([-0.0107], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02139089610427618 0.0060417858885921305 0.0011518681790439586 0.004880425469134934 0.18146160198748112 0.00024373573809862136 0.0094252412088681\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0937,  0.3463,  0.6447, -0.4734, -0.0073,  0.2930,  0.0457,  0.2283,\n",
      "        -0.1198,  0.2606]) tensor([ 0.0966,  0.3289,  0.6234, -0.4585, -0.0179,  0.2743,  0.0524,  0.2288,\n",
      "        -0.1021,  0.2184]) tensor([ 0.2581,  0.5156,  0.6070, -0.5204,  0.2341,  0.6368, -0.0987, -0.0719,\n",
      "        -0.1592,  0.1197])\n",
      "R[0]\n",
      "tensor([-0.0120], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021844699347391725 0.0056325598906187225 0.0013719326270620513 0.0050269939401187 0.17881545512378216 0.00026508210599422455 0.008840864845551551\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6725, -0.2588,  0.0594, -0.2632, -0.1802,  0.6869, -0.3728,  0.6962,\n",
      "        -0.3636,  0.0430]) tensor([ 0.7411, -0.0843,  0.2227, -0.3681, -0.0519,  0.7533, -0.3374,  0.6222,\n",
      "        -0.2103, -0.1073]) tensor([ 0.7282, -0.1781,  0.2999, -0.3280, -0.1443,  0.5693, -0.3137,  0.7469,\n",
      "        -0.1668,  0.0762])\n",
      "R[0]\n",
      "tensor([0.0378], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02174684643186629 0.005634811404750507 0.0005102366326627816 0.004950961759779602 0.18146123388409616 0.00026654715836048126 0.008956781852291897\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4910,  0.6986,  0.8780, -0.6571,  0.2631,  0.7054, -0.3063,  0.0021,\n",
      "         0.0852,  0.0807]) tensor([ 0.4849,  0.7156,  0.8761, -0.6690,  0.2759,  0.7194, -0.2967, -0.0213,\n",
      "         0.1066,  0.0267]) tensor([ 0.4286,  0.5875,  0.7395, -0.5691,  0.1968,  0.7274, -0.3067, -0.1280,\n",
      "         0.0456,  0.1306])\n",
      "R[0]\n",
      "tensor([-0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021580438086763024 0.005862933693286322 0.0015816931126155396 0.005268337788875215 0.18434202149510384 0.0004163787290453911 0.008474369717529044\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4369, -0.4501, -0.2321, -0.0484, -0.6136,  0.1210,  0.3003, -0.0761,\n",
      "        -0.1857,  0.3012]) tensor([-0.3681, -0.3756, -0.1744, -0.0919, -0.5434,  0.1804,  0.2420, -0.1033,\n",
      "        -0.1770,  0.2907]) tensor([-0.0338,  0.0365, -0.3227, -0.1689, -0.0231,  0.6081,  0.0828, -0.4083,\n",
      "        -0.2890, -0.3299])\n",
      "R[0]\n",
      "tensor([-0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02135344152711332 0.004418051873399236 0.0010961198895874987 0.0046767301845829935 0.18243623389303684 0.00039540063589811326 0.008858882911736145\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3149, -0.1534, -0.4896,  0.0025, -0.3852,  0.5555, -0.1272, -0.6604,\n",
      "        -0.4189,  0.1088]) tensor([-0.2419, -0.0771, -0.4084, -0.0510, -0.3073,  0.5926, -0.1543, -0.6814,\n",
      "        -0.3753,  0.0817]) tensor([-0.3442, -0.2919, -0.6184,  0.0985, -0.4626,  0.4321,  0.0018, -0.6000,\n",
      "        -0.3680,  0.0095])\n",
      "R[0]\n",
      "tensor([-0.0076], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02166389151290059 0.0057266311348575984 0.001375984927830359 0.00500019124802202 0.1830651675015688 0.00037252024561166766 0.008844862556492444\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.7181,  0.9015,  0.8721, -0.6840,  0.4857,  0.6987, -0.1482, -0.0534,\n",
      "         0.3964, -0.7862]) tensor([ 0.7031,  0.9357,  0.8764, -0.7030,  0.5223,  0.7310, -0.1540, -0.1160,\n",
      "         0.3914, -0.7859]) tensor([ 0.7357,  0.9403,  0.8538, -0.7017,  0.5365,  0.7771, -0.1835, -0.1268,\n",
      "         0.3776, -0.7831])\n",
      "R[0]\n",
      "tensor([-0.0008], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021907478109002114 0.005529680499836104 0.0012827138345637649 0.005214479380869307 0.1791915548592806 0.0004517033025622368 0.008756922856206075\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2470,  0.1474,  0.7710, -0.4509, -0.1928,  0.0342,  0.2488,  0.4573,\n",
      "         0.4120, -0.0384]) tensor([ 0.3542,  0.2650,  0.7545, -0.4849, -0.0438,  0.2057,  0.1409,  0.3075,\n",
      "         0.3648, -0.1346]) tensor([ 0.1738,  0.1601,  0.3699, -0.3558, -0.0245,  0.3543,  0.1537, -0.0535,\n",
      "         0.1207, -0.0973])\n",
      "R[0]\n",
      "tensor([0.0038], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021567500963807107 0.005604430060790037 0.001333629731210749 0.004784276928287 0.18078803846240044 0.00022558712214231492 0.008941009298083372\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5395,  0.7107,  0.7499, -0.6411,  0.4280,  0.6971, -0.0750, -0.1202,\n",
      "         0.2156, -0.3343]) tensor([ 0.5457,  0.7264,  0.7490, -0.6524,  0.4436,  0.7054, -0.0840, -0.1301,\n",
      "         0.2176, -0.3657]) tensor([ 0.6275,  0.8823,  0.6615, -0.6724,  0.6017,  0.8690, -0.1661, -0.3130,\n",
      "         0.2000, -0.5683])\n",
      "R[0]\n",
      "tensor([-0.0098], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02136018050648272 0.004870171188296808 0.0011458874495274358 0.004904166320106014 0.18245164734125138 0.00020504382997751236 0.009131676069344393\n",
      "Average (on the epoch) training loss: 0.004965052527445369\n",
      "Episode average V value: 0\n",
      "epoch 15:\n",
      "Learning rate: 2.2876792454961016e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.7656,  0.9826,  0.8325, -0.7166,  0.5983,  0.8726, -0.2311, -0.2173,\n",
      "         0.3520, -0.7708]) tensor([ 0.7620,  0.9791,  0.8381, -0.7184,  0.6047,  0.8475, -0.2084, -0.2115,\n",
      "         0.3701, -0.7910]) tensor([ 0.7688,  0.9906,  0.8275, -0.7212,  0.6086,  0.8906, -0.2388, -0.2338,\n",
      "         0.3491, -0.7704])\n",
      "R[0]\n",
      "tensor([0.0012], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021311094967648388 0.005003722605259099 0.0011379112805425394 0.004459344425064046 0.17907493448257447 0.0003100198283791542 0.008918409514939413\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4485,  0.2750,  0.7609, -0.4133, -0.1547,  0.0249,  0.2435,  0.5685,\n",
      "         0.5629, -0.7175]) tensor([ 0.4432,  0.3206,  0.7491, -0.4331, -0.1054,  0.0812,  0.2168,  0.4772,\n",
      "         0.5445, -0.7099]) tensor([ 0.5411,  0.4601,  0.8452, -0.4925,  0.0147,  0.1748,  0.1344,  0.4469,\n",
      "         0.5377, -0.7533])\n",
      "R[0]\n",
      "tensor([0.0013], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021619128530845046 0.005630335787063814 0.0007032134165547176 0.004977959305222612 0.17689010781049727 0.0003388891741633415 0.008742144799907692\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2404,  0.4613,  0.2671, -0.4487,  0.3203,  0.8061, -0.0375, -0.4824,\n",
      "        -0.1066, -0.2018]) tensor([ 0.2555,  0.4572,  0.2876, -0.4534,  0.3025,  0.7770, -0.0393, -0.4245,\n",
      "        -0.0917, -0.2268]) tensor([ 0.2410,  0.3211,  0.4102, -0.4373,  0.1502,  0.5652,  0.0929, -0.2081,\n",
      "         0.0882, -0.1546])\n",
      "R[0]\n",
      "tensor([-0.0041], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02146684361808002 0.00543788614922596 0.0009551593809364931 0.004788657656230498 0.1784946545958519 0.0003718378469347954 0.009026629681815394\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0978,  0.4873, -0.0789, -0.3480,  0.2106,  0.7291, -0.0464, -0.5792,\n",
      "        -0.0804, -0.6770]) tensor([ 0.0868,  0.4478, -0.0975, -0.3175,  0.2008,  0.7428, -0.0652, -0.5654,\n",
      "        -0.1453, -0.5838]) tensor([-0.0369,  0.3438, -0.2224, -0.2571,  0.1231,  0.7152, -0.0025, -0.6988,\n",
      "        -0.1669, -0.5442])\n",
      "R[0]\n",
      "tensor([0.0097], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021196357049047945 0.005061533326384961 0.0008540876194783778 0.004687982636736706 0.18020132094621658 0.00026019052416086195 0.009269087999593466\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0303, -0.1029,  0.4240, -0.2295, -0.4740, -0.2510,  0.5388,  0.5402,\n",
      "         0.4954, -0.5598]) tensor([ 0.0278, -0.0908,  0.4092, -0.2286, -0.4599, -0.2447,  0.5288,  0.5148,\n",
      "         0.4835, -0.5509]) tensor([ 0.3302,  0.1717,  0.7031, -0.3889, -0.2583, -0.0569,  0.3335,  0.5884,\n",
      "         0.5776, -0.6694])\n",
      "R[0]\n",
      "tensor([-0.0077], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020846693173050882 0.005470798892529274 0.0011750733667131499 0.004631075008539483 0.1841920365691185 0.0003484874740242958 0.009201579377870075\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2749,  0.1894,  0.7862, -0.4809, -0.1547,  0.0962,  0.2224,  0.4413,\n",
      "         0.4114, -0.0650]) tensor([ 0.3862,  0.3205,  0.7775, -0.5225,  0.0068,  0.2787,  0.1072,  0.2823,\n",
      "         0.3643, -0.1589]) tensor([ 0.1797,  0.2042,  0.3861, -0.3922,  0.0079,  0.4204,  0.1311, -0.0803,\n",
      "         0.1178, -0.0979])\n",
      "R[0]\n",
      "tensor([-0.0019], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020907451948150992 0.005279633375888807 0.0007976455837606409 0.004629319114959798 0.17935230074822903 0.000340679369866848 0.008921313681639731\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1781,  0.2623,  0.8633, -0.4861, -0.2757,  0.4067, -0.4285,  0.4250,\n",
      "        -0.2013,  0.9404]) tensor([ 0.1441,  0.1966,  0.8009, -0.4472, -0.3042,  0.3996, -0.3682,  0.4068,\n",
      "        -0.1986,  0.9043]) tensor([ 0.2964,  0.3819,  0.8778, -0.5312, -0.1366,  0.4645, -0.4583,  0.4414,\n",
      "        -0.2560,  0.7591])\n",
      "R[0]\n",
      "tensor([-0.0003], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02099722616560757 0.005121258164632309 0.0009912112346701178 0.004188684303197078 0.17925032116472722 0.00029206311702728273 0.00919538593501784\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3662, -0.6255, -0.5002,  0.0016, -0.3666,  0.5321,  0.0033,  0.4682,\n",
      "        -0.3675, -0.2940]) tensor([ 0.2982, -0.6806, -0.4311, -0.0196, -0.4895,  0.3873,  0.0561,  0.6242,\n",
      "        -0.3255, -0.2087]) tensor([ 0.6670, -0.4005, -0.1407, -0.1349, -0.1432,  0.5895, -0.1287,  0.5879,\n",
      "        -0.2379, -0.2612])\n",
      "R[0]\n",
      "tensor([0.0023], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021513648699969055 0.005094165812290157 0.0010406359397402412 0.00443425616127206 0.17938320070505143 0.0003498265817761421 0.009148764528450556\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2115, -0.1299,  0.5704, -0.2651, -0.5895, -0.3889,  0.3119,  0.9621,\n",
      "         0.4545, -0.3635]) tensor([ 0.3046,  0.0262,  0.6484, -0.3365, -0.4663, -0.2544,  0.2470,  0.8659,\n",
      "         0.4981, -0.4758]) tensor([ 0.3489,  0.0790,  0.7355, -0.3739, -0.3948, -0.2029,  0.2628,  0.8399,\n",
      "         0.5479, -0.5034])\n",
      "R[0]\n",
      "tensor([0.0047], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021389484794810416 0.005063033747377631 0.0008862620782056183 0.004460820308886468 0.1790846225619316 0.00029115236550569535 0.009445815709186718\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4600,  0.7598,  0.4996, -0.6015,  0.4997,  0.8806, -0.1545, -0.4030,\n",
      "         0.0531, -0.4311]) tensor([ 0.4633,  0.7609,  0.5166, -0.6096,  0.4930,  0.8587, -0.1486, -0.3731,\n",
      "         0.0684, -0.4519]) tensor([ 0.3555,  0.5136,  0.5671, -0.5392,  0.2609,  0.6105,  0.0058, -0.1436,\n",
      "         0.1335, -0.2403])\n",
      "R[0]\n",
      "tensor([-0.0040], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021415250631049276 0.005125660581157717 0.0009290497884021534 0.004482299676863476 0.17951168882846832 0.00036475498974323275 0.009213313037354965\n",
      "Average (on the epoch) training loss: 0.0045740398596972225\n",
      "Episode average V value: 0\n",
      "epoch 16:\n",
      "Learning rate: 2.0589113209464913e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3336,  0.2779,  0.7073, -0.4793, -0.0291,  0.2389,  0.1683,  0.2850,\n",
      "         0.3636, -0.1896]) tensor([ 0.3475,  0.3284,  0.6891, -0.4999,  0.0286,  0.3104,  0.1275,  0.2009,\n",
      "         0.3406, -0.2177]) tensor([ 0.2588,  0.1764,  0.7637, -0.4675, -0.1700,  0.0771,  0.2261,  0.4490,\n",
      "         0.3986, -0.0670])\n",
      "R[0]\n",
      "tensor([-0.0046], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021711725322529674 0.00635126769314229 0.0007819106202150578 0.004735687180655077 0.18164812514185905 0.00041497774422168734 0.008982731656578835\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 1.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4293, -0.1952, -0.3465, -0.1763, -0.3511,  0.3621,  0.3175, -0.3795,\n",
      "        -0.1507, -0.0767]) tensor([-0.3688, -0.1672, -0.2960, -0.1857, -0.3306,  0.3436,  0.3102, -0.3210,\n",
      "        -0.1225, -0.1158]) tensor([ 0.1044,  0.1180,  0.3984, -0.2988, -0.0686,  0.3144,  0.2562, -0.1482,\n",
      "         0.1752, -0.1712])\n",
      "R[0]\n",
      "tensor([-0.0062], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021432433253154158 0.0058809063436929135 0.0006887731520691887 0.00477953914552927 0.1799320938438177 0.00028760087490081786 0.008916788274917053\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2607,  0.3501, -0.0319, -0.2854,  0.2943,  0.7889, -0.0412, -0.5608,\n",
      "        -0.1814, -0.4405]) tensor([ 0.2420,  0.2983, -0.0215, -0.2750,  0.2462,  0.7179, -0.0026, -0.5106,\n",
      "        -0.1442, -0.4195]) tensor([ 0.2399,  0.2892,  0.2243, -0.3654,  0.2114,  0.6004,  0.1383, -0.3430,\n",
      "         0.0388, -0.3126])\n",
      "R[0]\n",
      "tensor([-0.0001], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020788356270641087 0.0047598917378054465 0.00039899269057423227 0.004103290717757772 0.18222843964397908 0.00032630321383476257 0.009614868910924997\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6065,  0.8943,  0.4787, -0.6106,  0.6500,  0.9421, -0.1778, -0.4864,\n",
      "         0.1513, -0.7293]) tensor([ 0.6002,  0.9023,  0.4993, -0.6249,  0.6508,  0.9261, -0.1756, -0.4698,\n",
      "         0.1601, -0.7285]) tensor([ 0.4351,  0.6610,  0.5579, -0.5862,  0.4175,  0.7294, -0.0451, -0.2622,\n",
      "         0.1283, -0.3748])\n",
      "R[0]\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020999170834198593 0.005756795646753745 0.0010127675408239155 0.004412106428353582 0.1820045582205057 0.000222758986055851 0.009315658313105815\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3787, -0.3337, -0.5425, -0.0224, -0.3402,  0.3351,  0.3862, -0.4752,\n",
      "        -0.1781, -0.2105]) tensor([-0.3408, -0.3078, -0.4869, -0.0415, -0.3326,  0.3192,  0.3683, -0.4150,\n",
      "        -0.1667, -0.2043]) tensor([-0.4927, -0.1920, -0.4533, -0.2366, -0.4177,  0.3997,  0.1927, -0.1919,\n",
      "        -0.3998,  0.0119])\n",
      "R[0]\n",
      "tensor([0.0405], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02102554040402174 0.005081378575454437 0.0007850068527695839 0.004601488169690128 0.18239854343235493 0.00029831655323505404 0.009488644659053535\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4169,  0.7516,  0.3022, -0.5129,  0.5201,  0.9101, -0.1423, -0.5891,\n",
      "         0.0293, -0.6194]) tensor([ 0.3549,  0.6745,  0.2369, -0.4636,  0.4698,  0.8958, -0.1503, -0.5835,\n",
      "        -0.0570, -0.5094]) tensor([ 0.2125,  0.5069,  0.0116, -0.3386,  0.3442,  0.8171, -0.0333, -0.7342,\n",
      "        -0.0503, -0.6116])\n",
      "R[0]\n",
      "tensor([-0.0025], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021407897282391786 0.006116428914072458 0.0007721376895606227 0.004820328017114662 0.1772551206946373 0.0002151958867907524 0.009587938581767957\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5921, -0.5996,  0.0462, -0.0025, -0.9546, -0.5873,  0.6793,  0.4659,\n",
      "         0.2047,  0.2025]) tensor([-0.5435, -0.5383,  0.0576, -0.0148, -0.8834, -0.5395,  0.6518,  0.4233,\n",
      "         0.1968,  0.1702]) tensor([-0.5602, -0.6184, -0.0961,  0.0765, -0.8895, -0.5328,  0.7370,  0.3366,\n",
      "         0.2334, -0.0631])\n",
      "R[0]\n",
      "tensor([-0.0076], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021425520308315755 0.005977470602709218 0.0007379157795103311 0.004762834725494031 0.18030478636920452 0.0003398980721831322 0.009653835195000283\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3523, -0.7150, -0.4507,  0.3769, -0.6669, -0.2406, -0.1583,  0.7744,\n",
      "        -0.5211,  0.1225]) tensor([ 0.3559, -0.4696, -0.2336,  0.2016, -0.5211, -0.1329, -0.1455,  0.6252,\n",
      "        -0.3759,  0.0840]) tensor([ 0.2418, -0.6337, -0.4226,  0.2912, -0.7058, -0.2109, -0.2311,  0.7910,\n",
      "        -0.6044,  0.2272])\n",
      "R[0]\n",
      "tensor([0.2567], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021548218540847302 0.005638260935746075 0.0004523918690329083 0.004747839809453581 0.17760167479515077 0.0004016687795519829 0.009490456751489545\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3686,  0.1493,  0.6703, -0.3489, -0.2553, -0.0568,  0.3305,  0.5965,\n",
      "         0.5762, -0.7099]) tensor([ 0.3612,  0.1867,  0.6598, -0.3649, -0.2165, -0.0119,  0.3056,  0.5206,\n",
      "         0.5594, -0.6943]) tensor([ 0.1151, -0.1359,  0.3975, -0.1870, -0.4720, -0.2573,  0.5318,  0.6025,\n",
      "         0.5082, -0.6491])\n",
      "R[0]\n",
      "tensor([0.0012], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02121348475664854 0.00528849090679796 0.000548482037024769 0.004507441962370649 0.18131911619007587 0.0003522445857524872 0.008595418677723501\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3280,  0.6169,  0.2449, -0.4466,  0.4348,  0.9029, -0.1972, -0.5566,\n",
      "        -0.1952, -0.2630]) tensor([ 0.2639,  0.5176,  0.1753, -0.3913,  0.3653,  0.8614, -0.1844, -0.5237,\n",
      "        -0.2650, -0.1712]) tensor([ 0.4770,  0.4052, -0.0443, -0.2901,  0.3891,  0.8288, -0.0706, -0.5229,\n",
      "        -0.0580, -0.7771])\n",
      "R[0]\n",
      "tensor([-0.0109], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021387260379269718 0.005075297139708709 0.0005578551394100942 0.0044596797230187805 0.17975647258758545 0.00028471484780311584 0.00847703980084043\n",
      "Average (on the epoch) training loss: 0.0045930235879437535\n",
      "Episode average V value: 0\n",
      "epoch 17:\n",
      "Learning rate: 1.8530201888518422e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 5.3411e-01,  4.4252e-01,  8.4614e-01, -4.9275e-01, -5.9533e-04,\n",
      "         1.6563e-01,  1.5512e-01,  4.5992e-01,  5.7073e-01, -7.6512e-01]) tensor([ 0.5211,  0.5284,  0.8451, -0.5345,  0.0808,  0.2792,  0.1010,  0.3080,\n",
      "         0.5364, -0.7394]) tensor([ 0.2716,  0.0386,  0.5683, -0.2912, -0.3442, -0.1413,  0.4155,  0.6152,\n",
      "         0.5598, -0.6945])\n",
      "R[0]\n",
      "tensor([0.0016], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021340465558692814 0.005399854369799869 0.00035374849034542423 0.004634816635982133 0.17808181236684323 0.00024307353049516677 0.009037724308436737\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6296, -0.4541, -0.0691, -0.0753, -0.7780, -0.2026,  0.3875, -0.0682,\n",
      "        -0.1277,  0.4226]) tensor([-0.5690, -0.3835, -0.0514, -0.0934, -0.7048, -0.1397,  0.3467, -0.1066,\n",
      "        -0.1311,  0.3658]) tensor([-0.4963, -0.2466,  0.0640, -0.2006, -0.5904, -0.0383,  0.2590, -0.1991,\n",
      "        -0.0879,  0.4123])\n",
      "R[0]\n",
      "tensor([0.0094], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02108067070506513 0.004816545910529385 0.0004995611934909903 0.004429575128364377 0.17972066946327686 0.0002554792463779449 0.009119658820447511\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4957,  0.7470,  0.5418, -0.5801,  0.4634,  0.7810, -0.0875, -0.2487,\n",
      "         0.1540, -0.6064]) tensor([ 0.4923,  0.7574,  0.5548, -0.5910,  0.4723,  0.7761, -0.0849, -0.2483,\n",
      "         0.1611, -0.6124]) tensor([ 0.5385,  0.8260,  0.5162, -0.5972,  0.5599,  0.8220, -0.0765, -0.3546,\n",
      "         0.1958, -0.7587])\n",
      "R[0]\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021237553842365742 0.005486103329370963 0.0005987416527477763 0.0044123654232244005 0.18029206350445748 0.0003910116255283356 0.008980220262950752\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5717, -0.6146, -0.1115,  0.0717, -0.8877, -0.5419,  0.7502,  0.3391,\n",
      "         0.2467, -0.0952]) tensor([-0.5690, -0.5602, -0.0882,  0.0416, -0.8637, -0.4501,  0.6464,  0.2902,\n",
      "         0.1675,  0.0255]) tensor([-0.5962, -0.5724,  0.1055, -0.0540, -0.9787, -0.6220,  0.6662,  0.5472,\n",
      "         0.2461,  0.2376])\n",
      "R[0]\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021037789380177856 0.005149266001171782 0.00035356446302375845 0.0042579948413185775 0.18122829794883727 0.00024108860641717912 0.009166899827599991\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4930, -0.1280,  0.3381, -0.3184, -0.6587, -0.2026,  0.2008,  0.2990,\n",
      "        -0.1001,  0.6255]) tensor([-0.4396, -0.0695,  0.3362, -0.3261, -0.5934, -0.1415,  0.1723,  0.2537,\n",
      "        -0.0991,  0.5480]) tensor([-0.4965, -0.0961,  0.3964, -0.3579, -0.6695, -0.2227,  0.1824,  0.3471,\n",
      "        -0.0742,  0.6615])\n",
      "R[0]\n",
      "tensor([-0.0120], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021014075323939323 0.005942631338752107 0.0004926441822717607 0.004465485445514787 0.183747539550066 0.00023950444906949996 0.009069326264085248\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4724,  0.3832,  0.8533, -0.4919, -0.0567,  0.1402,  0.1880,  0.4655,\n",
      "         0.5729, -0.6731]) tensor([ 0.5704,  0.5441,  0.8825, -0.5548,  0.1054,  0.3439,  0.0586,  0.2937,\n",
      "         0.5291, -0.7247]) tensor([ 0.6046,  0.6045,  0.9133, -0.5763,  0.1781,  0.3557,  0.0581,  0.2802,\n",
      "         0.5353, -0.7455])\n",
      "R[0]\n",
      "tensor([-0.0055], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02117041353136301 0.005581478312731633 0.0004949468652548603 0.004187894171744119 0.18196454325318337 0.00030585454404354095 0.009379482979304157\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4346, -0.1855, -0.3880, -0.2680, -0.3804,  0.4107,  0.2337, -0.1649,\n",
      "        -0.3460, -0.0323]) tensor([-0.3511, -0.1865, -0.3462, -0.2583, -0.3534,  0.3990,  0.2433, -0.0861,\n",
      "        -0.3217, -0.0790]) tensor([-0.5317, -0.2530, -0.0104, -0.2914, -0.6407,  0.0578,  0.2138,  0.2454,\n",
      "        -0.2804,  0.4603])\n",
      "R[0]\n",
      "tensor([0.0172], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02063140764273703 0.0059285952490099585 0.0003532690201445803 0.004644847487797961 0.18765142165124415 0.0003366841450333595 0.009635738708253485\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3248,  0.6289,  0.2883, -0.5099,  0.4319,  0.8734, -0.0902, -0.5555,\n",
      "        -0.0116, -0.4271]) tensor([ 0.3441,  0.6104,  0.3182, -0.5094,  0.4033,  0.8191, -0.0750, -0.4714,\n",
      "         0.0171, -0.4485]) tensor([ 0.2085,  0.4726,  0.0416, -0.3614,  0.3476,  0.8422, -0.0195, -0.7408,\n",
      "        -0.0589, -0.5047])\n",
      "R[0]\n",
      "tensor([-0.0112], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02108660739660263 0.0056876528868015155 0.00024995634170954875 0.0044349096933729015 0.18183795568346978 0.0002546285316348076 0.009475847213179804\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2666,  0.1747,  0.3549, -0.4412, -0.2255,  0.3721, -0.0450,  0.0407,\n",
      "        -0.4248,  0.5924]) tensor([-0.3040,  0.0368,  0.2863, -0.3642, -0.3225,  0.2703,  0.0414,  0.1393,\n",
      "        -0.4261,  0.5860]) tensor([ 0.0522,  0.2016,  0.4638, -0.3152, -0.0693,  0.4627, -0.1768, -0.0374,\n",
      "        -0.4290,  0.6797])\n",
      "R[0]\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020848309401422738 0.0045645978668762835 0.00035614104398700873 0.003779109986498952 0.18176915884017944 0.00018857032805681228 0.00992596506641712\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4687, -0.3052,  0.1482, -0.1823, -0.7000, -0.1250,  0.2480,  0.1957,\n",
      "        -0.1006,  0.5533]) tensor([-0.4242, -0.3428,  0.1171, -0.1457, -0.6912, -0.1398,  0.2835,  0.2231,\n",
      "        -0.0912,  0.4834]) tensor([-0.3379, -0.5967, -0.1741,  0.0805, -0.6760, -0.1179,  0.4362,  0.0663,\n",
      "        -0.0479,  0.2265])\n",
      "R[0]\n",
      "tensor([-0.0003], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020616415642201902 0.004633763610931055 0.000438027598210283 0.0041256683656247335 0.18430468645691872 0.00020545477420091628 0.009451451075437944\n",
      "Average (on the epoch) training loss: 0.0043372667179442945\n",
      "Episode average V value: 0\n",
      "epoch 18:\n",
      "Learning rate: 1.667718169966658e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3566,  0.6577,  0.1504, -0.4357,  0.4619,  0.8371, -0.0294, -0.6870,\n",
      "         0.1089, -0.8032]) tensor([ 0.2793,  0.5211,  0.1229, -0.3817,  0.3265,  0.6941,  0.0567, -0.5812,\n",
      "         0.1436, -0.7445]) tensor([ 0.3412,  0.6632,  0.2467, -0.4937,  0.4645,  0.9000, -0.1018, -0.6317,\n",
      "         0.0076, -0.5313])\n",
      "R[0]\n",
      "tensor([0.0078], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021085604820400478 0.004818155172590196 0.00011277735544445022 0.004264434093318414 0.18219856087863445 0.00017717768996953963 0.008979438859736547\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1568,  0.0653, -0.2289, -0.1749,  0.0722,  0.7449,  0.0445, -0.7538,\n",
      "        -0.4689,  0.1493]) tensor([-0.0848,  0.1045, -0.1689, -0.1994,  0.0918,  0.7205,  0.0343, -0.6844,\n",
      "        -0.4138,  0.0616]) tensor([-0.2265,  0.0018, -0.4052, -0.0823, -0.0116,  0.5814,  0.2004, -0.8365,\n",
      "        -0.2926, -0.2117])\n",
      "R[0]\n",
      "tensor([-0.0173], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020874596854671836 0.005091255718696629 0.0003114033367864977 0.003989937224949245 0.1823068007081747 0.00019676200300455094 0.009174248011317105\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7802, -0.6831, -0.3520,  0.1402, -0.9532, -0.4651,  0.6627,  0.0703,\n",
      "        -0.0453,  0.0688]) tensor([-0.7495, -0.6029, -0.2974,  0.0922, -0.8977, -0.3550,  0.5585,  0.0119,\n",
      "        -0.1007,  0.1724]) tensor([-0.7379, -0.5760, -0.1400,  0.0058, -0.9064, -0.3821,  0.5263,  0.0690,\n",
      "        -0.0987,  0.3704])\n",
      "R[0]\n",
      "tensor([-0.0054], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021157965667545794 0.005384097498324991 0.0001614502289039592 0.004480803664424457 0.1805811547935009 0.00021110746264457704 0.009146975146897603\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0134, -0.2374,  0.2398, -0.1118, -0.5199, -0.3162,  0.6355,  0.5228,\n",
      "         0.4941, -0.6783]) tensor([ 0.1476, -0.0580,  0.3456, -0.1893, -0.3771, -0.1620,  0.5133,  0.4790,\n",
      "         0.5307, -0.7974]) tensor([ 0.1968, -0.0381,  0.4872, -0.2424, -0.3933, -0.2098,  0.4911,  0.6117,\n",
      "         0.5627, -0.6987])\n",
      "R[0]\n",
      "tensor([-0.0105], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020986222608014943 0.005292132690148719 0.00026083868816795074 0.004205155954696238 0.18377601112425326 0.00030601558834314344 0.008716661572805606\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2927,  0.0146,  0.4618, -0.4159, -0.5219, -0.0144,  0.0970,  0.2472,\n",
      "         0.0810,  0.5164]) tensor([-0.2750, -0.0413,  0.3992, -0.3711, -0.5247, -0.0119,  0.1238,  0.2412,\n",
      "         0.0574,  0.4801]) tensor([-0.1011,  0.2279,  0.4643, -0.4510, -0.4898,  0.0080, -0.0985,  0.4264,\n",
      "         0.1194,  0.0804])\n",
      "R[0]\n",
      "tensor([0.0021], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021042224660515786 0.005922183098664391 0.0001600729922888604 0.004607854608562775 0.18095487275719643 0.00031396712362766264 0.00926635755627649\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1077, -0.0959,  0.6381, -0.3351, -0.6089, -0.3960,  0.3249,  0.9515,\n",
      "         0.4775, -0.2061]) tensor([ 0.0282, -0.1269,  0.6068, -0.3238, -0.6344, -0.4202,  0.3865,  0.9000,\n",
      "         0.4856, -0.1660]) tensor([-0.1078, -0.1679,  0.6141, -0.3450, -0.7195, -0.4592,  0.3454,  0.9370,\n",
      "         0.3779,  0.1369])\n",
      "R[0]\n",
      "tensor([0.0101], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020895900340750815 0.004982062950046383 0.0004092283144605062 0.004169479270814918 0.18202099581062794 0.00021502815186977387 0.00973442465759581\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.7400,  0.9217,  0.9217, -0.7182,  0.5109,  0.7059, -0.1383, -0.0481,\n",
      "         0.4598, -0.7699]) tensor([ 0.4943,  0.5063,  0.7613, -0.5369,  0.1038,  0.2941,  0.1070,  0.2745,\n",
      "         0.5014, -0.6520]) tensor([ 0.5393,  0.4614,  0.8700, -0.5188,  0.0509,  0.1456,  0.2388,  0.4564,\n",
      "         0.6746, -0.8335])\n",
      "R[0]\n",
      "tensor([-0.0011], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020987989922985433 0.004971228903283191 0.00020907974362853566 0.00419088652508799 0.18115159302949904 0.00019906005263328552 0.009050713710661512\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4470, -0.5656, -0.2014,  0.1178, -0.7714, -0.4612,  0.7760,  0.2619,\n",
      "         0.3276, -0.4758]) tensor([-0.4496, -0.5264, -0.1769,  0.0971, -0.7580, -0.4371,  0.7401,  0.2556,\n",
      "         0.3085, -0.4235]) tensor([-0.4945, -0.5521,  0.0100,  0.0186, -0.8914, -0.5675,  0.6938,  0.4687,\n",
      "         0.3040, -0.0912])\n",
      "R[0]\n",
      "tensor([-0.0022], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02125970615632832 0.006161762088013347 0.0006052376080306203 0.00447336336056469 0.17732055009901523 0.00029410661011934283 0.010140475241700187\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5079,  0.6868,  0.7426, -0.6491,  0.3879,  0.6501, -0.0488, -0.0609,\n",
      "         0.2429, -0.3375]) tensor([ 0.3859,  0.4543,  0.6514, -0.5435,  0.1630,  0.4107,  0.0932,  0.1237,\n",
      "         0.2871, -0.3103]) tensor([ 0.4079,  0.4706,  0.7363, -0.5741,  0.1621,  0.4105,  0.0851,  0.1742,\n",
      "         0.3175, -0.2705])\n",
      "R[0]\n",
      "tensor([-0.0019], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021715040378272533 0.006030943210993428 0.00020938755736642635 0.004660027467645705 0.18147549879550934 0.0003617437779903412 0.009196122932131403\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2531,  0.7999,  0.7417, -0.7606,  0.2400,  0.6576, -0.3351, -0.1793,\n",
      "         0.0538,  0.1404]) tensor([ 0.2124,  0.7474,  0.7183, -0.7428,  0.1856,  0.6162, -0.2979, -0.1439,\n",
      "         0.0594,  0.1409]) tensor([ 0.2808,  0.8232,  0.7461, -0.7628,  0.2762,  0.6806, -0.3379, -0.2006,\n",
      "         0.0587,  0.1113])\n",
      "R[0]\n",
      "tensor([-0.0100], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021163287768140436 0.006053284771543986 0.00010935467963781776 0.004512871175713372 0.1814936892837286 0.0002853315249085426 0.008911649393849076\n",
      "Average (on the epoch) training loss: 0.00435548133457778\n",
      "Episode average V value: 0\n",
      "epoch 19:\n",
      "Learning rate: 1.5009463529699922e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0915,  0.0981, -0.4195, -0.1405,  0.0609,  0.5401,  0.3141, -0.7796,\n",
      "        -0.0275, -0.7610]) tensor([-0.0481,  0.1034, -0.4077, -0.1322,  0.0802,  0.5808,  0.2470, -0.7443,\n",
      "        -0.0802, -0.7031]) tensor([-0.1878,  0.0494, -0.4302, -0.0849, -0.0158,  0.5325,  0.2219, -0.8589,\n",
      "        -0.1425, -0.4596])\n",
      "R[0]\n",
      "tensor([0.0177], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021381977213546635 0.00498493081554625 8.943240819462517e-06 0.004285284431418404 0.17911752672493458 0.0002755168005824089 0.008808014591573737\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1642,  0.1637,  0.7077, -0.4771, -0.1850,  0.0396,  0.3015,  0.4148,\n",
      "         0.4187, -0.1000]) tensor([ 0.2883,  0.2864,  0.6987, -0.5140, -0.0262,  0.2164,  0.1858,  0.2690,\n",
      "         0.3728, -0.1949]) tensor([ 0.3565,  0.4513,  0.6764, -0.5602,  0.1657,  0.4200,  0.1209,  0.0848,\n",
      "         0.3054, -0.2758])\n",
      "R[0]\n",
      "tensor([0.0048], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021100940011441707 0.005275791154184845 8.434894161382544e-06 0.00369768745591864 0.18030614633858205 0.00033155540376901625 0.008707093613571488\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1152,  0.3894,  0.4650, -0.6067, -0.2387,  0.3913, -0.2308, -0.1006,\n",
      "         0.0293,  0.3336]) tensor([-0.1844,  0.2512,  0.3682, -0.5298, -0.3249,  0.3327, -0.1671, -0.0729,\n",
      "        -0.0237,  0.3962]) tensor([ 0.0274,  0.5800,  0.5885, -0.6836, -0.0378,  0.5049, -0.2962, -0.1397,\n",
      "         0.0236,  0.2773])\n",
      "R[0]\n",
      "tensor([0.0014], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021723317444324493 0.005743024126029923 8.267418885679944e-06 0.004453832863771822 0.17973387549817563 0.00024015873670578004 0.00868828722287435\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0573,  0.0840,  0.0107, -0.1237, -0.1141,  0.5265, -0.1025, -0.5400,\n",
      "        -0.1537,  0.0598]) tensor([ 0.0289,  0.0469,  0.0035, -0.1174, -0.1587,  0.4688, -0.0781, -0.4842,\n",
      "        -0.1463,  0.0635]) tensor([ 0.0576,  0.1037,  0.0516, -0.1526, -0.1192,  0.5568, -0.1363, -0.5363,\n",
      "        -0.1562,  0.0979])\n",
      "R[0]\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021371413407847286 0.005016067146869318 7.432068355683441e-06 0.004201785434270277 0.1771212266087532 0.0002397605925798416 0.009435936610447242\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4335, -0.5438, -0.2723,  0.1404, -0.7093, -0.4105,  0.8027,  0.1620,\n",
      "         0.3363, -0.6248]) tensor([-0.4263, -0.5177, -0.2470,  0.1278, -0.7078, -0.4123,  0.7742,  0.1856,\n",
      "         0.3279, -0.5882]) tensor([-0.4676, -0.5695, -0.3707,  0.1756, -0.6993, -0.3877,  0.8238,  0.0756,\n",
      "         0.3096, -0.6882])\n",
      "R[0]\n",
      "tensor([-0.0050], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02127557927928865 0.0057152766845101725 6.790187531350966e-06 0.004607221042737365 0.1803899183422327 0.0003133396506309509 0.008742286209657323\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3312, -0.4737, -0.2216,  0.0981, -0.6372, -0.3381,  0.7809,  0.1728,\n",
      "         0.3853, -0.7499]) tensor([-0.4353, -0.5586, -0.2198,  0.1220, -0.7908, -0.4154,  0.7640,  0.2820,\n",
      "         0.3175, -0.5479]) tensor([-0.3898, -0.5182, -0.3347,  0.1458, -0.6514, -0.3369,  0.8008,  0.0879,\n",
      "         0.3479, -0.7828])\n",
      "R[0]\n",
      "tensor([-0.0086], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02118960489332676 0.00619702511090145 6.720853409206029e-06 0.004852647517982405 0.17967117224633694 0.0002605982646346092 0.009367186060757376\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3264,  0.1281,  0.6882, -0.3615, -0.2796, -0.1379,  0.3987,  0.6615,\n",
      "         0.6344, -0.6920]) tensor([ 0.3086,  0.1481,  0.6679, -0.3682, -0.2553, -0.1067,  0.3853,  0.5933,\n",
      "         0.6158, -0.6689]) tensor([ 0.4438,  0.2840,  0.8124, -0.4439, -0.1401,  0.0132,  0.2876,  0.5869,\n",
      "         0.6487, -0.7177])\n",
      "R[0]\n",
      "tensor([0.0008], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020891540583223105 0.005136832883079478 6.136852344752697e-06 0.004446685378090479 0.18164512829482554 0.0002625108286738396 0.008755441112094559\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5105,  0.7942,  0.7912, -0.6798,  0.4383,  0.7668, -0.1822, -0.0798,\n",
      "         0.1095, -0.2636]) tensor([ 0.4077,  0.5859,  0.7124, -0.5839,  0.2397,  0.5482, -0.0492,  0.0903,\n",
      "         0.1595, -0.2555]) tensor([ 0.5875,  0.9081,  0.7320, -0.6902,  0.5647,  0.8470, -0.2036, -0.2118,\n",
      "         0.1492, -0.4935])\n",
      "R[0]\n",
      "tensor([0.0003], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020984747797250746 0.004393008762905083 5.605088544825776e-06 0.0040261524620000275 0.18130920282006263 0.0002708820775151253 0.008476905485149473\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0501,  0.5779,  0.4975, -0.6442,  0.0169,  0.5881, -0.2992, -0.2834,\n",
      "         0.0141,  0.1922]) tensor([ 0.0156,  0.4993,  0.4745, -0.6093, -0.0593,  0.5027, -0.2414, -0.2022,\n",
      "         0.0323,  0.1881]) tensor([-0.0165,  0.5181,  0.6216, -0.6386, -0.0685,  0.4408, -0.2902, -0.0906,\n",
      "        -0.0868,  0.5066])\n",
      "R[0]\n",
      "tensor([0.0071], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021233665369451045 0.005624577334594505 5.273474783507481e-06 0.004535402980225626 0.17877324131131173 0.00030547116696834565 0.008826696228468791\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0097,  0.2901,  0.0978, -0.3592,  0.1994,  0.6892,  0.0272, -0.3833,\n",
      "        -0.4120,  0.0246]) tensor([ 0.0610,  0.3027,  0.1457, -0.3700,  0.1934,  0.6378,  0.0380, -0.2912,\n",
      "        -0.3604, -0.0521]) tensor([ 0.2653,  0.5497,  0.2447, -0.4764,  0.4392,  0.8093, -0.0190, -0.3817,\n",
      "        -0.2219, -0.3220])\n",
      "R[0]\n",
      "tensor([0.0038], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021153474541381 0.005537924817908788 5.148364910382952e-06 0.0042349125011242 0.1795082207620144 0.0003002530112862587 0.00889316265983507\n",
      "Average (on the epoch) training loss: 0.004334161206753924\n",
      "Episode average V value: 0\n",
      "epoch 20:\n",
      "Learning rate: 1.350851717672993e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3485,  0.0773,  0.3866, -0.5057, -0.5574,  0.1383, -0.0766,  0.1068,\n",
      "         0.0396,  0.5426]) tensor([-0.3381,  0.0787,  0.3645, -0.4923, -0.5447,  0.1474, -0.0673,  0.0952,\n",
      "         0.0403,  0.4973]) tensor([-0.1803,  0.2357,  0.6440, -0.5299, -0.3705,  0.1076, -0.1572,  0.2238,\n",
      "        -0.0743,  0.7727])\n",
      "R[0]\n",
      "tensor([0.0025], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021230316113680602 0.005191589572306839 4.844496088253436e-06 0.004213343029899988 0.1782010730057955 0.0003155739679932594 0.00893574873590842\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4299,  0.3174,  0.5222, -0.3647, -0.1275,  0.7407, -0.7690,  0.1268,\n",
      "        -0.4605,  0.6689]) tensor([ 0.3711,  0.3122,  0.5326, -0.3718, -0.1498,  0.6830, -0.6820,  0.1345,\n",
      "        -0.3820,  0.5999]) tensor([ 0.4133,  0.2819,  0.6239, -0.3319, -0.2191,  0.5722, -0.7696,  0.2908,\n",
      "        -0.4250,  0.8097])\n",
      "R[0]\n",
      "tensor([0.2102], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02159812143817544 0.005892992295463045 4.442589570544442e-06 0.0046937063565128485 0.17664390751719475 0.0002687854766845703 0.008019761614210438\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2403,  0.3008,  0.8632, -0.5577, -0.1514,  0.1454,  0.1824,  0.3953,\n",
      "         0.4533, -0.2129]) tensor([ 0.1386,  0.1513,  0.7712, -0.4777, -0.3019,  0.0056,  0.2501,  0.4810,\n",
      "         0.4361, -0.1662]) tensor([-0.2063, -0.1884,  0.3785, -0.2317, -0.6222, -0.3223,  0.4995,  0.5149,\n",
      "         0.3529, -0.1878])\n",
      "R[0]\n",
      "tensor([-0.0013], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02123327019996941 0.004939270971692168 4.456094305851366e-06 0.004402099098369945 0.17868779166042806 0.00023375026136636734 0.008704022454097867\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0951, -0.0479,  0.5165, -0.2012, -0.4807, -0.0057, -0.2538,  0.4425,\n",
      "        -0.1785,  0.7647]) tensor([ 0.1333,  0.0316,  0.5052, -0.2414, -0.3894,  0.1071, -0.3251,  0.3240,\n",
      "        -0.1983,  0.7219]) tensor([ 0.0394,  0.1495,  0.7274, -0.3956, -0.3960,  0.1749, -0.3346,  0.4250,\n",
      "        -0.1788,  0.9401])\n",
      "R[0]\n",
      "tensor([-0.0040], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021165317133069038 0.005290100569276547 3.910213455128542e-06 0.004674394807836506 0.17701732090115546 0.00026223061978816986 0.009086188826942817\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3087, -0.1447,  0.4230, -0.2922, -0.7600, -0.3686,  0.2355,  0.6823,\n",
      "         0.1947,  0.2754]) tensor([-0.2931, -0.0630,  0.4135, -0.3315, -0.6772, -0.2170,  0.1236,  0.5365,\n",
      "         0.1118,  0.3479]) tensor([-0.1287, -0.2330,  0.1782, -0.0779, -0.7020, -0.2725,  0.0802,  0.4116,\n",
      "         0.0745,  0.1176])\n",
      "R[0]\n",
      "tensor([0.0053], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02104953801445663 0.005973981272691162 3.6814345753555245e-06 0.004785111586912535 0.17650944411754607 0.0002885666862130165 0.008709412666095886\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7411, -0.5122, -0.4004,  0.0306, -0.7398, -0.0871,  0.4315, -0.3457,\n",
      "        -0.2629,  0.2470]) tensor([-0.6715, -0.4403, -0.3592,  0.0050, -0.6700, -0.0532,  0.4013, -0.3578,\n",
      "        -0.2492,  0.1891]) tensor([-0.6289, -0.5423, -0.5053,  0.1123, -0.6757, -0.1174,  0.4824, -0.3505,\n",
      "        -0.2289,  0.0095])\n",
      "R[0]\n",
      "tensor([-0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020597824594005942 0.004646893931756495 3.4140002653657576e-06 0.004065275294007733 0.1770187224447727 0.00020147240906953813 0.009049853818491101\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4858, -0.2854, -0.1317, -0.1211, -0.5429,  0.0389,  0.2919, -0.3850,\n",
      "        -0.1001,  0.2243]) tensor([-0.4413, -0.2571, -0.1167, -0.1250, -0.5145,  0.0399,  0.2802, -0.3703,\n",
      "        -0.0882,  0.1740]) tensor([-0.3799, -0.1126,  0.0282, -0.2506, -0.4107,  0.1571,  0.1828, -0.4015,\n",
      "        -0.0733,  0.2727])\n",
      "R[0]\n",
      "tensor([-0.0061], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020593071073293685 0.005550333205690549 3.180603651799174e-06 0.004279974086675793 0.18076646988093853 0.00020246250927448273 0.009013692281034309\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2869,  0.0945,  0.2732, -0.4332, -0.2922,  0.2324,  0.0976, -0.2983,\n",
      "        -0.0077,  0.3294]) tensor([-0.2519,  0.1179,  0.2725, -0.4344, -0.2562,  0.2738,  0.0757, -0.3500,\n",
      "         0.0072,  0.2914]) tensor([-0.1999,  0.2009,  0.3372, -0.4890, -0.1948,  0.3095,  0.0420, -0.3163,\n",
      "         0.0081,  0.2883])\n",
      "R[0]\n",
      "tensor([-0.0030], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02101379357278347 0.0055884025521590955 3.08570997640345e-06 0.0043326244020136075 0.17806060218811035 0.00025888015329837797 0.009054373074439355\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1242,  0.2792,  0.3922, -0.5226, -0.1225,  0.3617, -0.0062, -0.3226,\n",
      "         0.0151,  0.2661]) tensor([-0.1228,  0.2344,  0.3770, -0.4952, -0.1697,  0.3003,  0.0215, -0.2487,\n",
      "         0.0312,  0.2288]) tensor([-0.0731,  0.3403,  0.4304, -0.5534, -0.0682,  0.4028, -0.0375, -0.3252,\n",
      "         0.0244,  0.2398])\n",
      "R[0]\n",
      "tensor([-0.0033], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020934026740491392 0.005690490127264638 2.8779895944808233e-06 0.0047213981923414396 0.17814174276590347 0.00015986733883619308 0.008973448220960563\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4864, -0.4204, -0.7225,  0.1476, -0.5481,  0.1894,  0.1659, -0.4655,\n",
      "        -0.4528,  0.0477]) tensor([-0.4361, -0.3542, -0.6454,  0.0947, -0.4893,  0.2408,  0.1235, -0.4865,\n",
      "        -0.4394,  0.0802]) tensor([-0.5363, -0.4715, -0.6996,  0.1288, -0.6418,  0.1398,  0.1668, -0.3047,\n",
      "        -0.4701,  0.1211])\n",
      "R[0]\n",
      "tensor([0.0048], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020993077086284756 0.005164692708203802 2.675261173862964e-06 0.004256028450559825 0.17888338074088098 0.00025698257982730867 0.008957907006784808\n",
      "Average (on the epoch) training loss: 0.004442395530513022\n",
      "Episode average V value: 0\n",
      "epoch 21:\n",
      "Learning rate: 1.2157665459056937e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5895, -0.5457,  0.1635, -0.0854, -0.9887, -0.6484,  0.6345,  0.6199,\n",
      "         0.2585,  0.3068]) tensor([-0.5528, -0.4409,  0.1768, -0.1405, -0.8958, -0.4672,  0.4856,  0.4797,\n",
      "         0.1653,  0.4007]) tensor([-0.5489, -0.4787,  0.3180, -0.1586, -0.9810, -0.6911,  0.5667,  0.7424,\n",
      "         0.2569,  0.4832])\n",
      "R[0]\n",
      "tensor([-0.0019], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020698201002553106 0.0059303517973530685 2.621763389925036e-06 0.004431465760048013 0.17827118861675262 0.00014498617500066757 0.009544510149455164\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3055,  0.3195,  0.5115, -0.4423, -0.1286,  0.7839, -0.6306,  0.0503,\n",
      "        -0.3683,  0.6145]) tensor([ 0.2654,  0.3021,  0.5194, -0.4420, -0.1605,  0.7173, -0.5546,  0.0865,\n",
      "        -0.2988,  0.5452]) tensor([ 0.1091,  0.3312,  0.8186, -0.6727, -0.1283,  0.4064, -0.0130,  0.2287,\n",
      "         0.2070,  0.2995])\n",
      "R[0]\n",
      "tensor([0.1500], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020939984189346433 0.0056077788410475475 2.517624075835556e-06 0.004257502092572395 0.17756053848564624 0.00020056192576885224 0.008244739676476456\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2705, -0.0519,  0.6738, -0.4353, -0.7217, -0.3828,  0.2183,  0.8433,\n",
      "         0.2689,  0.4537]) tensor([-0.1982, -0.0390,  0.6529, -0.4228, -0.6541, -0.3369,  0.2438,  0.7868,\n",
      "         0.2912,  0.3436]) tensor([-0.3095, -0.2833,  0.4064, -0.2225, -0.8354, -0.5359,  0.4291,  0.7738,\n",
      "         0.3401,  0.1487])\n",
      "R[0]\n",
      "tensor([0.0093], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020930445238947867 0.006041326969207148 2.430703638538034e-06 0.004450159762025578 0.17516440330445768 0.00032697996497154237 0.008771353783784434\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1007,  0.1856, -0.3340, -0.1471,  0.0556,  0.6300,  0.0972, -0.8852,\n",
      "        -0.1112, -0.4849]) tensor([-0.0705,  0.1972, -0.2835, -0.1617,  0.0536,  0.6111,  0.0868, -0.8255,\n",
      "        -0.1017, -0.4703]) tensor([ 0.0551,  0.3992, -0.1665, -0.2640,  0.1916,  0.7435, -0.0237, -0.8169,\n",
      "        -0.0508, -0.6118])\n",
      "R[0]\n",
      "tensor([-0.0187], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020734872939065098 0.004578026215225691 2.2416673482439364e-06 0.003913058087171521 0.17677865444123744 0.0002630316838622093 0.009136787673574872\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3165, -0.1537, -0.4252, -0.0147, -0.4176,  0.5258, -0.1684, -0.6346,\n",
      "        -0.4462,  0.2092]) tensor([-0.2823, -0.1208, -0.3776, -0.0401, -0.3880,  0.5015, -0.1496, -0.6095,\n",
      "        -0.4056,  0.1618]) tensor([-0.2006,  0.0073, -0.2639, -0.0957, -0.3256,  0.6038, -0.2637, -0.7325,\n",
      "        -0.3189,  0.0768])\n",
      "R[0]\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020870783181861044 0.005430550229975779 2.198768031803411e-06 0.0041706157230655665 0.1776047563701868 0.00020804867148399353 0.008523182476637884\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2440,  0.2245,  0.7357, -0.4870, -0.1073,  0.1375,  0.2245,  0.3507,\n",
      "         0.3888, -0.0945]) tensor([ 0.2697,  0.2291,  0.7068, -0.4793, -0.0834,  0.1494,  0.2210,  0.3279,\n",
      "         0.3880, -0.1504]) tensor([ 0.1560,  0.1269,  0.7810, -0.4791, -0.2509, -0.0210,  0.2823,  0.5174,\n",
      "         0.4191,  0.0238])\n",
      "R[0]\n",
      "tensor([-0.0002], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020712351532652974 0.005288372359937057 2.0680846220102466e-06 0.004325884550169576 0.17938677524030208 0.0003156211972236633 0.00952553409396205\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1523, -0.4288, -0.4961,  0.0675, -0.2919,  0.4226,  0.2668, -0.4514,\n",
      "        -0.2336, -0.0619]) tensor([-0.0911, -0.4253, -0.4797,  0.0739, -0.2679,  0.4269,  0.2397, -0.3902,\n",
      "        -0.2388, -0.0834]) tensor([ 0.0254, -0.5316, -0.4922,  0.0991, -0.2751,  0.3410,  0.4024, -0.2812,\n",
      "        -0.0513, -0.3619])\n",
      "R[0]\n",
      "tensor([-0.0003], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02075416798517108 0.00534087525587529 1.9765569603578113e-06 0.004272200843319297 0.1781531673669815 0.0002078966349363327 0.009144511632272043\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5351, -0.2508,  0.1026, -0.2270, -0.6127, -0.0652,  0.2804, -0.1411,\n",
      "        -0.0771,  0.4728]) tensor([-0.5036, -0.3189,  0.0565, -0.1747, -0.6378, -0.1093,  0.3109, -0.0756,\n",
      "        -0.0861,  0.4270]) tensor([-0.4800, -0.2915, -0.1020, -0.1250, -0.5186,  0.1034,  0.2451, -0.3757,\n",
      "        -0.1964,  0.3615])\n",
      "R[0]\n",
      "tensor([-0.0022], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020656768351793288 0.0053602695945701275 2.0122927019201597e-06 0.004228011918021366 0.1788859173953533 0.00019895730167627336 0.009427690106793308\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1016,  0.4718,  0.8033, -0.7070, -0.2062,  0.2335, -0.1973,  0.2459,\n",
      "         0.0033,  0.6259]) tensor([-0.1570,  0.3565,  0.6989, -0.6374, -0.2709,  0.2128, -0.1527,  0.2369,\n",
      "        -0.0555,  0.6636]) tensor([-0.2424,  0.2689,  0.8183, -0.6447, -0.4181,  0.0183, -0.0622,  0.4593,\n",
      "         0.0573,  0.7687])\n",
      "R[0]\n",
      "tensor([0.0014], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02109569917432964 0.006184952480485663 1.8374150474755879e-06 0.004695248591015115 0.17667027790844442 0.00027797278761863706 0.008825243735918776\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4173, -0.1659, -0.3728, -0.0395,  0.0082,  0.5200,  0.1913, -0.2550,\n",
      "         0.0754, -0.8827]) tensor([ 0.2669, -0.3178, -0.3741, -0.0058, -0.2015,  0.3500,  0.2480, -0.0862,\n",
      "         0.0465, -0.6700]) tensor([-0.0221, -0.4894, -0.3896,  0.0355, -0.3218,  0.2456,  0.4501, -0.1882,\n",
      "         0.0460, -0.3698])\n",
      "R[0]\n",
      "tensor([0.0077], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02055495546013117 0.005316260160063394 1.7594961503846208e-06 0.004162934575113468 0.18097148637473584 0.0002345702424645424 0.009717193366144784\n",
      "Average (on the epoch) training loss: 0.00429070819025219\n",
      "Episode average V value: 0\n",
      "epoch 22:\n",
      "Learning rate: 1.0941898913151244e-05\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5463,  0.5236,  0.9796, -0.5871,  0.0960,  0.1928,  0.1883,  0.4419,\n",
      "         0.6720, -0.6878]) tensor([ 0.6377,  0.6799,  0.9846, -0.6436,  0.2606,  0.4124,  0.0471,  0.2398,\n",
      "         0.6055, -0.7281]) tensor([ 0.6282,  0.6775,  0.9812, -0.6427,  0.2581,  0.3873,  0.0737,  0.2697,\n",
      "         0.6156, -0.7421])\n",
      "R[0]\n",
      "tensor([-0.0044], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0204582677744329 0.005030193912149116 1.7092777657126135e-06 0.003986681417096407 0.17816969573497773 0.00026260879635810853 0.009640740356291644\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4688,  0.7609,  0.2266, -0.5132,  0.5484,  0.8818, -0.0455, -0.6833,\n",
      "         0.2411, -0.9440]) tensor([ 0.4674,  0.7418,  0.2464, -0.5073,  0.5337,  0.8403, -0.0257, -0.6334,\n",
      "         0.2528, -0.9286]) tensor([ 0.2422,  0.5585,  0.0178, -0.3987,  0.3786,  0.7903,  0.0494, -0.7557,\n",
      "         0.1351, -0.8380])\n",
      "R[0]\n",
      "tensor([-0.0196], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020565742837265134 0.005028347074523481 1.6599641103312024e-06 0.003883120630431222 0.17702004392445087 0.00021469608694314957 0.008792598798871041\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1672,  0.0276, -0.2001, -0.1364, -0.3478,  0.5852, -0.2779, -0.7012,\n",
      "        -0.2289,  0.0220]) tensor([-0.1903,  0.0048, -0.1804, -0.1411, -0.3763,  0.5366, -0.2416, -0.6557,\n",
      "        -0.2145,  0.0408]) tensor([-2.3646e-01, -9.9127e-02, -4.1535e-01,  3.1169e-04, -3.5738e-01,\n",
      "         5.3889e-01, -1.6790e-01, -7.8000e-01, -3.1962e-01,  2.4719e-03])\n",
      "R[0]\n",
      "tensor([0.0328], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020615917967632413 0.005625397260126192 1.6410251362231065e-06 0.004327549676643685 0.1783466185927391 0.00024223125725984573 0.00928678082273109\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0510, -0.2589,  0.1931, -0.0963, -0.5152, -0.3141,  0.6648,  0.4897,\n",
      "         0.4825, -0.6738]) tensor([-0.1624, -0.3629,  0.1703, -0.0650, -0.6803, -0.4171,  0.6622,  0.6038,\n",
      "         0.4201, -0.4986]) tensor([-0.0990, -0.2929,  0.3316, -0.1455, -0.6927, -0.4830,  0.5442,  0.7512,\n",
      "         0.4455, -0.3242])\n",
      "R[0]\n",
      "tensor([-0.0083], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021027216663584113 0.005796341427441802 1.6298491946145078e-06 0.004039237227407284 0.17720456038415433 0.0003259846270084381 0.009206902376492508\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2984,  0.5768,  0.0911, -0.4068,  0.4066,  0.8241, -0.0279, -0.7210,\n",
      "         0.0718, -0.6880]) tensor([ 0.3110,  0.5610,  0.1230, -0.4077,  0.3846,  0.7804, -0.0183, -0.6472,\n",
      "         0.0861, -0.6788]) tensor([ 0.4774,  0.7743,  0.2856, -0.5275,  0.5592,  0.9104, -0.1066, -0.6533,\n",
      "         0.1673, -0.7831])\n",
      "R[0]\n",
      "tensor([-0.0225], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02085583329945803 0.0052896622639891575 1.5961035490477116e-06 0.004133700954378583 0.17876872177422046 0.0002883275896310806 0.008652139649027958\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4962,  0.4997,  0.8259, -0.5285,  0.0665,  0.2393,  0.1411,  0.2989,\n",
      "         0.5834, -0.6891]) tensor([ 0.5741,  0.6291,  0.8293, -0.5744,  0.2072,  0.4265,  0.0214,  0.1265,\n",
      "         0.5226, -0.7154]) tensor([ 0.5981,  0.6557,  0.8812, -0.5928,  0.2370,  0.4079,  0.0386,  0.1734,\n",
      "         0.5458, -0.7266])\n",
      "R[0]\n",
      "tensor([-0.0028], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02086870500072837 0.005493352758894616 1.5765629125326086e-06 0.004056085987365805 0.17874796403944493 0.00029990988224744795 0.00950937802705448\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2444, -0.0534, -0.4081, -0.0489, -0.2030,  0.3104,  0.3519, -0.5349,\n",
      "        -0.0191, -0.7077]) tensor([-0.1588, -0.0065, -0.3586, -0.0609, -0.1433,  0.3746,  0.2871, -0.5063,\n",
      "        -0.0359, -0.6984]) tensor([-0.1923,  0.0952, -0.0692, -0.2286, -0.1579,  0.3561,  0.2377, -0.2957,\n",
      "        -0.0678, -0.4271])\n",
      "R[0]\n",
      "tensor([0.0176], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020832332605496048 0.005955928552240948 1.5364528119334863e-06 0.004491608307231217 0.17861248022317885 0.0003623430132865906 0.008749850980238989\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3726, -0.1377, -0.4374, -0.0440, -0.3893,  0.4807, -0.0630, -0.7189,\n",
      "        -0.3611,  0.1241]) tensor([-0.4030, -0.2533, -0.4710,  0.0138, -0.4730,  0.3859, -0.0071, -0.6027,\n",
      "        -0.3830,  0.1764]) tensor([-0.4397, -0.0073, -0.2486, -0.2461, -0.3370,  0.5128, -0.0670, -0.5582,\n",
      "        -0.3843,  0.3236])\n",
      "R[0]\n",
      "tensor([-0.0033], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0204850876070559 0.0048939177438151096 1.423498010694857e-06 0.003939366211241577 0.1771786144077778 0.00029358737170696257 0.009048402512446046\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5062,  0.0526,  0.1244, -0.2023,  0.0340,  0.0763,  0.1163,  0.5692,\n",
      "        -0.0971, -0.3928]) tensor([ 0.3946, -0.0392,  0.1132, -0.1933, -0.0929, -0.0289,  0.1499,  0.6489,\n",
      "        -0.1232, -0.2488]) tensor([ 0.1839, -0.0887, -0.1120, -0.0267, -0.1498, -0.0348,  0.2083,  0.3901,\n",
      "        -0.2571, -0.3863])\n",
      "R[0]\n",
      "tensor([0.0093], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020474822405725717 0.004827887087019917 1.4088180906810521e-06 0.003966751911269967 0.1770835860967636 0.00023732180148363115 0.00931325061514508\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0976, -0.2688,  0.2259, -0.1045, -0.5693, -0.3836,  0.6554,  0.5441,\n",
      "         0.4782, -0.5673]) tensor([-0.1183, -0.2772,  0.2138, -0.0967, -0.5804, -0.4084,  0.6671,  0.5463,\n",
      "         0.4773, -0.5472]) tensor([-0.2618, -0.4013, -0.0256,  0.0208, -0.6304, -0.3923,  0.7357,  0.3461,\n",
      "         0.4130, -0.6152])\n",
      "R[0]\n",
      "tensor([-0.0068], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020644329890608788 0.00512278379718191 1.3617341493841196e-06 0.004198065430740826 0.17843809953331946 0.0003514120653271675 0.009070916079275775\n",
      "Average (on the epoch) training loss: 0.004102216775380657\n",
      "Episode average V value: 0\n",
      "epoch 23:\n",
      "Learning rate: 9.84770902183612e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2876,  0.1015,  0.6768, -0.3529, -0.3226, -0.1964,  0.4078,  0.7119,\n",
      "         0.6270, -0.6525]) tensor([ 0.2559,  0.1015,  0.6485, -0.3506, -0.3147, -0.1863,  0.4142,  0.6551,\n",
      "         0.6140, -0.6269]) tensor([ 0.4094,  0.2617,  0.8065, -0.4394, -0.1715, -0.0330,  0.3083,  0.6244,\n",
      "         0.6527, -0.6955])\n",
      "R[0]\n",
      "tensor([0.0030], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020615144222974776 0.004750421282355092 1.3136068553762925e-06 0.0038459943251218645 0.1787773876339197 0.0003439649343490601 0.009619399934192188\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7412, -0.5818, -0.5372, -0.0126, -0.9276,  0.0466,  0.1872,  0.1017,\n",
      "        -0.4397,  0.4120]) tensor([-0.6519, -0.4674, -0.4644, -0.0673, -0.8057,  0.1082,  0.1725,  0.0473,\n",
      "        -0.3993,  0.3264]) tensor([-0.4472, -0.3729, -0.6373,  0.0942, -0.5610,  0.2975,  0.0608, -0.5002,\n",
      "        -0.4171,  0.1249])\n",
      "R[0]\n",
      "tensor([0.0084], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020461021138355136 0.0053857540425706245 1.2833403161494062e-06 0.004281983277469408 0.18071370109915733 0.0002595019340515137 0.008832499526091851\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3958, -0.2831,  0.2904, -0.2361, -0.8240, -0.3239,  0.2008,  0.5001,\n",
      "         0.1194,  0.4979]) tensor([-0.3994, -0.2631,  0.2711, -0.2346, -0.7849, -0.2835,  0.2081,  0.4370,\n",
      "         0.1074,  0.4870]) tensor([-0.6598, -0.3884,  0.0884, -0.1817, -0.9438, -0.2992,  0.2475,  0.4322,\n",
      "        -0.1149,  0.5978])\n",
      "R[0]\n",
      "tensor([0.0083], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020274763222783803 0.005035783947358141 1.2034959050311044e-06 0.004414980304194615 0.18173732103407383 0.0002174863815307617 0.009351640525041147\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1598,  0.3187,  0.7089, -0.6078, -0.2985,  0.1468, -0.1269,  0.2104,\n",
      "         0.0021,  0.6895]) tensor([-0.1815,  0.3039,  0.6640, -0.5893, -0.3119,  0.1686, -0.1336,  0.1937,\n",
      "        -0.0241,  0.6752]) tensor([-0.3690,  0.0244,  0.6916, -0.5099, -0.5923, -0.2059,  0.1430,  0.4913,\n",
      "         0.1411,  0.7670])\n",
      "R[0]\n",
      "tensor([-0.0157], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020693348169326782 0.005633494869478455 1.1918894898030885e-06 0.00450890773703577 0.17964494037628173 0.00030762574821710585 0.009118876982829533\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1099,  0.3192,  0.4164, -0.5511, -0.0833,  0.3888, -0.0187, -0.3298,\n",
      "         0.0137,  0.2740]) tensor([-0.1825,  0.1656,  0.3136, -0.4648, -0.1982,  0.2998,  0.0322, -0.2575,\n",
      "        -0.0485,  0.3418]) tensor([-0.4613, -0.1317,  0.1638, -0.3038, -0.4987,  0.0393,  0.2150, -0.2043,\n",
      "        -0.0626,  0.4521])\n",
      "R[0]\n",
      "tensor([0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020489421401172877 0.004834801000150037 1.1525397544573935e-06 0.0038193043675273657 0.18006406106054784 0.0002562895864248276 0.008996939093689434\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0602,  0.0642,  0.8714, -0.5373, -0.4437, -0.1350,  0.2329,  0.6493,\n",
      "         0.3312,  0.4436]) tensor([ 0.0297,  0.1091,  0.8288, -0.5339, -0.3444, -0.0317,  0.1969,  0.5442,\n",
      "         0.3127,  0.3304]) tensor([ 0.2123,  0.3583,  0.8856, -0.6272, -0.0584,  0.2773,  0.0699,  0.3074,\n",
      "         0.2723,  0.1716])\n",
      "R[0]\n",
      "tensor([0.0042], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02017660666257143 0.005089816946827341 1.1285756697247962e-06 0.00445181377674453 0.18061293518543242 0.00016968562453985214 0.009368526772246696\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2065, -0.0493, -0.5026, -0.0364,  0.0612,  0.5943,  0.1608, -0.5955,\n",
      "        -0.0981, -0.8094]) tensor([ 0.2028, -0.0333, -0.4499, -0.0675,  0.0529,  0.5698,  0.1462, -0.5428,\n",
      "        -0.0965, -0.7537]) tensor([ 0.4113,  0.4422, -0.1201, -0.3199,  0.3763,  0.8433,  0.0091, -0.7211,\n",
      "         0.1353, -1.0202])\n",
      "R[0]\n",
      "tensor([-0.0135], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020351757062599063 0.005389544990939612 1.0704605610385442e-06 0.004124300744733773 0.1761854518353939 0.000219582736492157 0.00878047388844425\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1517, -0.3758, -0.4464,  0.0756, -0.5087,  0.1396,  0.1738, -0.2969,\n",
      "        -0.1001, -0.3076]) tensor([-0.1082, -0.3541, -0.4080,  0.0621, -0.4735,  0.1675,  0.1572, -0.2773,\n",
      "        -0.0961, -0.2955]) tensor([-0.0486, -0.4386, -0.4496,  0.1168, -0.4732,  0.0816,  0.2601, -0.1801,\n",
      "        -0.0545, -0.4459])\n",
      "R[0]\n",
      "tensor([0.0047], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020484276667237282 0.005181012976601778 1.0754925107221424e-06 0.0038297029156819915 0.17740304879844188 0.0002591327801346779 0.009040697255637496\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5545, -0.3999, -0.5373,  0.0927, -0.5056,  0.0539,  0.4811, -0.4629,\n",
      "        -0.1636, -0.2330]) tensor([-0.5192, -0.3663, -0.4930,  0.0759, -0.4813,  0.0510,  0.4620, -0.4291,\n",
      "        -0.1628, -0.2250]) tensor([-0.6179, -0.5230, -0.5941,  0.1635, -0.6516, -0.1479,  0.6011, -0.2935,\n",
      "        -0.0597, -0.3736])\n",
      "R[0]\n",
      "tensor([-0.0075], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020131666211411357 0.005112505581972073 1.0516577755765866e-06 0.003880237308330834 0.1803345431536436 0.0002200437858700752 0.009490012580004986\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 2.8075e-04, -1.7838e-01,  4.9387e-01, -2.3905e-01, -6.5361e-01,\n",
      "        -4.8033e-01,  4.4607e-01,  8.7402e-01,  4.9071e-01, -2.8107e-01]) tensor([ 0.1088, -0.0558,  0.5532, -0.2884, -0.5410, -0.3756,  0.4001,  0.8085,\n",
      "         0.5410, -0.4107]) tensor([-0.1833, -0.3335,  0.2547, -0.1098, -0.7259, -0.4936,  0.5810,  0.6770,\n",
      "         0.4341, -0.3217])\n",
      "R[0]\n",
      "tensor([6.5982e-05], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02047542987577617 0.004831823512824485 1.0016566916988267e-06 0.003805736722657457 0.17781806588172913 0.0001921946108341217 0.008974104869645089\n",
      "Average (on the epoch) training loss: 0.004096296147949761\n",
      "Episode average V value: 0\n",
      "epoch 24:\n",
      "Learning rate: 8.862938119652508e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6395,  0.5870,  0.9348, -0.6320,  0.1477,  0.7158, -0.5132,  0.3831,\n",
      "        -0.0874,  0.3162]) tensor([ 0.6432,  0.6800,  0.9844, -0.6851,  0.2100,  0.7709, -0.4972,  0.3021,\n",
      "        -0.0142,  0.2157]) tensor([ 0.5094,  0.4414,  0.8965, -0.5404, -0.0212,  0.6566, -0.5608,  0.4099,\n",
      "        -0.2033,  0.6063])\n",
      "R[0]\n",
      "tensor([0.0781], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020437697146087886 0.005615922473436513 1.009191107073093e-06 0.004539308115199674 0.18201895190775394 0.00023810716718435288 0.008955705019528977\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6416,  0.7782,  0.9150, -0.6607,  0.3626,  0.5205, -0.0203,  0.0689,\n",
      "         0.5153, -0.7720]) tensor([ 0.6712,  0.8298,  0.9197, -0.6818,  0.4347,  0.5771, -0.0366, -0.0143,\n",
      "         0.5183, -0.8048]) tensor([ 0.7006,  0.8689,  0.8847, -0.6887,  0.4804,  0.6610, -0.0833, -0.0653,\n",
      "         0.4841, -0.8095])\n",
      "R[0]\n",
      "tensor([0.0016], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020194746723398566 0.003934462891160365 9.623109635867877e-07 0.00348024712438928 0.18160337294638157 0.00025967825949192047 0.009159574959485326\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1628, -0.0458,  0.0119,  0.0343, -0.1729,  0.6074, -0.2538, -0.5136,\n",
      "        -0.3095,  0.2430]) tensor([ 0.1278, -0.0683,  0.0173,  0.0300, -0.2130,  0.5334, -0.2112, -0.4549,\n",
      "        -0.2750,  0.2144]) tensor([ 0.1078, -0.0413,  0.0011,  0.0062, -0.1871,  0.5917, -0.2206, -0.5426,\n",
      "        -0.2889,  0.2386])\n",
      "R[0]\n",
      "tensor([-0.0058], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020453642634674907 0.005282096007445943 9.489054102118644e-07 0.004409503672737628 0.17941511423885823 0.00019948837161064148 0.009885946560185403\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5074,  0.8058,  0.3686, -0.5617,  0.6023,  0.9181, -0.1014, -0.6474,\n",
      "         0.1782, -0.7182]) tensor([ 0.4308,  0.7195,  0.2843, -0.5045,  0.5406,  0.8918, -0.1041, -0.6473,\n",
      "         0.0856, -0.6122]) tensor([ 0.2660,  0.5387,  0.0679, -0.3889,  0.3982,  0.8012,  0.0324, -0.7796,\n",
      "         0.0956, -0.6972])\n",
      "R[0]\n",
      "tensor([0.0020], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020339349448680877 0.004529555994973634 9.251292728436056e-07 0.00411567094392376 0.17943564261496067 0.00015677113085985184 0.009389161162020173\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1760,  0.2357,  0.1728, -0.4059, -0.2422,  0.4237, -0.1623, -0.4565,\n",
      "        -0.0300,  0.1947]) tensor([-0.2081,  0.1899,  0.1583, -0.3883, -0.2889,  0.3813, -0.1353, -0.4143,\n",
      "        -0.0375,  0.2181]) tensor([-0.0026,  0.4898,  0.3889, -0.5504, -0.0074,  0.5450, -0.2396, -0.4059,\n",
      "         0.0008,  0.1723])\n",
      "R[0]\n",
      "tensor([0.0147], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020606295943260192 0.005338575870919158 9.289901267948153e-07 0.004283247327140998 0.17667860008776187 0.00023135291785001755 0.009434076210309285\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4650,  0.8374,  0.6409, -0.6721,  0.4871,  0.8391, -0.2006, -0.2330,\n",
      "         0.0598, -0.4061]) tensor([ 0.4774,  0.8198,  0.6656, -0.6712,  0.4684,  0.7850, -0.1615, -0.1692,\n",
      "         0.1025, -0.4480]) tensor([ 0.5262,  0.8845,  0.5947, -0.6649,  0.5598,  0.8840, -0.1946, -0.3139,\n",
      "         0.1153, -0.5708])\n",
      "R[0]\n",
      "tensor([0.0114], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02038721815869212 0.0053484326010075165 9.050786455873094e-07 0.00421028277638834 0.17852733108401297 0.0002325742095708847 0.008982031419873238\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0317,  0.1047,  0.9002, -0.5455, -0.3520, -0.0591,  0.2309,  0.6007,\n",
      "         0.3496,  0.3700]) tensor([ 0.0517,  0.0821,  0.8572, -0.5196, -0.3543, -0.0692,  0.2453,  0.6068,\n",
      "         0.3526,  0.3009]) tensor([-0.0191,  0.0327,  0.8485, -0.5011, -0.4329, -0.1704,  0.2902,  0.6590,\n",
      "         0.3800,  0.3453])\n",
      "R[0]\n",
      "tensor([-0.0009], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020751922942698002 0.005208138515190513 8.957973592487178e-07 0.004233209179888945 0.17582006104290485 0.00020451779663562775 0.008939271805458702\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3758, -0.3062, -0.3128, -0.0007, -0.3759,  0.2756,  0.1947, -0.5701,\n",
      "        -0.3500,  0.2064]) tensor([-0.3325, -0.2778, -0.2792, -0.0171, -0.3567,  0.2576,  0.1883, -0.5248,\n",
      "        -0.3260,  0.1530]) tensor([-0.2888, -0.1612, -0.2939, -0.0765, -0.1741,  0.4216,  0.1874, -0.5756,\n",
      "        -0.4154,  0.0933])\n",
      "R[0]\n",
      "tensor([0.0103], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02064617992565036 0.00536462365162879 8.588321551883382e-07 0.004404040143301245 0.1763328182399273 0.00017752114683389664 0.009126218597288243\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1203, -0.2927,  0.2346, -0.1030, -0.6280, -0.4209,  0.6256,  0.6012,\n",
      "         0.4615, -0.4983]) tensor([-0.1586, -0.2870,  0.2304, -0.1062, -0.6347, -0.4242,  0.6299,  0.5819,\n",
      "         0.4515, -0.4516]) tensor([-0.2708, -0.4173, -0.0034,  0.0172, -0.6762, -0.4217,  0.7107,  0.4054,\n",
      "         0.4033, -0.5578])\n",
      "R[0]\n",
      "tensor([0.0014], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020842384357005358 0.005590654041960079 8.573750999403273e-07 0.004319670138764195 0.17610988399386407 0.0002801427692174911 0.008811183846206404\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0734,  0.1557, -0.1750, -0.2012,  0.0769,  0.6019,  0.1765, -0.6180,\n",
      "        -0.1908, -0.4140]) tensor([-0.0459,  0.1446, -0.1825, -0.1809,  0.0917,  0.6270,  0.1398, -0.5910,\n",
      "        -0.2361, -0.3765]) tensor([ 0.1311,  0.3442, -0.0473, -0.2978,  0.2584,  0.7233,  0.1086, -0.6159,\n",
      "        -0.0888, -0.6037])\n",
      "R[0]\n",
      "tensor([0.0047], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020828566340729594 0.005702523121250124 8.014301652679023e-07 0.004300017372122966 0.17669589431583882 0.0002973324954509735 0.008996129248524085\n",
      "Average (on the epoch) training loss: 0.004229519679385703\n",
      "Episode average V value: 0\n",
      "epoch 25:\n",
      "Learning rate: 7.976644307687257e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5216,  0.4956,  0.8177, -0.5069,  0.0930,  0.2468,  0.1579,  0.3186,\n",
      "         0.5788, -0.7513]) tensor([ 0.5293,  0.5823,  0.8125, -0.5461,  0.1866,  0.3604,  0.1017,  0.1670,\n",
      "         0.5476, -0.7463]) tensor([ 0.6089,  0.6758,  0.8680, -0.5946,  0.2694,  0.4424,  0.0271,  0.1699,\n",
      "         0.5161, -0.7577])\n",
      "R[0]\n",
      "tensor([6.8791e-05], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020768966117873787 0.004904968631533848 8.478984986766136e-07 0.004247062470181845 0.17756326116621493 0.00028310129791498187 0.009004237942863256\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3873, -0.1198, -0.1499, -0.1833, -0.5230,  0.3178, -0.0601, -0.4532,\n",
      "        -0.1918,  0.2280]) tensor([-0.4175, -0.2353, -0.1964, -0.1203, -0.5953,  0.2338,  0.0075, -0.3643,\n",
      "        -0.2084,  0.2614]) tensor([-0.4733, -0.2092, -0.4041, -0.0620, -0.5050,  0.3239,  0.0332, -0.5901,\n",
      "        -0.3158,  0.1755])\n",
      "R[0]\n",
      "tensor([-0.0016], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020946046443656087 0.005421771499721217 8.187840526261425e-07 0.004166562258440536 0.1776527856439352 0.0002740192264318466 0.009164623082790058\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.8000, -0.6520, -0.2674,  0.0867, -0.9395, -0.4332,  0.5984,  0.0583,\n",
      "        -0.1207,  0.2921]) tensor([-0.6688, -0.5898, -0.2325,  0.0785, -0.8465, -0.3801,  0.5673,  0.0742,\n",
      "        -0.0974,  0.1794]) tensor([-0.6531, -0.5278, -0.3754,  0.0887, -0.7095, -0.0976,  0.4256, -0.3249,\n",
      "        -0.2292,  0.2293])\n",
      "R[0]\n",
      "tensor([-0.0020], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02082374946959317 0.005545733986087725 8.184186377775404e-07 0.004403049667365849 0.17758371558785438 0.0002479304000735283 0.008862531832535752\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7436, -0.6116, -0.4006,  0.1128, -0.8190, -0.2519,  0.5508, -0.1686,\n",
      "        -0.1601,  0.1591]) tensor([-0.6889, -0.5385, -0.3629,  0.0834, -0.7496, -0.2047,  0.5100, -0.1907,\n",
      "        -0.1656,  0.1360]) tensor([-0.7834, -0.7055, -0.4791,  0.1749, -0.9012, -0.3801,  0.6784, -0.0422,\n",
      "        -0.0589, -0.0281])\n",
      "R[0]\n",
      "tensor([0.0128], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02074055681563914 0.005253177367398166 8.255902310452256e-07 0.004001053290383425 0.1775406237989664 0.00024083075672388078 0.008550709123839624\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0632, -0.2212,  0.5134, -0.2541, -0.7264, -0.5164,  0.4040,  0.9450,\n",
      "         0.4198, -0.0756]) tensor([-0.1445, -0.2672,  0.4805, -0.2328, -0.7619, -0.5618,  0.4771,  0.9169,\n",
      "         0.4273, -0.0300]) tensor([-0.2283, -0.2423,  0.5321, -0.2915, -0.8004, -0.5426,  0.3945,  0.9310,\n",
      "         0.3475,  0.2188])\n",
      "R[0]\n",
      "tensor([0.0076], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0206558648776263 0.004925160076003522 8.230317394009035e-07 0.004043929453007877 0.17938365529477596 0.0002825281620025635 0.008826929365925025\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2252,  0.0184, -0.1253, -0.1780, -0.3670,  0.4462, -0.1592, -0.6325,\n",
      "        -0.1458,  0.0469]) tensor([-0.2779, -0.0983, -0.1782, -0.1171, -0.4458,  0.3698, -0.1028, -0.5583,\n",
      "        -0.1812,  0.1171]) tensor([-0.3244,  0.0624, -0.0120, -0.2923, -0.3478,  0.3532, -0.0598, -0.5149,\n",
      "        -0.1253,  0.2155])\n",
      "R[0]\n",
      "tensor([-0.0020], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020779271114617585 0.005803218091590679 7.932680827025251e-07 0.004573160271625966 0.17793506303429604 0.00023147907108068467 0.00891467892046785\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2856,  0.0972,  0.1683, -0.3191, -0.1669,  0.4500,  0.0035, -0.1720,\n",
      "        -0.5126,  0.5027]) tensor([-0.2341,  0.1282,  0.1954, -0.3279, -0.1476,  0.4193,  0.0158, -0.1263,\n",
      "        -0.4591,  0.3880]) tensor([-0.1709,  0.2553,  0.4657, -0.4759, -0.1277,  0.3678, -0.0151,  0.1257,\n",
      "        -0.3650,  0.4851])\n",
      "R[0]\n",
      "tensor([0.0020], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020819538736715913 0.004805337265919661 7.757725996953014e-07 0.004181797956582159 0.1766058151870966 0.00022678875178098679 0.008823354941967409\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0118,  0.4356,  0.4827, -0.5933,  0.0241,  0.4717, -0.0962, -0.3558,\n",
      "         0.0047,  0.2549]) tensor([-0.0247,  0.3750,  0.4647, -0.5639, -0.0411,  0.3957, -0.0600, -0.2688,\n",
      "         0.0213,  0.2294]) tensor([-0.0048,  0.4435,  0.4873, -0.5968,  0.0319,  0.4774, -0.0995, -0.3572,\n",
      "         0.0063,  0.2501])\n",
      "R[0]\n",
      "tensor([-0.0042], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020749902671203017 0.004860446464557753 7.347319757400328e-07 0.004159175916574895 0.1784160454273224 0.00030876574665308 0.008733029885741416\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5021,  0.3852,  0.8229, -0.4657, -0.0208,  0.1366,  0.2156,  0.4729,\n",
      "         0.6151, -0.7591]) tensor([ 0.2511,  0.0594,  0.6806, -0.3236, -0.3733, -0.1961,  0.3823,  0.7157,\n",
      "         0.5874, -0.5829]) tensor([ 0.2454,  0.0398,  0.5281, -0.2703, -0.3229, -0.1631,  0.4537,  0.5934,\n",
      "         0.5859, -0.7438])\n",
      "R[0]\n",
      "tensor([-0.0035], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020858361290767787 0.005081419420581369 7.271034224629603e-07 0.004376773212687112 0.17821770833432674 0.0002138294205069542 0.009266375373816117\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5548, -0.4360, -0.4880,  0.1115, -0.6206, -0.1366,  0.5653, -0.1935,\n",
      "        -0.0158, -0.4486]) tensor([-0.5523, -0.4051, -0.4435,  0.0886, -0.6087, -0.1282,  0.5393, -0.1705,\n",
      "        -0.0371, -0.3793]) tensor([-0.5884, -0.5571, -0.4730,  0.1728, -0.7556, -0.3024,  0.6445, -0.0407,\n",
      "         0.0697, -0.4575])\n",
      "R[0]\n",
      "tensor([0.0120], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020731717677786945 0.005816526693881315 7.089066137950795e-07 0.004115899512136821 0.17789510855078697 0.0002683417722582817 0.008148630276409677\n",
      "Average (on the epoch) training loss: 0.004226846400898649\n",
      "Episode average V value: 0\n",
      "epoch 26:\n",
      "Learning rate: 7.1789798769185315e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3627, -0.0550,  0.5825, -0.4064, -0.7301, -0.3404,  0.1624,  0.7103,\n",
      "         0.1601,  0.5788]) tensor([-0.3532, -0.0481,  0.5552, -0.3928, -0.6991, -0.3133,  0.1754,  0.6640,\n",
      "         0.1536,  0.5455]) tensor([-0.4372, -0.1626,  0.6023, -0.3831, -0.8320, -0.4905,  0.2596,  0.8049,\n",
      "         0.2158,  0.6945])\n",
      "R[0]\n",
      "tensor([-0.0005], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020786355152726175 0.005226587873963581 7.214952757976789e-07 0.004454405789088923 0.17833160711824894 0.0003126829192042351 0.009421050586155616\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5032,  0.4126,  0.8751, -0.5024, -0.0191,  0.1276,  0.2066,  0.5042,\n",
      "         0.6391, -0.7339]) tensor([ 0.5996,  0.5747,  0.9057, -0.5668,  0.1364,  0.3302,  0.0798,  0.3307,\n",
      "         0.5992, -0.7878]) tensor([ 0.5937,  0.5953,  0.9254, -0.5813,  0.1707,  0.3205,  0.0934,  0.3356,\n",
      "         0.5930, -0.7620])\n",
      "R[0]\n",
      "tensor([-0.0030], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020786127218976616 0.005144235831932747 7.148666915668401e-07 0.0037544837644672953 0.17907824704051017 0.00031593021750450135 0.008631495498935693\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1022,  0.4318,  0.6849, -0.6583, -0.1986,  0.2288, -0.1307,  0.1048,\n",
      "         0.0843,  0.4565]) tensor([-0.1259,  0.4115,  0.6459, -0.6418, -0.2138,  0.2475, -0.1338,  0.0872,\n",
      "         0.0549,  0.4626]) tensor([ 0.0462,  0.6097,  0.7343, -0.7172,  0.0013,  0.4034, -0.2283, -0.0111,\n",
      "         0.0566,  0.3630])\n",
      "R[0]\n",
      "tensor([-0.0121], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021028227984905244 0.004626308727856667 6.945039915535744e-07 0.00423936495417729 0.1759484938532114 0.000303711898624897 0.008527776739938417\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1517,  0.0292, -0.4520, -0.0547,  0.0140,  0.5998,  0.1677, -0.9369,\n",
      "        -0.1938, -0.3655]) tensor([-0.0991,  0.0723, -0.3910, -0.0851,  0.0384,  0.6100,  0.1325, -0.8924,\n",
      "        -0.1846, -0.3696]) tensor([-0.1953,  0.0871, -0.2999, -0.1450,  0.0063,  0.6377,  0.0653, -0.8484,\n",
      "        -0.2846, -0.0545])\n",
      "R[0]\n",
      "tensor([-0.0275], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02073138615116477 0.004648943953940034 6.918750673889917e-07 0.00411530976742506 0.177194076359272 0.0002755357399582863 0.009127295366022735\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5982, -0.1917, -0.4617, -0.1590, -0.4676,  0.4897, -0.0156, -0.4593,\n",
      "        -0.5745,  0.4846]) tensor([-0.4986, -0.1204, -0.4052, -0.1980, -0.3840,  0.5279, -0.0328, -0.4865,\n",
      "        -0.5085,  0.3750]) tensor([-0.6656, -0.3302, -0.5371, -0.1126, -0.6175,  0.3895,  0.0317, -0.3068,\n",
      "        -0.5754,  0.4994])\n",
      "R[0]\n",
      "tensor([-0.0031], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02130658192932606 0.006145595455011062 7.096664782579864e-07 0.004437615646515042 0.17595234428346157 0.00031891512125730516 0.008425858077593149\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1960,  0.1665,  0.7438, -0.4781, -0.1929,  0.0305,  0.2808,  0.4475,\n",
      "         0.4497, -0.0945]) tensor([ 0.2255,  0.1747,  0.7129, -0.4720, -0.1609,  0.0523,  0.2736,  0.4135,\n",
      "         0.4439, -0.1475]) tensor([ 0.1044,  0.0867,  0.7752, -0.4722, -0.3135, -0.1020,  0.3281,  0.5723,\n",
      "         0.4597,  0.0311])\n",
      "R[0]\n",
      "tensor([-0.0002], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021087517051026225 0.005125832942670968 6.575692240744502e-07 0.004353016897686757 0.1773861436843872 0.0002962610647082329 0.008936471126216929\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0626,  0.1122, -0.3532, -0.1078,  0.0972,  0.6671,  0.1304, -0.9327,\n",
      "        -0.1659, -0.3980]) tensor([-0.0115,  0.1500, -0.2928, -0.1363,  0.1153,  0.6691,  0.0995, -0.8793,\n",
      "        -0.1525, -0.4065]) tensor([-0.1129,  0.0512, -0.4305, -0.0712,  0.0557,  0.5903,  0.2212, -0.9348,\n",
      "        -0.1358, -0.4786])\n",
      "R[0]\n",
      "tensor([-0.0286], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021105289163067937 0.005597143463623069 6.64990381011421e-07 0.004541821425314993 0.17673333136737346 0.00029503385722637175 0.009134921955876053\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3790,  0.0256,  0.0352, -0.3258, -0.2836,  0.2947,  0.1023, -0.5264,\n",
      "        -0.1387,  0.3388]) tensor([-0.3214,  0.0739,  0.0574, -0.3419, -0.2243,  0.3519,  0.0733, -0.5874,\n",
      "        -0.1090,  0.2865]) tensor([-0.3142,  0.1185,  0.1564, -0.4133, -0.2243,  0.3623,  0.0358, -0.5058,\n",
      "        -0.1078,  0.3896])\n",
      "R[0]\n",
      "tensor([-0.0007], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020677518812939524 0.005254824819799978 6.378316035124953e-07 0.004098784060624893 0.17850167106091977 0.0002234245017170906 0.009304333643463905\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3066, -0.5435, -0.5080,  0.2944, -0.3727, -0.1946,  0.1694,  0.3662,\n",
      "        -0.2973, -0.3560]) tensor([ 0.2322, -0.5237, -0.4391,  0.2486, -0.4104, -0.1971,  0.1983,  0.3644,\n",
      "        -0.2528, -0.2727]) tensor([ 0.3099, -0.4828, -0.4220,  0.2299, -0.4254, -0.1259, -0.0813,  0.5648,\n",
      "        -0.5673,  0.0052])\n",
      "R[0]\n",
      "tensor([-0.0155], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020613378290086984 0.0049461028925252325 6.45755768005074e-07 0.004575360266200732 0.17709251953661442 0.0003321823924779892 0.008720025640912354\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.7911,  0.9877,  0.8159, -0.7025,  0.6532,  0.8675, -0.1864, -0.2794,\n",
      "         0.4134, -0.8249]) tensor([ 0.7700,  0.9974,  0.8148, -0.7095,  0.6657,  0.8565, -0.1640, -0.3076,\n",
      "         0.4263, -0.8428]) tensor([ 0.7962,  0.9958,  0.8102, -0.7042,  0.6648,  0.8823, -0.1935, -0.2975,\n",
      "         0.4091, -0.8262])\n",
      "R[0]\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021012341206893326 0.005825543903731159 6.483606534288811e-07 0.004750942412880249 0.1758289332687855 0.00035243265330791473 0.008318085104692728\n",
      "Average (on the epoch) training loss: 0.004332110498438123\n",
      "Episode average V value: 0\n",
      "epoch 27:\n",
      "Learning rate: 6.461081889226678e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1917,  0.0743, -0.0606, -0.1953, -0.0375,  0.5383,  0.0881, -0.4848,\n",
      "        -0.4108,  0.1214]) tensor([-0.1522,  0.0924, -0.0235, -0.2089, -0.0364,  0.5089,  0.0867, -0.4180,\n",
      "        -0.3816,  0.0654]) tensor([-0.1358,  0.1975,  0.2282, -0.3520, -0.0246,  0.5343,  0.0108, -0.2193,\n",
      "        -0.4037,  0.3114])\n",
      "R[0]\n",
      "tensor([-0.0014], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021248863495886325 0.0062333558619211546 6.156101933925128e-07 0.0046420185858733025 0.1742890004068613 0.0002549061551690102 0.008539317138318438\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0169,  0.3848,  0.3860, -0.4055, -0.1022,  0.4276, -0.3994, -0.1989,\n",
      "        -0.3965,  0.6020]) tensor([-0.1453,  0.1829,  0.2854, -0.3096, -0.2734,  0.2745, -0.2793, -0.0871,\n",
      "        -0.4214,  0.6803]) tensor([ 0.0401,  0.4427,  0.5906, -0.4764, -0.1364,  0.4076, -0.5084,  0.0226,\n",
      "        -0.4216,  0.8024])\n",
      "R[0]\n",
      "tensor([-0.0065], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021111958123743533 0.005070389276719652 6.135388461814273e-07 0.004503072549123317 0.17557879148423672 0.00036321366578340533 0.008799503118963912\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3428,  0.4543,  0.9736, -0.6356, -0.0305,  0.1911,  0.0971,  0.3811,\n",
      "         0.4938, -0.2137]) tensor([ 0.2094,  0.2527,  0.8599, -0.5293, -0.2303, -0.0045,  0.2019,  0.5084,\n",
      "         0.4864, -0.1642]) tensor([-0.1101, -0.1005,  0.5320, -0.3213, -0.5561, -0.2994,  0.4515,  0.6127,\n",
      "         0.4442, -0.1221])\n",
      "R[0]\n",
      "tensor([-4.3444e-05], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021128585785627364 0.005806790403250489 6.092560836350458e-07 0.004824299715983216 0.17575998601317405 0.00036673227697610854 0.008476097994542214\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5377,  0.4746,  0.8661, -0.5177,  0.0547,  0.2246,  0.1506,  0.4189,\n",
      "         0.5940, -0.7484]) tensor([ 0.6201,  0.6178,  0.8842, -0.5726,  0.1981,  0.4156,  0.0316,  0.2479,\n",
      "         0.5447, -0.7851]) tensor([ 0.6077,  0.6269,  0.9026, -0.5859,  0.2110,  0.3800,  0.0587,  0.2786,\n",
      "         0.5551, -0.7666])\n",
      "R[0]\n",
      "tensor([-0.0029], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02108124706149101 0.005507098587877408 5.762856378908055e-07 0.004593684386345558 0.17508744682371616 0.0004356047958135605 0.008731562094704713\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2299,  0.5326,  0.9931, -0.7205,  0.0444,  0.3694, -0.0471,  0.4479,\n",
      "         0.1239,  0.2653]) tensor([ 0.1934,  0.4468,  0.9187, -0.6592, -0.0327,  0.2952, -0.0082,  0.4900,\n",
      "         0.1356,  0.2180]) tensor([ 0.4218,  0.7702,  0.9564, -0.7713,  0.3384,  0.6645, -0.1888,  0.1460,\n",
      "         0.0736,  0.0455])\n",
      "R[0]\n",
      "tensor([0.0006], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021292905511334538 0.005711804177961312 5.851329635220282e-07 0.004668063049088232 0.17299591560661792 0.00026439961791038515 0.008425745579763316\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3534, -0.5015, -0.2539,  0.1274, -0.6573, -0.3625,  0.7807,  0.1794,\n",
      "         0.3656, -0.7368]) tensor([-0.4597, -0.5940, -0.2387,  0.1476, -0.8281, -0.4524,  0.7590,  0.3194,\n",
      "         0.2956, -0.5132]) tensor([-0.3845, -0.5129, -0.0463,  0.0598, -0.7985, -0.5080,  0.7000,  0.4356,\n",
      "         0.3494, -0.3704])\n",
      "R[0]\n",
      "tensor([-0.0072], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021106293892487884 0.005756698523073283 5.760331313240386e-07 0.004225099778384901 0.17521638967096806 0.0003378479406237602 0.008203925610869191\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2264,  0.1735,  0.5139, -0.5126, -0.4555,  0.1036, -0.0954,  0.1447,\n",
      "         0.0882,  0.5373]) tensor([-0.2510,  0.0835,  0.4355, -0.4553, -0.4933,  0.0892, -0.0523,  0.1372,\n",
      "         0.0507,  0.5491]) tensor([-0.3667, -0.0050,  0.6230, -0.4708, -0.6409, -0.2127,  0.1163,  0.4634,\n",
      "         0.1711,  0.7020])\n",
      "R[0]\n",
      "tensor([0.0032], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02099452893435955 0.00489656394600388 5.629868539926974e-07 0.004067126952752006 0.175230562672019 0.0004539003819227219 0.00827569391700672\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2646, -0.1691, -0.6005,  0.0296, -0.1242,  0.4322,  0.3059, -0.8492,\n",
      "        -0.2191, -0.3274]) tensor([-0.2190, -0.1754, -0.5975,  0.0410, -0.1110,  0.4663,  0.2446, -0.7982,\n",
      "        -0.2725, -0.2808]) tensor([-0.3347, -0.0547, -0.3554, -0.1166, -0.1276,  0.5171,  0.1711, -0.7889,\n",
      "        -0.2810,  0.0440])\n",
      "R[0]\n",
      "tensor([0.0137], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02111568939499557 0.004998481829105003 5.549687150505633e-07 0.004195706698461436 0.17342595405876637 0.00035526907444000246 0.008900540248840115\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.7462,  0.9779,  0.5421, -0.6535,  0.7543,  0.9846, -0.1852, -0.5691,\n",
      "         0.3050, -0.8808]) tensor([ 0.7152,  0.9587,  0.5401, -0.6500,  0.7331,  0.9400, -0.1578, -0.5459,\n",
      "         0.3119, -0.8759]) tensor([ 0.7585,  0.9790,  0.5341, -0.6474,  0.7622,  0.9896, -0.1878, -0.5781,\n",
      "         0.3064, -0.9012])\n",
      "R[0]\n",
      "tensor([-0.0093], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0210734556093812 0.0046343425446684705 5.51880749725342e-07 0.004141457200690638 0.17376018871366977 0.0003402273133397102 0.008319918181223329\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1740,  0.2139,  0.2437, -0.2310, -0.0337,  0.6782, -0.2995, -0.4773,\n",
      "        -0.2365,  0.3125]) tensor([ 0.1358,  0.1730,  0.2363, -0.2229, -0.0916,  0.6042, -0.2600, -0.4089,\n",
      "        -0.2144,  0.2939]) tensor([ 0.1140,  0.1346,  0.1757, -0.1784, -0.1008,  0.6369, -0.2694, -0.5058,\n",
      "        -0.2523,  0.3296])\n",
      "R[0]\n",
      "tensor([-0.0067], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02107926683872938 0.005459806522227154 5.249884054308041e-07 0.004599943262990564 0.1752671201825142 0.0003524085059762001 0.009434759933385066\n",
      "Average (on the epoch) training loss: 0.004446047217969317\n",
      "Episode average V value: 0\n",
      "epoch 28:\n",
      "Learning rate: 5.81497370030401e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2829,  0.2739,  0.5804, -0.3944,  0.0707,  0.6012, -0.1223,  0.0204,\n",
      "        -0.1566,  0.3470]) tensor([ 0.2723,  0.2271,  0.5186, -0.3575,  0.0580,  0.5891, -0.1064,  0.0136,\n",
      "        -0.1802,  0.3205]) tensor([ 0.3934,  0.2938,  0.4955, -0.3100,  0.1216,  0.7638, -0.2868, -0.2109,\n",
      "        -0.2202,  0.3296])\n",
      "R[0]\n",
      "tensor([-0.0167], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02099984231777489 0.0052075250774651066 5.240429943853541e-07 0.004395008831867017 0.17469936536252498 0.0003069174811244011 0.008943678411189466\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5848, -0.4667, -0.3428,  0.0412, -0.6634, -0.1823,  0.4841, -0.2655,\n",
      "        -0.1148,  0.0521]) tensor([-0.5632, -0.4208, -0.2991,  0.0091, -0.6295, -0.1201,  0.4223, -0.3055,\n",
      "        -0.1399,  0.1293]) tensor([-0.5141, -0.3006, -0.1960, -0.0838, -0.5426,  0.0244,  0.3048, -0.4378,\n",
      "        -0.1297,  0.2110])\n",
      "R[0]\n",
      "tensor([0.0011], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020936498679220678 0.005235629463350051 5.106808989410183e-07 0.004473076463793404 0.1751949836164713 0.00032178112864494326 0.008813059866282856\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0426,  0.0430,  0.6983, -0.4371, -0.3554, -0.1261,  0.3414,  0.5328,\n",
      "         0.4348,  0.0476]) tensor([ 0.0619,  0.0312,  0.6603, -0.4182, -0.3423, -0.1261,  0.3514,  0.5164,\n",
      "         0.4320, -0.0034]) tensor([-0.0186, -0.0126,  0.7149, -0.4294, -0.4456, -0.2279,  0.3700,  0.6313,\n",
      "         0.4439,  0.1297])\n",
      "R[0]\n",
      "tensor([4.6745e-05], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020771251268684864 0.004880192423304834 5.17228209332643e-07 0.0039702329264837315 0.17614515337347986 0.00034118451178073885 0.008806277750409208\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5038, -0.2340, -0.2749, -0.0559, -0.4284,  0.2093,  0.2291, -0.2852,\n",
      "        -0.4127,  0.2334]) tensor([-0.4400, -0.1801, -0.2285, -0.0864, -0.3696,  0.2502,  0.2008, -0.3149,\n",
      "        -0.3808,  0.1962]) tensor([-0.2245,  0.0445, -0.1250, -0.1629, -0.0974,  0.4710,  0.1439, -0.4841,\n",
      "        -0.3181, -0.0535])\n",
      "R[0]\n",
      "tensor([-0.0003], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020756814643740653 0.0049776231545693005 5.094073611928706e-07 0.0038720884482027033 0.17307566480338574 0.00026573946326971054 0.008807487734302413\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5984, -0.5530,  0.1019, -0.0494, -0.9676, -0.6176,  0.6484,  0.5594,\n",
      "         0.2505,  0.2301]) tensor([-0.5843, -0.5206,  0.0979, -0.0519, -0.9228, -0.5848,  0.6409,  0.5146,\n",
      "         0.2362,  0.2262]) tensor([-0.5697, -0.4857,  0.2843, -0.1417, -0.9956, -0.6815,  0.5573,  0.7323,\n",
      "         0.2518,  0.4648])\n",
      "R[0]\n",
      "tensor([-0.0046], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021063056841492653 0.004529955537334899 5.077533035660054e-07 0.0040383263803669255 0.17480411030352117 0.0002367905303835869 0.009097056517086458\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4058, -0.1511,  0.3378, -0.2844, -0.6309, -0.2061,  0.2694,  0.4082,\n",
      "         0.0481,  0.4892]) tensor([-0.3897, -0.1279,  0.3165, -0.2805, -0.5949, -0.1674,  0.2582,  0.3657,\n",
      "         0.0330,  0.4545]) tensor([-0.2421,  0.1705,  0.5413, -0.4859, -0.3919,  0.0441,  0.0358,  0.2993,\n",
      "         0.0279,  0.5203])\n",
      "R[0]\n",
      "tensor([-0.0144], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0209497799705714 0.0041544042231653295 4.863580737719531e-07 0.00410541574889794 0.17220419143140317 0.00028514935821294786 0.009012770878965966\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3961, -0.5151,  0.0679,  0.0084, -0.8761, -0.5768,  0.6322,  0.5877,\n",
      "         0.3191, -0.1436]) tensor([-0.3964, -0.4994,  0.0702,  0.0068, -0.8515, -0.5791,  0.6458,  0.5687,\n",
      "         0.3212, -0.1369]) tensor([-0.4633, -0.5851, -0.1454,  0.1069, -0.8281, -0.5174,  0.7426,  0.3693,\n",
      "         0.3099, -0.3419])\n",
      "R[0]\n",
      "tensor([-0.0023], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021136477129533888 0.005991440947269439 4.928698276103205e-07 0.00417174372903537 0.1730860836803913 0.00035876988619565965 0.008611402915441432\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4350,  0.6913,  0.1868, -0.4618,  0.5099,  0.8912, -0.0747, -0.7299,\n",
      "         0.1544, -0.8083]) tensor([ 0.3377,  0.5228,  0.1498, -0.3918,  0.3370,  0.7231,  0.0252, -0.5911,\n",
      "         0.1880, -0.7373]) tensor([ 0.3852,  0.7016,  0.3170, -0.5438,  0.4855,  0.8875, -0.1074, -0.6086,\n",
      "         0.0822, -0.5333])\n",
      "R[0]\n",
      "tensor([0.0071], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02158437510766089 0.005747075710067293 4.974274829692149e-07 0.0044596985481912274 0.17077054741978645 0.0002925792783498764 0.008698836201801896\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0951,  0.1454,  0.7673, -0.3913, -0.4123,  0.2649, -0.4313,  0.5034,\n",
      "        -0.2150,  0.9626]) tensor([ 0.0925,  0.1915,  0.7640, -0.4028, -0.3759,  0.2906, -0.4108,  0.4567,\n",
      "        -0.1703,  0.8613]) tensor([ 0.2936,  0.3191,  0.7761, -0.4302, -0.2156,  0.5217, -0.6099,  0.3396,\n",
      "        -0.3122,  0.8661])\n",
      "R[0]\n",
      "tensor([0.0826], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02095717553049326 0.00485622803332808 4.754062779568358e-07 0.0039599495038855825 0.1721770751774311 0.0002947145476937294 0.00906879497668706\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2624, -0.4687, -0.4401,  0.1749, -0.5350, -0.1877,  0.4047, -0.1014,\n",
      "        -0.1901, -0.2688]) tensor([-0.2511, -0.4104, -0.3965,  0.1327, -0.4886, -0.1600,  0.3734, -0.1202,\n",
      "        -0.2015, -0.2268]) tensor([-0.0439, -0.5437, -0.7079,  0.3217, -0.4912, -0.0207,  0.1972, -0.2781,\n",
      "        -0.2679, -0.4849])\n",
      "R[0]\n",
      "tensor([-0.0020], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020932062720879913 0.00547765663253449 4.7359569066429685e-07 0.004151040992932394 0.17440536849200725 0.0002214679941534996 0.008988965223237756\n",
      "Average (on the epoch) training loss: 0.004159658157365629\n",
      "Episode average V value: 0\n",
      "epoch 29:\n",
      "Learning rate: 5.23347633027361e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1527,  0.3190,  0.9537, -0.6385, -0.1744,  0.1382,  0.0869,  0.4547,\n",
      "         0.3215,  0.2548]) tensor([ 0.2449,  0.3991,  0.9080, -0.6526, -0.0403,  0.3007,  0.0048,  0.2862,\n",
      "         0.2679,  0.1730]) tensor([ 0.3473,  0.5002,  0.9953, -0.6986,  0.0399,  0.3502, -0.0124,  0.3091,\n",
      "         0.3272,  0.0742])\n",
      "R[0]\n",
      "tensor([0.0001], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021338286539539696 0.005675116219004849 4.730840350646304e-07 0.004123318390804343 0.17194282133877278 0.0003883655965328217 0.008610083907376975\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1477, -0.2628,  0.5066, -0.2522, -0.8020, -0.5474,  0.3505,  0.9825,\n",
      "         0.3349,  0.1339]) tensor([-0.2182, -0.2976,  0.4789, -0.2338, -0.8209, -0.5789,  0.4240,  0.9398,\n",
      "         0.3466,  0.1672]) tensor([-0.2959, -0.2571,  0.5440, -0.3043, -0.8395, -0.5694,  0.3602,  0.9578,\n",
      "         0.2912,  0.4021])\n",
      "R[0]\n",
      "tensor([0.0068], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021009722467511892 0.0054529506352009776 4.76790088157486e-07 0.004049920397228561 0.17380883595347404 0.0002390158250927925 0.008525835942622507\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4123,  0.5218,  0.8903, -0.6008,  0.1660,  0.5416, -0.1453,  0.2269,\n",
      "         0.0706,  0.2118]) tensor([ 0.4232,  0.5644,  0.8895, -0.6210,  0.1999,  0.5776, -0.1564,  0.1827,\n",
      "         0.0850,  0.1467]) tensor([ 0.2848,  0.4491,  0.9334, -0.6269,  0.0174,  0.3587, -0.0618,  0.4424,\n",
      "         0.1127,  0.2823])\n",
      "R[0]\n",
      "tensor([-0.0146], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021001798883080482 0.004791974943822425 4.647965642732288e-07 0.004248504122660961 0.17278586891293526 0.0002435634210705757 0.008576138149248435\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5285, -0.3523,  0.3488, -0.2198, -0.9038, -0.5400,  0.4078,  0.6848,\n",
      "         0.1676,  0.5688]) tensor([-0.4459, -0.3553,  0.3297, -0.1955, -0.8529, -0.5212,  0.4389,  0.6749,\n",
      "         0.1980,  0.4475]) tensor([-0.5240, -0.3416,  0.4484, -0.2637, -0.9373, -0.6276,  0.4150,  0.7960,\n",
      "         0.2331,  0.6486])\n",
      "R[0]\n",
      "tensor([0.0065], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020891541143879293 0.005737654977281636 4.61358692604108e-07 0.004116118065081537 0.17508336488902568 0.00027666613459587097 0.008056936792912892\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2578,  0.1257,  0.5140, -0.4985, -0.4911,  0.0575, -0.0578,  0.1663,\n",
      "         0.0982,  0.5767]) tensor([-0.2600,  0.1149,  0.4794, -0.4797, -0.4919,  0.0635, -0.0526,  0.1534,\n",
      "         0.0881,  0.5452]) tensor([-0.1086,  0.2261,  0.7318, -0.5333, -0.3383,  0.0964, -0.1489,  0.3086,\n",
      "        -0.0449,  0.8548])\n",
      "R[0]\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02109837348200381 0.005246240470223711 4.5465033610980756e-07 0.0044169605380739085 0.174837783947587 0.00023439976572990417 0.009011740734800697\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4050, -0.5487, -0.0959,  0.0847, -0.7986, -0.5028,  0.7272,  0.4076,\n",
      "         0.3295, -0.3816]) tensor([-0.4163, -0.5202, -0.0847,  0.0721, -0.7837, -0.4886,  0.7098,  0.3936,\n",
      "         0.3148, -0.3420]) tensor([-0.4304, -0.5227,  0.1112, -0.0127, -0.9103, -0.6059,  0.6200,  0.6277,\n",
      "         0.2956, -0.0133])\n",
      "R[0]\n",
      "tensor([0.0007], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021240247992798687 0.005723648785118712 4.82387090642078e-07 0.0043108462914824485 0.1725617098659277 0.0003014974445104599 0.009029003691626713\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0486, -0.2104, -0.6028,  0.0045, -0.1851,  0.6137, -0.1564, -0.3321,\n",
      "        -0.5647, -0.2028]) tensor([ 0.0855, -0.1365, -0.5096, -0.0595, -0.1293,  0.6106, -0.1456, -0.3154,\n",
      "        -0.5149, -0.2264]) tensor([ 0.0858, -0.3938, -0.6892,  0.0756, -0.2523,  0.4634,  0.0988, -0.2474,\n",
      "        -0.3416, -0.4772])\n",
      "R[0]\n",
      "tensor([0.0615], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021050666380673648 0.0054665389794281505 4.689886507094343e-07 0.004387325557647273 0.17234950579702854 0.00026177525520324705 0.00791121483547613\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2143,  0.1978,  0.3995, -0.2892, -0.0206,  0.5237, -0.0694, -0.0697,\n",
      "        -0.0932,  0.0885]) tensor([ 0.2233,  0.1792,  0.3529, -0.2643,  0.0017,  0.5444, -0.0705, -0.1033,\n",
      "        -0.1204,  0.0683]) tensor([ 0.3259,  0.2169,  0.2878, -0.2374,  0.0605,  0.6230, -0.0825, -0.1862,\n",
      "        -0.0472, -0.1717])\n",
      "R[0]\n",
      "tensor([-0.0135], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021294290136545897 0.00531750976380863 4.5200419185675856e-07 0.0045883456619339996 0.17191132393479347 0.00026855535060167315 0.009208500422944781\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5439,  0.3313,  0.7724, -0.4106, -0.0706,  0.5921, -0.6262,  0.4596,\n",
      "        -0.3676,  0.6492]) tensor([ 0.5228,  0.4361,  0.8380, -0.4685, -0.0158,  0.6320, -0.5863,  0.3806,\n",
      "        -0.2686,  0.5419]) tensor([ 0.5135,  0.4103,  0.5474, -0.3936, -0.0259,  0.7669, -0.8032,  0.1182,\n",
      "        -0.4842,  0.5695])\n",
      "R[0]\n",
      "tensor([0.1266], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021321879072114826 0.005566201550558617 4.4906353346618743e-07 0.00440570766403107 0.17103899006545545 0.000282840296626091 0.008271325662557501\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5565, -0.1963,  0.1110, -0.2434, -0.6173, -0.0393,  0.1836,  0.0529,\n",
      "        -0.2256,  0.6215]) tensor([-0.4801, -0.1149,  0.1240, -0.2742, -0.5230,  0.0605,  0.1183, -0.0449,\n",
      "        -0.2187,  0.5555]) tensor([-0.4615, -0.0879, -0.1114, -0.2066, -0.3401,  0.3069,  0.1336, -0.4681,\n",
      "        -0.2840,  0.3867])\n",
      "R[0]\n",
      "tensor([-0.0012], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021314374338835478 0.005574377032302436 4.4765207482555523e-07 0.004313040108710993 0.17002865482866764 0.00022079204022884368 0.008673064253933263\n",
      "Average (on the epoch) training loss: 0.004296008679765509\n",
      "Episode average V value: 0\n",
      "epoch 30:\n",
      "Learning rate: 4.710128697246249e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5385, -0.6232, -0.4529,  0.2120, -0.7231, -0.4036,  0.8534,  0.0359,\n",
      "         0.2775, -0.6676]) tensor([-0.6236, -0.6883, -0.4227,  0.2218, -0.8641, -0.4616,  0.8127,  0.1564,\n",
      "         0.2053, -0.4468]) tensor([-0.5443, -0.6464, -0.2156,  0.1462, -0.8509, -0.5350,  0.7856,  0.3052,\n",
      "         0.2652, -0.2688])\n",
      "R[0]\n",
      "tensor([-0.0044], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021435345083475114 0.005628592457138438 4.5181921552739367e-07 0.004488409996964037 0.1720065757930279 0.00020337864011526108 0.008511525929381605\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5741, -0.1641,  0.1963, -0.2950, -0.6465, -0.0943,  0.1778,  0.1545,\n",
      "        -0.1968,  0.6731]) tensor([-0.5193, -0.1073,  0.1962, -0.3046, -0.5859, -0.0500,  0.1546,  0.1148,\n",
      "        -0.1869,  0.5811]) tensor([-0.5757, -0.1535,  0.2370, -0.3139, -0.6593, -0.1236,  0.1809,  0.2044,\n",
      "        -0.1753,  0.6912])\n",
      "R[0]\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021200671928003432 0.006021425758386613 4.5190459502464364e-07 0.0043866960886516605 0.17500313614308835 0.0002466902732849121 0.008780730883183423\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4491, -0.3363, -0.1589, -0.0980, -0.5572,  0.0489,  0.2344, -0.0150,\n",
      "        -0.3004,  0.3966]) tensor([-0.4046, -0.2851, -0.1363, -0.1182, -0.5063,  0.0665,  0.2269, -0.0241,\n",
      "        -0.2800,  0.3268]) tensor([-0.5234, -0.2358,  0.0757, -0.1941, -0.6087, -0.0899,  0.2415,  0.0855,\n",
      "        -0.2122,  0.5393])\n",
      "R[0]\n",
      "tensor([0.0224], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021196672121062875 0.005297381803487952 4.4065839398399476e-07 0.0048745029697893185 0.1743567263484001 0.0002812575325369835 0.008554486478446052\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0036, -0.5927, -0.4814,  0.2191, -0.4876, -0.1314,  0.4477,  0.1173,\n",
      "        -0.0679, -0.6135]) tensor([-0.0164, -0.5439, -0.4202,  0.1695, -0.4780, -0.1308,  0.4281,  0.1322,\n",
      "        -0.0618, -0.5447]) tensor([-0.1294, -0.6390, -0.5534,  0.2216, -0.6165, -0.1150,  0.3622,  0.0955,\n",
      "        -0.1715, -0.4229])\n",
      "R[0]\n",
      "tensor([-0.0003], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020952743669971824 0.005533237058218219 4.3021436499657284e-07 0.004334340401168447 0.17509398543834687 0.0002724355459213257 0.00873552594362991\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0741,  0.0435,  0.3151, -0.2764, -0.4079,  0.3596, -0.3396, -0.0707,\n",
      "        -0.2760,  0.6690]) tensor([-0.0334,  0.0708,  0.3054, -0.2846, -0.3555,  0.3932, -0.3442, -0.1295,\n",
      "        -0.2413,  0.5834]) tensor([-0.0372,  0.0238,  0.1040, -0.1936, -0.3641,  0.4644, -0.2993, -0.4144,\n",
      "        -0.1632,  0.3008])\n",
      "R[0]\n",
      "tensor([-0.0096], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021614420607686043 0.0052984135116203104 4.3378615092137807e-07 0.004618121628125664 0.17225185544788837 0.00022385308146476745 0.008757801731815562\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4960, -0.0204,  0.2669, -0.3641, -0.5015,  0.0633,  0.0578,  0.0215,\n",
      "        -0.2544,  0.7406]) tensor([-0.5374, -0.1740,  0.1804, -0.2753, -0.6105, -0.0444,  0.1414,  0.1228,\n",
      "        -0.2727,  0.7420]) tensor([-0.4974, -0.2920,  0.0055, -0.1448, -0.6066,  0.0037,  0.2061,  0.0083,\n",
      "        -0.2463,  0.5678])\n",
      "R[0]\n",
      "tensor([-0.0006], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02128646407648921 0.005602084613237821 4.375956543611892e-07 0.0044691856483696024 0.17380954872071744 0.00030142631381750105 0.008568005677661859\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5513, -0.4988, -0.3293,  0.0676, -0.6528, -0.0628,  0.3914, -0.3611,\n",
      "        -0.1824,  0.1895]) tensor([-0.5150, -0.4601, -0.3100,  0.0530, -0.6170, -0.0523,  0.3690, -0.3543,\n",
      "        -0.1851,  0.1617]) tensor([-0.4897, -0.3360, -0.1398, -0.0683, -0.5546,  0.0160,  0.2929, -0.3900,\n",
      "        -0.1447,  0.2944])\n",
      "R[0]\n",
      "tensor([0.0139], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021111129827797413 0.005236766342379269 4.201680182234213e-07 0.004529335905332118 0.17566884899139404 0.00033861181885004045 0.00826160483676358\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2356,  0.1058,  0.3614, -0.3691, -0.3211,  0.1229,  0.1543,  0.2412,\n",
      "        -0.1258,  0.1396]) tensor([-0.1923,  0.1360,  0.3778, -0.3745, -0.2875,  0.1259,  0.1612,  0.2482,\n",
      "        -0.1006,  0.0685]) tensor([-0.3474, -0.1215,  0.0703, -0.1343, -0.4769, -0.0740,  0.3261,  0.1204,\n",
      "        -0.0520, -0.0978])\n",
      "R[0]\n",
      "tensor([-0.0085], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02101799904182553 0.005105632250124472 4.247903571013012e-07 0.0041465102239744735 0.17662395584583282 0.0002948162704706192 0.008715511250076816\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6988,  0.7962,  0.8974, -0.6548,  0.4070,  0.6201, -0.0796,  0.0512,\n",
      "         0.4766, -0.7489]) tensor([ 0.7068,  0.8387,  0.8532, -0.6561,  0.4735,  0.7350, -0.1522, -0.0819,\n",
      "         0.3879, -0.7109]) tensor([ 0.7306,  0.8618,  0.8764, -0.6758,  0.4872,  0.7104, -0.1249, -0.0522,\n",
      "         0.4483, -0.7606])\n",
      "R[0]\n",
      "tensor([-0.0008], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02120272159203887 0.005333997114183148 4.1288265978778324e-07 0.004662436010781676 0.17388297820091247 0.0002496933490037918 0.008582381612213794\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1044,  0.0598,  0.7941, -0.4657, -0.3415, -0.1221,  0.3192,  0.6197,\n",
      "         0.4481,  0.0895]) tensor([ 0.0477, -0.0016,  0.7173, -0.4219, -0.4020, -0.1568,  0.3146,  0.6116,\n",
      "         0.3993,  0.1282]) tensor([ 0.0274, -0.0042,  0.7738, -0.4455, -0.4319, -0.2224,  0.3556,  0.6861,\n",
      "         0.4492,  0.1556])\n",
      "R[0]\n",
      "tensor([0.0037], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021001286432147027 0.006040596497048682 4.143746018314687e-07 0.00471630458231084 0.1743736609518528 0.00029816997796297073 0.008269414020818659\n",
      "Average (on the epoch) training loss: 0.0045225843455467835\n",
      "Episode average V value: 0\n",
      "epoch 31:\n",
      "Learning rate: 4.239115827521624e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0060,  0.4322,  0.4705, -0.5845,  0.0292,  0.4682, -0.0828, -0.3760,\n",
      "         0.0279,  0.2194]) tensor([-4.4686e-04,  4.0550e-01,  4.5083e-01, -5.5925e-01,  1.4977e-02,\n",
      "         4.5922e-01, -6.8720e-02, -3.8795e-01,  5.5246e-02,  1.8408e-01]) tensor([-0.0046,  0.4372,  0.4805, -0.5894,  0.0318,  0.4635, -0.0799, -0.3664,\n",
      "         0.0306,  0.2198])\n",
      "R[0]\n",
      "tensor([0.0037], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02138231598958373 0.006147845909094031 4.1053345410091423e-07 0.004990368156461045 0.1733270131200552 0.0003429588079452515 0.008855600067181512\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6953, -0.6039, -0.0925, -0.0037, -0.9162, -0.5125,  0.7372,  0.3245,\n",
      "         0.2113,  0.1052]) tensor([-0.6620, -0.5596, -0.0867, -0.0110, -0.8659, -0.4748,  0.7100,  0.2951,\n",
      "         0.1940,  0.0934]) tensor([-0.6608, -0.6368, -0.2774,  0.1211, -0.8398, -0.4903,  0.8256,  0.1676,\n",
      "         0.2387, -0.2299])\n",
      "R[0]\n",
      "tensor([0.0042], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021252255011349915 0.005986052573549386 3.9573282995775117e-07 0.004701129984401632 0.17222166354954244 0.00032370173186063765 0.009504899671534076\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2063,  0.3357,  0.5131, -0.4036, -0.1844,  0.5600, -0.6182,  0.0021,\n",
      "        -0.3519,  0.6802]) tensor([ 0.1462,  0.3423,  0.5164, -0.4122, -0.1999,  0.5386, -0.5669, -0.0164,\n",
      "        -0.3115,  0.6495]) tensor([-0.0548,  0.1484,  0.2225, -0.2775, -0.3175,  0.5294, -0.5583, -0.1066,\n",
      "        -0.6237,  0.8044])\n",
      "R[0]\n",
      "tensor([0.0546], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021426946071907878 0.005441615824944165 4.019331508828827e-07 0.004334395157697145 0.171821920722723 0.00022852201014757158 0.007923423490283313\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0734,  0.1428,  0.1907, -0.2596, -0.3018,  0.3738, -0.2678, -0.3970,\n",
      "        -0.1545,  0.3247]) tensor([-0.0509,  0.1585,  0.1861, -0.2646, -0.2620,  0.4090, -0.2737, -0.4586,\n",
      "        -0.1362,  0.2956]) tensor([ 0.0261,  0.3158,  0.4425, -0.3947, -0.1881,  0.4637, -0.4542, -0.1728,\n",
      "        -0.3288,  0.6775])\n",
      "R[0]\n",
      "tensor([-0.0049], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020986314607784152 0.005867002991071786 4.070838640188867e-07 0.004370714382908773 0.17515652208030225 0.00032415734976530075 0.008390968686260748\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2601, -0.4485, -0.1022,  0.0669, -0.6276, -0.3693,  0.7550,  0.3142,\n",
      "         0.3928, -0.6871]) tensor([-0.0647, -0.2714,  0.0236, -0.0135, -0.4884, -0.2178,  0.6245,  0.3225,\n",
      "         0.4454, -0.8218]) tensor([-0.0073, -0.2497,  0.2262, -0.0953, -0.5202, -0.3216,  0.6247,  0.5531,\n",
      "         0.4742, -0.6755])\n",
      "R[0]\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020982771677896382 0.006262120294784836 3.93913805766033e-07 0.00483603223063983 0.17605949115753175 0.0002785501629114151 0.008905670538835692\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5140, -0.3665, -0.3845,  0.0413, -0.4756,  0.1914,  0.2887, -0.5780,\n",
      "        -0.3040,  0.1668]) tensor([-0.4696, -0.3356, -0.3519,  0.0271, -0.4533,  0.1808,  0.2754, -0.5373,\n",
      "        -0.2908,  0.1254]) tensor([-0.3376, -0.1533, -0.3451, -0.0371, -0.2089,  0.3969,  0.2229, -0.6547,\n",
      "        -0.3358, -0.0393])\n",
      "R[0]\n",
      "tensor([0.0105], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021283194394782184 0.006649018789517868 3.9790493548252925e-07 0.005020823841332458 0.17697041521966458 0.00022489482909440994 0.0084295684263343\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5125, -0.1987,  0.1499, -0.2590, -0.5794, -0.0483,  0.2456, -0.1422,\n",
      "        -0.0597,  0.4947]) tensor([-0.4990, -0.2906,  0.0864, -0.1952, -0.6253, -0.1030,  0.2828, -0.0733,\n",
      "        -0.0791,  0.4633]) tensor([-0.6924, -0.4886, -0.0352, -0.0746, -0.8335, -0.2933,  0.4427,  0.0201,\n",
      "        -0.0737,  0.4831])\n",
      "R[0]\n",
      "tensor([-0.0033], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021129080299288033 0.005599999209200178 3.789571817662818e-07 0.0047346216157311575 0.17436303490400315 0.00032290248572826384 0.008683701012749226\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0336,  0.0508,  0.0429, -0.1378, -0.2989,  0.5061, -0.3363, -0.5296,\n",
      "        -0.1906,  0.1799]) tensor([ 0.0440,  0.0546,  0.0467, -0.1453, -0.2740,  0.5162, -0.3266, -0.5682,\n",
      "        -0.1658,  0.1656]) tensor([ 1.6426e-04,  3.7482e-02, -3.6268e-02, -1.1580e-01, -2.9642e-01,\n",
      "         5.0167e-01, -2.6034e-01, -6.5690e-01, -1.0622e-01,  1.3078e-02])\n",
      "R[0]\n",
      "tensor([-0.0078], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021024720843881368 0.005313450212306634 3.8957828743946267e-07 0.004474736908567138 0.17764661948382854 0.00026028185337781905 0.008241275306325405\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3356,  0.1207,  0.6124, -0.3157, -0.2509, -0.0647,  0.3631,  0.5749,\n",
      "         0.5791, -0.7302]) tensor([ 0.1231, -0.1376,  0.5016, -0.2063, -0.5448, -0.3294,  0.4729,  0.7738,\n",
      "         0.5276, -0.5432]) tensor([ 0.1993, -0.0598,  0.6165, -0.2938, -0.5190, -0.3535,  0.3506,  0.8985,\n",
      "         0.5220, -0.4069])\n",
      "R[0]\n",
      "tensor([-0.0038], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021243695735931396 0.0058709223285259215 3.7974723909428576e-07 0.004614474449073896 0.17323121389746665 0.00027597463876008986 0.008705816418922041\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1113, -0.3426,  0.3296, -0.1209, -0.7793, -0.5499,  0.4725,  0.8598,\n",
      "         0.3894, -0.2093]) tensor([-0.0138, -0.2449,  0.3873, -0.1598, -0.6917, -0.4775,  0.4493,  0.8286,\n",
      "         0.4538, -0.3392]) tensor([ 0.1310, -0.1420,  0.5239, -0.2345, -0.5896, -0.4262,  0.3932,  0.9097,\n",
      "         0.4921, -0.3953])\n",
      "R[0]\n",
      "tensor([-0.0007], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021124039562419056 0.005653211885597557 3.662049784196597e-07 0.004822656716103666 0.17342104563117028 0.00028337272256612775 0.00884907911682967\n",
      "Average (on the epoch) training loss: 0.004689995344291674\n",
      "Episode average V value: 0\n",
      "epoch 32:\n",
      "Learning rate: 3.815204244769462e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3864, -0.4997, -0.3610,  0.1167, -0.5887, -0.2846,  0.5498, -0.0381,\n",
      "        -0.0784, -0.2200]) tensor([-0.2546, -0.4081, -0.3044,  0.0791, -0.4828, -0.1851,  0.4676, -0.0393,\n",
      "        -0.0740, -0.2810]) tensor([-0.4050, -0.5795, -0.5320,  0.2460, -0.5891, -0.2819,  0.5732, -0.1716,\n",
      "        -0.1355, -0.3343])\n",
      "R[0]\n",
      "tensor([0.0062], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021179248161613942 0.005770264723490982 3.8148290170170183e-07 0.004452051926695276 0.17187625753879548 0.0002493270859122276 0.00920741125906352\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7928, -0.6958, -0.3634,  0.1544, -0.9328, -0.4577,  0.6628,  0.0415,\n",
      "        -0.0742,  0.1301]) tensor([-0.7682, -0.6293, -0.3107,  0.1084, -0.8935, -0.3702,  0.5656, -0.0014,\n",
      "        -0.1236,  0.2368]) tensor([-0.7147, -0.5541, -0.1126, -0.0095, -0.8667, -0.3347,  0.4981,  0.0219,\n",
      "        -0.0756,  0.4167])\n",
      "R[0]\n",
      "tensor([-0.0035], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021093797845765948 0.005031909851924866 3.7744367254788357e-07 0.004299880216713063 0.17377512876689433 0.00026865769922733304 0.0076779501621495\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5146, -0.1027, -0.0670, -0.0712, -0.0328,  0.0311,  0.1412,  0.4486,\n",
      "        -0.0834, -0.4536]) tensor([ 0.4378, -0.0920, -0.0497, -0.1067, -0.0441, -0.0032,  0.1784,  0.4214,\n",
      "        -0.0591, -0.4010]) tensor([ 0.4798,  0.1772,  0.3694, -0.3587, -0.0098,  0.0468,  0.0594,  0.6965,\n",
      "         0.0020, -0.2645])\n",
      "R[0]\n",
      "tensor([0.0857], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020912627821788193 0.005514784762985073 3.7901125290318307e-07 0.004397145288938191 0.17532718439400197 0.00021622614562511445 0.008348321186495013\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2926,  0.1769,  0.7767, -0.5926, -0.5002, -0.0809,  0.0043,  0.5083,\n",
      "         0.1050,  0.7849]) tensor([-2.8365e-01,  1.7328e-01,  7.3721e-01, -5.7172e-01, -4.9526e-01,\n",
      "        -6.1678e-02,  6.1959e-05,  4.9106e-01,  9.3773e-02,  7.3368e-01]) tensor([-0.3825,  0.0304,  0.7450, -0.5144, -0.6446, -0.2569,  0.1071,  0.6291,\n",
      "         0.1371,  0.8498])\n",
      "R[0]\n",
      "tensor([-0.0102], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020676369909197093 0.004741191649824032 3.752847062230558e-07 0.004381495233392343 0.1740481448471546 0.00024150250852108003 0.00889056928199716\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5947,  0.5691,  0.9237, -0.5744,  0.1447,  0.3342,  0.0695,  0.3541,\n",
      "         0.5739, -0.7102]) tensor([ 0.6704,  0.7032,  0.9241, -0.6219,  0.2901,  0.5337, -0.0533,  0.1646,\n",
      "         0.5068, -0.7321]) tensor([ 0.6544,  0.7000,  0.9296, -0.6262,  0.2853,  0.4837, -0.0165,  0.2066,\n",
      "         0.5256, -0.7269])\n",
      "R[0]\n",
      "tensor([0.0011], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02145826910249889 0.005399876406641852 3.65627617711084e-07 0.004645513614057563 0.17281516233086586 0.0002581971436738968 0.008512346576957498\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1764, -0.0133,  0.1504, -0.2409, -0.4405,  0.2783, -0.1282, -0.2298,\n",
      "        -0.0663,  0.3897]) tensor([-0.2046, -0.0327,  0.1288, -0.2301, -0.4574,  0.2565, -0.1077, -0.2237,\n",
      "        -0.0728,  0.3872]) tensor([-0.0202,  0.3821,  0.3873, -0.4994, -0.1475,  0.4745, -0.2711, -0.2442,\n",
      "        -0.0245,  0.3077])\n",
      "R[0]\n",
      "tensor([0.0108], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020825847666710615 0.005482221738257067 3.6890875631456763e-07 0.004446522448211908 0.17641108374297618 0.0002558630406856537 0.008592826378531754\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6693, -0.4006, -0.5944, -0.1076, -0.6591,  0.3495,  0.0915, -0.1623,\n",
      "        -0.5663,  0.4061]) tensor([-0.6020, -0.3414, -0.5305, -0.1411, -0.6025,  0.3411,  0.1087, -0.1351,\n",
      "        -0.5132,  0.3163]) tensor([-0.4707, -0.6334, -0.7201,  0.0511, -0.7922,  0.2259, -0.0112,  0.1266,\n",
      "        -0.6697,  0.3627])\n",
      "R[0]\n",
      "tensor([0.0877], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02067251137830317 0.004780059819444432 3.6177916565804935e-07 0.004124380538880359 0.17409841376543045 0.00023032031953334808 0.008575815903313924\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4399, -0.0323, -0.3395, -0.1217, -0.2515,  0.3741,  0.1265, -0.7146,\n",
      "        -0.3004,  0.0522]) tensor([-0.4527, -0.1361, -0.3771, -0.0622, -0.3255,  0.3078,  0.1480, -0.6024,\n",
      "        -0.3445,  0.1106]) tensor([-0.4979, -0.2584, -0.6302,  0.0087, -0.3350,  0.2687,  0.3384, -0.6741,\n",
      "        -0.2769, -0.1741])\n",
      "R[0]\n",
      "tensor([0.0086], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021185445316135882 0.005459884969211999 3.6385399801019956e-07 0.00462714021379361 0.17531979313492774 0.000239266961812973 0.00846300043858355\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2951, -0.2705, -0.4684,  0.0317, -0.4830,  0.4544, -0.1703, -0.5399,\n",
      "        -0.4896,  0.2561]) tensor([-0.2525, -0.2183, -0.4162, -0.0043, -0.4323,  0.4464, -0.1573, -0.5314,\n",
      "        -0.4493,  0.1990]) tensor([-0.1788, -0.0580, -0.3217, -0.0605, -0.3465,  0.5865, -0.2279, -0.7517,\n",
      "        -0.2750, -0.0111])\n",
      "R[0]\n",
      "tensor([0.0018], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021283863004297018 0.005137528289676993 3.473494093384488e-07 0.004435679830727167 0.172386100679636 0.00022103980928659439 0.008504395157215185\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3523,  0.5398,  0.4738, -0.5383,  0.3256,  0.7090, -0.0342, -0.3089,\n",
      "         0.0900, -0.2499]) tensor([ 0.3495,  0.5270,  0.4753, -0.5372,  0.3083,  0.6846, -0.0332, -0.2721,\n",
      "         0.0863, -0.2516]) tensor([ 0.4831,  0.7518,  0.5479, -0.6265,  0.4919,  0.8566, -0.1530, -0.3887,\n",
      "         0.1132, -0.3824])\n",
      "R[0]\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0212716237641871 0.0060665695912612136 3.603579905728793e-07 0.004583788338059094 0.17447681581974028 0.0003114093318581581 0.00829577381076524\n",
      "Average (on the epoch) training loss: 0.004439359764946857\n",
      "Episode average V value: 0\n",
      "epoch 33:\n",
      "Learning rate: 3.4336838202925156e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1160, -0.6028, -0.6843,  0.1685, -0.3591,  0.3763,  0.2563, -0.3331,\n",
      "        -0.3641, -0.0866]) tensor([-0.0964, -0.5761, -0.6084,  0.1287, -0.3612,  0.3704,  0.2282, -0.2811,\n",
      "        -0.3506, -0.0251]) tensor([-0.2698, -0.5050, -0.4289,  0.0891, -0.4401,  0.3434,  0.1661, -0.3225,\n",
      "        -0.3964,  0.3442])\n",
      "R[0]\n",
      "tensor([-0.0020], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021196729302406313 0.00568250457260001 3.588835006667068e-07 0.00463771703466773 0.17286010609567165 0.00023641324788331984 0.008344857223069994\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4486,  0.7860,  0.4860, -0.5992,  0.4938,  0.8777, -0.1810, -0.3556,\n",
      "         0.0283, -0.5136]) tensor([ 0.4492,  0.7961,  0.5133, -0.6120,  0.4973,  0.8650, -0.1730, -0.3336,\n",
      "         0.0396, -0.5127]) tensor([ 0.4273,  0.7752,  0.7058, -0.6762,  0.4167,  0.7969, -0.2075, -0.1419,\n",
      "         0.0140, -0.2090])\n",
      "R[0]\n",
      "tensor([-0.0079], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020828187681734563 0.005570567431208474 3.480874689216762e-07 0.004377461448137183 0.17267359983921052 0.0002572660893201828 0.008561599202046636\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5456,  0.4692,  0.9099, -0.5360,  0.0209,  0.2297,  0.1097,  0.4486,\n",
      "         0.6026, -0.6962]) tensor([ 0.5780,  0.5464,  0.9013, -0.5672,  0.1225,  0.3348,  0.0666,  0.3069,\n",
      "         0.5858, -0.7209]) tensor([ 0.6378,  0.6452,  0.9282, -0.6048,  0.2219,  0.4304,  0.0074,  0.2623,\n",
      "         0.5487, -0.7326])\n",
      "R[0]\n",
      "tensor([0.0031], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02084680755250156 0.006237125347826804 3.5020403274188536e-07 0.00448271404649131 0.17371003098785878 0.00024249423295259477 0.008632193720084615\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3897, -0.2952, -0.5081,  0.0831, -0.3447,  0.1286,  0.4697, -0.4964,\n",
      "        -0.1262, -0.4430]) tensor([-0.3702, -0.2828, -0.4681,  0.0728, -0.3451,  0.1085,  0.4528, -0.4359,\n",
      "        -0.1346, -0.4080]) tensor([-0.4998, -0.4576, -0.5742,  0.1546, -0.5297, -0.0793,  0.5926, -0.3217,\n",
      "        -0.0420, -0.5030])\n",
      "R[0]\n",
      "tensor([-0.0117], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021259839015081525 0.00563762347780721 3.4749065352457363e-07 0.004608345701242797 0.1727433335930109 0.00023186096549034118 0.009065227749873884\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2685, -0.3849, -0.7447,  0.1045, -0.2825,  0.1748,  0.4689, -0.5219,\n",
      "        -0.1709, -0.5933]) tensor([-0.1686, -0.3309, -0.6810,  0.0772, -0.2258,  0.2595,  0.3721, -0.4687,\n",
      "        -0.2014, -0.5591]) tensor([-0.4689, -0.3781, -0.6439,  0.0678, -0.3893,  0.2385,  0.3572, -0.5851,\n",
      "        -0.2946, -0.1162])\n",
      "R[0]\n",
      "tensor([0.0248], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02146129167638719 0.005847419474506751 3.4356371375565684e-07 0.004431453912460711 0.17036695921421052 0.000274937130510807 0.008110601732681971\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4066,  0.2294,  0.7751, -0.4156, -0.1987, -0.0160,  0.2748,  0.6258,\n",
      "         0.6167, -0.6606]) tensor([ 0.5150,  0.3946,  0.8220, -0.4855, -0.0511,  0.1776,  0.1505,  0.4784,\n",
      "         0.5969, -0.7321]) tensor([ 0.5019,  0.3807,  0.8676, -0.4926, -0.0542,  0.1454,  0.1698,  0.5247,\n",
      "         0.6043, -0.6710])\n",
      "R[0]\n",
      "tensor([-0.0025], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021344402920454742 0.005590767147579754 3.432477614353502e-07 0.0048689894911949525 0.17327913767099382 0.00027608736604452136 0.00878873947483953\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4441, -0.2057,  0.5604, -0.3475, -0.8619, -0.5107,  0.2663,  0.8195,\n",
      "         0.2078,  0.6941]) tensor([-0.3952, -0.2509,  0.5210, -0.3073, -0.8500, -0.5151,  0.3235,  0.8158,\n",
      "         0.2333,  0.5976]) tensor([-0.5187, -0.4186,  0.2942, -0.1569, -0.9457, -0.6043,  0.4826,  0.7047,\n",
      "         0.2294,  0.4165])\n",
      "R[0]\n",
      "tensor([0.0098], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020795015091076494 0.0051645832251488175 3.4217059155139396e-07 0.004219410899939248 0.17436258393526077 0.00018897397816181182 0.0090046196191106\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2047,  0.2403,  0.6198, -0.4643, -0.2827, -0.0363,  0.2667,  0.5306,\n",
      "         0.5346, -0.5144]) tensor([ 0.0439,  0.0569,  0.5262, -0.3788, -0.4805, -0.2047,  0.3397,  0.6316,\n",
      "         0.4791, -0.3582]) tensor([ 0.0373, -0.1031,  0.3493, -0.2195, -0.4955, -0.2484,  0.4907,  0.5883,\n",
      "         0.4864, -0.5849])\n",
      "R[0]\n",
      "tensor([0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021171293593943118 0.005827759759107721 3.4240617078751256e-07 0.0046089727974613194 0.17126477827131747 0.00025773485749959945 0.008262323808099608\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3309,  0.1014,  0.7409, -0.5512, -0.5775, -0.1531,  0.0471,  0.5799,\n",
      "         0.1210,  0.7952]) tensor([-0.3189,  0.0998,  0.7023, -0.5307, -0.5671, -0.1321,  0.0454,  0.5576,\n",
      "         0.1107,  0.7420]) tensor([-0.2190,  0.2778,  0.7485, -0.6223, -0.3960,  0.0466, -0.0767,  0.3976,\n",
      "         0.0692,  0.7098])\n",
      "R[0]\n",
      "tensor([-0.0053], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021179160881787538 0.005572665087780478 3.368212340149057e-07 0.004400749717722647 0.17250871481001376 0.00028994118422269823 0.009308143093367107\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1805,  0.3067,  0.7215, -0.4109, -0.2566,  0.3705, -0.5331,  0.2666,\n",
      "        -0.2820,  0.8383]) tensor([ 0.2004,  0.3305,  0.6790, -0.4091, -0.2067,  0.4170, -0.5462,  0.1875,\n",
      "        -0.2672,  0.7485]) tensor([ 0.0581,  0.2903,  0.4049, -0.4035, -0.1953,  0.5745, -0.4788, -0.1955,\n",
      "        -0.3027,  0.5978])\n",
      "R[0]\n",
      "tensor([-0.0074], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021063342632725836 0.005650556351076375 3.2611252926528777e-07 0.004518291687039891 0.17379295426607133 0.00021902084350585937 0.00863189020869322\n",
      "Average (on the epoch) training loss: 0.004515410673635779\n",
      "Episode average V value: 0\n",
      "epoch 34:\n",
      "Learning rate: 3.090315438263264e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1414,  0.4565, -0.0702, -0.3328,  0.3002,  0.7991,  0.0095, -0.7942,\n",
      "        -0.0237, -0.6341]) tensor([ 0.1830,  0.4753, -0.0142, -0.3500,  0.3089,  0.7895, -0.0047, -0.7295,\n",
      "        -0.0114, -0.6286]) tensor([ 0.3204,  0.6140,  0.0955, -0.4208,  0.4342,  0.8662, -0.0557, -0.7374,\n",
      "         0.0627, -0.7272])\n",
      "R[0]\n",
      "tensor([-0.0256], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02118867453187704 0.005589826617684594 3.313706570793329e-07 0.00458044774265727 0.17115679574012757 0.00029903542250394824 0.008799591436749325\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2858,  0.5701,  1.0005, -0.7265,  0.0828,  0.4055, -0.0726,  0.4363,\n",
      "         0.1473,  0.1878]) tensor([ 0.2290,  0.4571,  0.9140, -0.6511, -0.0173,  0.3046, -0.0145,  0.4941,\n",
      "         0.1577,  0.1544]) tensor([ 0.0031,  0.3053,  0.6701, -0.5263, -0.1390,  0.2729,  0.0300,  0.3220,\n",
      "        -0.0322,  0.2207])\n",
      "R[0]\n",
      "tensor([-0.0004], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021197498990222812 0.0059463907674253275 3.3368088064378297e-07 0.004434614726167638 0.1730928652435541 0.0002478373274207115 0.008499786384811159\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5027, -0.2098,  0.5997, -0.3767, -0.8606, -0.5537,  0.3248,  0.7660,\n",
      "         0.2485,  0.7717]) tensor([-0.4893, -0.2173,  0.5612, -0.3518, -0.8451, -0.5413,  0.3369,  0.7373,\n",
      "         0.2385,  0.7273]) tensor([-0.5109, -0.2286,  0.5933, -0.3697, -0.8787, -0.5863,  0.3550,  0.7924,\n",
      "         0.2695,  0.7548])\n",
      "R[0]\n",
      "tensor([-0.0032], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021026160264387726 0.005066319538957032 3.2108934986752044e-07 0.004386302952014375 0.17180375395715236 0.00023994063585996628 0.008788184852630365\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.7901,  0.9799,  0.8059, -0.7068,  0.6383,  0.8967, -0.2158, -0.2725,\n",
      "         0.3881, -0.7940]) tensor([ 0.7855,  0.9625,  0.8055, -0.6986,  0.6365,  0.8549, -0.1765, -0.2535,\n",
      "         0.4140, -0.8223]) tensor([ 0.7914,  0.9821,  0.8044, -0.7072,  0.6414,  0.9003, -0.2175, -0.2772,\n",
      "         0.3870, -0.7947])\n",
      "R[0]\n",
      "tensor([-0.0064], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020881834339350463 0.005523622288437764 3.30072951726379e-07 0.004890602793369908 0.17337039199471474 0.0003484390750527382 0.008877007593342569\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3977,  0.5542,  0.0939, -0.4025,  0.4546,  0.8463,  0.0087, -0.6069,\n",
      "         0.0592, -0.8537]) tensor([ 0.2985,  0.3917,  0.0631, -0.3346,  0.2858,  0.6806,  0.1036, -0.4689,\n",
      "         0.0841, -0.7652]) tensor([ 0.0255,  0.2680, -0.1314, -0.2393,  0.1354,  0.6331,  0.1259, -0.6246,\n",
      "        -0.0799, -0.6309])\n",
      "R[0]\n",
      "tensor([0.0102], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021220921769738197 0.00558287373703206 3.2331741823554696e-07 0.004562293309776578 0.17217711097002028 0.0002750007808208466 0.008911873770673992\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4863, -0.2123,  0.6059, -0.3729, -0.8620, -0.5500,  0.3143,  0.7983,\n",
      "         0.2404,  0.7778]) tensor([-0.4411, -0.2742,  0.5526, -0.3227, -0.8651, -0.5657,  0.3704,  0.8086,\n",
      "         0.2559,  0.6840]) tensor([-0.5306, -0.3827,  0.4181, -0.2349, -0.9576, -0.6553,  0.4508,  0.7981,\n",
      "         0.2516,  0.6183])\n",
      "R[0]\n",
      "tensor([0.0096], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020958492256700993 0.005365939633295056 3.1187305113178356e-07 0.004449583355512004 0.1751626383960247 0.00032292493432760236 0.008797404311801075\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4906,  0.3736,  0.8911, -0.5083, -0.0766,  0.1240,  0.1710,  0.5618,\n",
      "         0.6182, -0.6351]) tensor([ 0.4869,  0.4580,  0.8754, -0.5442,  0.0163,  0.2483,  0.1176,  0.3868,\n",
      "         0.5822, -0.6228]) tensor([ 0.5782,  0.5444,  0.9360, -0.5817,  0.1046,  0.3150,  0.0690,  0.3993,\n",
      "         0.5800, -0.6725])\n",
      "R[0]\n",
      "tensor([0.0102], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021055099323391915 0.006494253559554636 3.2161452067214215e-07 0.004905949458829127 0.1763559353351593 0.00029391439259052275 0.008669703176186885\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5416, -0.3065, -0.1118, -0.1173, -0.5712,  0.0671,  0.2334, -0.4164,\n",
      "        -0.2008,  0.4263]) tensor([-0.5081, -0.2891, -0.1073, -0.1172, -0.5567,  0.0589,  0.2246, -0.3924,\n",
      "        -0.1945,  0.3726]) tensor([-0.4818, -0.1875,  0.0066, -0.2301, -0.4883,  0.1312,  0.1947, -0.4060,\n",
      "        -0.1236,  0.4213])\n",
      "R[0]\n",
      "tensor([0.0088], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02103337343968451 0.0050504014047364765 3.1781676061370943e-07 0.004277268718928098 0.17425820106267928 0.00021064402908086777 0.008381141621619462\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1540,  0.4364, -0.0444, -0.3081,  0.2844,  0.8081, -0.0570, -0.7818,\n",
      "        -0.1015, -0.4879]) tensor([ 0.1948,  0.4529,  0.0107, -0.3276,  0.2876,  0.7903, -0.0672, -0.7118,\n",
      "        -0.0836, -0.4930]) tensor([ 0.3628,  0.6383,  0.1577, -0.4308,  0.4502,  0.8914, -0.1240, -0.7050,\n",
      "         0.0176, -0.6285])\n",
      "R[0]\n",
      "tensor([-0.0257], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02069759197346866 0.005504218470108754 3.1028343035188755e-07 0.004612957387638744 0.17584961925446987 0.0003012814000248909 0.00882341735259979\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6605,  0.7759,  0.8429, -0.6461,  0.3743,  0.6419, -0.1130,  0.0140,\n",
      "         0.4250, -0.6898]) tensor([ 0.6818,  0.8079,  0.8483, -0.6598,  0.4256,  0.6700, -0.1115, -0.0373,\n",
      "         0.4371, -0.7229]) tensor([ 0.7069,  0.8537,  0.8345, -0.6737,  0.4706,  0.7399, -0.1555, -0.0860,\n",
      "         0.4084, -0.7225])\n",
      "R[0]\n",
      "tensor([-0.0015], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021098376223817467 0.0056041451162091105 3.155498548608193e-07 0.004807413481932599 0.17191752833127977 0.0003075096383690834 0.008680947714834474\n",
      "Average (on the epoch) training loss: 0.004590743392682634\n",
      "Episode average V value: 0\n",
      "epoch 35:\n",
      "Learning rate: 2.7812838944369375e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5120,  0.7857,  0.8340, -0.7396,  0.4462,  0.8338, -0.2706, -0.1183,\n",
      "         0.0069,  0.0627]) tensor([ 0.5104,  0.7372,  0.8370, -0.7256,  0.3887,  0.7451, -0.2205, -0.0184,\n",
      "         0.0534,  0.0017]) tensor([ 5.6152e-01,  8.8028e-01,  7.6746e-01, -7.4696e-01,  5.4623e-01,\n",
      "         9.2795e-01, -3.1703e-01, -2.3505e-01, -5.6358e-04, -8.5201e-02])\n",
      "R[0]\n",
      "tensor([0.0238], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021099880781024694 0.006377747235823335 3.136139691690687e-07 0.004771508374309633 0.17327477495372295 0.00025135351717472075 0.008480222156154923\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0045,  0.4241,  0.6308, -0.5923, -0.1210,  0.2835, -0.0878,  0.1116,\n",
      "         0.1108,  0.2867]) tensor([-0.0073,  0.3948,  0.5821, -0.5596, -0.1323,  0.2909, -0.0866,  0.0727,\n",
      "         0.1061,  0.2691]) tensor([-0.0737,  0.1399,  0.2490, -0.3365, -0.3079,  0.3463, -0.1549, -0.2088,\n",
      "         0.0033,  0.2912])\n",
      "R[0]\n",
      "tensor([0.0013], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020830546708777545 0.005658303602926026 3.078024352731745e-07 0.004949339456274174 0.17395559808611868 0.00020726285874843596 0.007852570127870422\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4885,  0.5812,  0.9074, -0.5950,  0.2138,  0.5435, -0.1789,  0.2040,\n",
      "         0.1123,  0.1016]) tensor([ 0.4981,  0.5738,  0.8985, -0.5941,  0.2066,  0.5164, -0.1558,  0.2262,\n",
      "         0.1436,  0.0285]) tensor([ 0.5640,  0.2494,  0.5376, -0.4048,  0.0268,  0.1322, -0.0137,  0.7155,\n",
      "         0.0540, -0.2046])\n",
      "R[0]\n",
      "tensor([0.0186], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02135089590586722 0.006054168432550796 3.1435480444486077e-07 0.004577234370517544 0.17307520797848702 0.0002306269183754921 0.00851517384161707\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6760, -0.7182, -0.4506,  0.2134, -0.9057, -0.4517,  0.7306,  0.1006,\n",
      "         0.1052, -0.2870]) tensor([-0.6986, -0.6968, -0.3963,  0.1831, -0.9377, -0.4152,  0.6474,  0.1231,\n",
      "         0.0398, -0.1114]) tensor([-0.7768, -0.7337, -0.4502,  0.2023, -0.9247, -0.4559,  0.6847,  0.0271,\n",
      "        -0.0728,  0.0241])\n",
      "R[0]\n",
      "tensor([-0.0051], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020968836588785053 0.005464681059522263 3.046647073716713e-07 0.004677450362360105 0.17524413725733756 0.00023619596660137178 0.009263393594301306\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7794, -0.7391, -0.4668,  0.1968, -0.9307, -0.4259,  0.6862,  0.0070,\n",
      "        -0.0447, -0.0078]) tensor([-0.6149, -0.6287, -0.3995,  0.1608, -0.8033, -0.3239,  0.6081,  0.0126,\n",
      "        -0.0235, -0.1175]) tensor([-0.5872, -0.5025, -0.4719,  0.1221, -0.6109,  0.0142,  0.4359, -0.4391,\n",
      "        -0.1956,  0.0320])\n",
      "R[0]\n",
      "tensor([0.0026], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021022618528455495 0.005644224567440688 3.041741423714939e-07 0.004791214305965696 0.17472952830791474 0.0002268703430891037 0.007948637335503009\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1763,  0.0772,  0.4641, -0.3844, -0.6199, -0.1561, -0.0077,  0.6288,\n",
      "         0.1376,  0.2221]) tensor([-0.1814,  0.0956,  0.4619, -0.3877, -0.5788, -0.1243,  0.0239,  0.5619,\n",
      "         0.1484,  0.2112]) tensor([-0.1775,  0.2523,  0.5532, -0.5323, -0.4468,  0.0504, -0.0859,  0.4207,\n",
      "         0.0896,  0.3540])\n",
      "R[0]\n",
      "tensor([0.0109], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020885919656604527 0.0055036234974650145 3.007778308870002e-07 0.0045271194466622545 0.17367428256571293 0.00024958407133817675 0.008199177878763293\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6305, -0.6785, -0.6606,  0.2578, -0.7107, -0.2139,  0.5584, -0.2599,\n",
      "        -0.2478, -0.0497]) tensor([-0.5167, -0.6236, -0.6125,  0.2380, -0.6411, -0.1534,  0.4913, -0.2143,\n",
      "        -0.2559, -0.0840]) tensor([-0.5492, -0.7256, -0.7522,  0.3349, -0.7040, -0.2104,  0.5103, -0.2268,\n",
      "        -0.2952, -0.1093])\n",
      "R[0]\n",
      "tensor([0.0100], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02112940815091133 0.004869986664263706 2.874019454424115e-07 0.004587290156341624 0.17166822619736194 0.00020631662756204604 0.008654919502616393\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0520,  0.3778,  0.4410, -0.5630, -0.0285,  0.4269, -0.0611, -0.3606,\n",
      "         0.0260,  0.2456]) tensor([-0.0454,  0.3548,  0.4194, -0.5383, -0.0364,  0.4258, -0.0493, -0.3817,\n",
      "         0.0507,  0.2104]) tensor([-0.0323,  0.3978,  0.4603, -0.5731, -0.0116,  0.4359, -0.0694, -0.3548,\n",
      "         0.0328,  0.2352])\n",
      "R[0]\n",
      "tensor([0.0037], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020874327655881642 0.0054970463395875415 2.998940695846386e-07 0.004323970449040644 0.17361836270987988 0.00021441293507814407 0.009305931625887751\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6603, -0.5587, -0.3643,  0.1070, -0.7418, -0.1745,  0.4884, -0.2350,\n",
      "        -0.1623,  0.1631]) tensor([-0.6193, -0.5095, -0.3414,  0.0890, -0.6948, -0.1532,  0.4617, -0.2384,\n",
      "        -0.1672,  0.1368]) tensor([-0.6164, -0.4462, -0.1045, -0.0545, -0.7294, -0.1631,  0.3959, -0.1441,\n",
      "        -0.0947,  0.3757])\n",
      "R[0]\n",
      "tensor([0.0048], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021249706733971835 0.005851181362326315 3.028257699071446e-07 0.004840317936672363 0.17238606226444245 0.00026996810734272004 0.00871899998554727\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3447, -0.0507, -0.1245,  0.1209, -0.0414,  0.5556, -0.1685, -0.4975,\n",
      "        -0.2331, -0.1466]) tensor([ 0.3158, -0.0719, -0.1110,  0.1110, -0.0607,  0.5265, -0.1502, -0.4901,\n",
      "        -0.2168, -0.0991]) tensor([ 0.5462,  0.3673,  0.4250, -0.2755,  0.2644,  0.6912, -0.1944, -0.2647,\n",
      "        -0.0672, -0.0658])\n",
      "R[0]\n",
      "tensor([-0.0230], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02089603666216135 0.005402779040494352 3.0240894076882797e-07 0.0042616110926028345 0.17421883672475816 0.0001996506080031395 0.008298523076926358\n",
      "Average (on the epoch) training loss: 0.004630705595074687\n",
      "Episode average V value: 0\n",
      "epoch 36:\n",
      "Learning rate: 2.503155504993244e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0623,  0.1220,  0.7057, -0.3528, -0.4424,  0.1829, -0.4056,  0.5097,\n",
      "        -0.2430,  0.8979]) tensor([ 0.1125,  0.1996,  0.6818, -0.3792, -0.3458,  0.2915, -0.4623,  0.3865,\n",
      "        -0.2399,  0.8020]) tensor([ 0.0353,  0.0803,  0.7153, -0.3549, -0.4747,  0.1554, -0.3641,  0.5637,\n",
      "        -0.2192,  0.9272])\n",
      "R[0]\n",
      "tensor([-0.0045], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020830852134153248 0.0056059643075059284 2.951887636868378e-07 0.004546352488163393 0.1724890453070402 0.0001952434703707695 0.009379203558404697\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3566,  0.6929,  0.4311, -0.5565,  0.4544,  0.8474, -0.1370, -0.3552,\n",
      "        -0.0945, -0.3491]) tensor([ 0.3869,  0.6840,  0.4708, -0.5616,  0.4366,  0.7905, -0.1110, -0.2639,\n",
      "        -0.0520, -0.3946]) tensor([ 0.4895,  0.7974,  0.4549, -0.5832,  0.5578,  0.9037, -0.1663, -0.4034,\n",
      "         0.0167, -0.5583])\n",
      "R[0]\n",
      "tensor([-0.0064], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02110096684284508 0.005658799762415583 2.925962088511369e-07 0.004538046823290643 0.17237690594792365 0.00025784309953451155 0.008679374194238336\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0122, -0.0737,  0.7406, -0.4165, -0.5016, -0.2889,  0.3547,  0.7733,\n",
      "         0.3969,  0.2385]) tensor([-0.0311, -0.0634,  0.6882, -0.4101, -0.4904, -0.2355,  0.2952,  0.6928,\n",
      "         0.3309,  0.2802]) tensor([-0.0330, -0.0913,  0.7282, -0.4074, -0.5263, -0.3157,  0.3622,  0.7850,\n",
      "         0.3963,  0.2519])\n",
      "R[0]\n",
      "tensor([0.0055], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0208254260327667 0.004945642935170327 2.9786222965810795e-07 0.004777485756669193 0.17338765305280684 0.0002206524983048439 0.008576482165313792\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0486,  0.3718,  0.6483, -0.5421, -0.0189,  0.3848,  0.0045,  0.1887,\n",
      "        -0.0836,  0.2109]) tensor([ 0.0842,  0.3767,  0.6558, -0.5400, -0.0174,  0.3595,  0.0185,  0.2341,\n",
      "        -0.0512,  0.1272]) tensor([ 0.0326,  0.3479,  0.7779, -0.5847, -0.1264,  0.2387,  0.0313,  0.4212,\n",
      "         0.0166,  0.2892])\n",
      "R[0]\n",
      "tensor([-0.0056], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020961560890078546 0.00544725744569223 2.8647002949355736e-07 0.004614239695307333 0.17120545469224452 0.0002016002759337425 0.008897143520385726\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4722,  0.6348,  0.7322, -0.6385,  0.3344,  0.6385, -0.0715, -0.0269,\n",
      "         0.1957, -0.2236]) tensor([ 0.4914,  0.6189,  0.7345, -0.6339,  0.3233,  0.5986, -0.0487,  0.0192,\n",
      "         0.2229, -0.2775]) tensor([ 0.5648,  0.8018,  0.6851, -0.6738,  0.5011,  0.8121, -0.1797, -0.2323,\n",
      "         0.1663, -0.3657])\n",
      "R[0]\n",
      "tensor([0.0003], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021017454186454416 0.005697723917626718 2.905236082568763e-07 0.004297726127901114 0.17290866561233997 0.0001522049233317375 0.00868730246432824\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4188, -0.0493,  0.2243, -0.3565, -0.4500,  0.0720,  0.1728, -0.1973,\n",
      "        -0.0191,  0.4243]) tensor([-0.3919, -0.0506,  0.2141, -0.3427, -0.4522,  0.0518,  0.1750, -0.1666,\n",
      "        -0.0101,  0.3630]) tensor([-0.3082,  0.0890,  0.2877, -0.4284, -0.3102,  0.1926,  0.1062, -0.2769,\n",
      "         0.0093,  0.3514])\n",
      "R[0]\n",
      "tensor([-0.0022], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020863575829192994 0.0050040573026453785 2.903364706838829e-07 0.004206515510275495 0.1740619199872017 0.00023504044115543366 0.008883602521615103\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3031, -0.1694, -0.4568, -0.0089, -0.3849,  0.5655, -0.1537, -0.7050,\n",
      "        -0.4299,  0.1729]) tensor([-0.2572, -0.1271, -0.4052, -0.0390, -0.3478,  0.5478, -0.1429, -0.6754,\n",
      "        -0.3905,  0.1153]) tensor([-0.2889,  0.0129, -0.0217, -0.2336, -0.3902,  0.5851, -0.4022, -0.3521,\n",
      "        -0.6305,  0.7674])\n",
      "R[0]\n",
      "tensor([-0.0074], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020931899128481746 0.00538114887179836 2.8125077024299114e-07 0.004468342629319523 0.1727533695399761 0.00022080856561660766 0.008408607594668865\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2606,  0.5652,  0.0252, -0.4057,  0.3760,  0.8032,  0.0066, -0.7279,\n",
      "         0.1050, -0.8081]) tensor([ 0.2847,  0.5676,  0.0667, -0.4109,  0.3774,  0.7809,  0.0082, -0.6680,\n",
      "         0.1164, -0.7944]) tensor([ 0.4292,  0.7150,  0.1916, -0.4906,  0.5088,  0.8883, -0.0764, -0.6849,\n",
      "         0.1604, -0.8433])\n",
      "R[0]\n",
      "tensor([-0.0225], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02064424051530659 0.004377068979323667 2.874842494549057e-07 0.003961439192877151 0.17173912851512432 0.00019615305960178376 0.00830593570903875\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3708,  0.5029,  0.8002, -0.6316,  0.1691,  0.4935, -0.0117,  0.1707,\n",
      "         0.1972, -0.0286]) tensor([ 0.4030,  0.5026,  0.7978, -0.6310,  0.1743,  0.4778, -0.0028,  0.1963,\n",
      "         0.2206, -0.0997]) tensor([ 0.4796,  0.6898,  0.7703, -0.6741,  0.3718,  0.6892, -0.1232, -0.0576,\n",
      "         0.1525, -0.1532])\n",
      "R[0]\n",
      "tensor([0.0010], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02070752076804638 0.004604808228599723 2.773444629013966e-07 0.004239228440565057 0.17515156073868274 0.00016831864416599275 0.009188095492892899\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5932, -0.5392,  0.1566, -0.0705, -0.9657, -0.6177,  0.5966,  0.5998,\n",
      "         0.2017,  0.3823]) tensor([-0.4751, -0.4994,  0.1634, -0.0648, -0.8839, -0.5758,  0.5957,  0.5934,\n",
      "         0.2399,  0.2366]) tensor([-0.6012, -0.6297, -0.0553,  0.0620, -0.9219, -0.5868,  0.7230,  0.4235,\n",
      "         0.2193,  0.0841])\n",
      "R[0]\n",
      "tensor([-0.0034], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0207693488355726 0.005046971084135293 2.807346990607584e-07 0.004242994042986538 0.17313774448633193 0.00017719074338674545 0.008587162883137352\n",
      "Average (on the epoch) training loss: 0.004389237070735544\n",
      "Episode average V value: 0\n",
      "epoch 37:\n",
      "Learning rate: 2.2528399544939195e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5898, -0.5880,  0.1179, -0.0335, -0.9978, -0.6651,  0.6424,  0.6285,\n",
      "         0.2348,  0.2923]) tensor([-0.5506, -0.4721,  0.1517, -0.1000, -0.9027, -0.4949,  0.4956,  0.4914,\n",
      "         0.1558,  0.3918]) tensor([-0.5535, -0.5148,  0.2843, -0.1202, -0.9947, -0.7037,  0.5677,  0.7569,\n",
      "         0.2373,  0.4929])\n",
      "R[0]\n",
      "tensor([-0.0028], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020952562240883706 0.005918885927210795 2.8507533338029134e-07 0.0046204208321869375 0.1727769018858671 0.00020787525177001954 0.00910302420204971\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3389, -0.4731, -0.3020,  0.0693, -0.1392,  0.3878,  0.3329, -0.1277,\n",
      "         0.0635, -0.4684]) tensor([ 0.2928, -0.4697, -0.2921,  0.0487, -0.1721,  0.3414,  0.3206, -0.0818,\n",
      "         0.0613, -0.4295]) tensor([ 0.1816, -0.4803, -0.3748,  0.0077, -0.3092,  0.4598,  0.1042,  0.0206,\n",
      "        -0.2450, -0.0988])\n",
      "R[0]\n",
      "tensor([0.0485], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020737539477646352 0.00495115729921963 2.7800151313783773e-07 0.004155950663494877 0.17402279706299306 0.000200831800699234 0.008762094048201106\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3081, -0.0377, -0.2609, -0.1127, -0.1289,  0.6524, -0.2202, -0.1785,\n",
      "        -0.3016, -0.3629]) tensor([ 0.3256,  0.0082, -0.1981, -0.1428, -0.0819,  0.6887, -0.2087, -0.1992,\n",
      "        -0.2667, -0.3555]) tensor([ 0.2350, -0.0280, -0.1353, -0.1541, -0.1989,  0.6904, -0.3029, -0.1742,\n",
      "        -0.3447, -0.0767])\n",
      "R[0]\n",
      "tensor([-0.0063], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021243789056316017 0.005411011265132402 2.716732948329081e-07 0.004558939742564689 0.17086945217847824 0.00015540374815464018 0.008972063008113764\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4774,  0.5136,  0.8483, -0.5877,  0.1673,  0.4025, -0.0656,  0.3810,\n",
      "         0.1659, -0.0060]) tensor([ 0.4880,  0.5750,  0.8479, -0.6198,  0.2266,  0.4738, -0.0909,  0.2887,\n",
      "         0.1675, -0.0483]) tensor([ 0.6099,  0.6820,  0.8007, -0.5654,  0.3883,  0.7104, -0.2645, -0.0598,\n",
      "         0.0517, -0.0196])\n",
      "R[0]\n",
      "tensor([-0.0071], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020601756785064936 0.006109637300571194 2.740465501744893e-07 0.004690471798181534 0.17421957811713218 0.00015739849209785463 0.008547416767221875\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5710,  0.1659, -0.2730, -0.1964,  0.2575,  0.7388, -0.0363, -0.3760,\n",
      "         0.0049, -0.9988]) tensor([ 0.4890,  0.1586, -0.2454, -0.2221,  0.2130,  0.6923, -0.0246, -0.3491,\n",
      "        -0.0017, -0.8965]) tensor([ 0.2917,  0.0803, -0.4093, -0.1739,  0.0655,  0.6590, -0.0270, -0.3319,\n",
      "        -0.1666, -0.8035])\n",
      "R[0]\n",
      "tensor([0.0593], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02050529190339148 0.005385566374563495 2.6807355033042766e-07 0.004440295005915687 0.17438480092585087 0.00021411295235157014 0.008653463239665144\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2760,  0.0325,  0.5233, -0.2609, -0.3208, -0.1216,  0.4121,  0.5917,\n",
      "         0.5563, -0.7383]) tensor([ 0.0847, -0.1916,  0.4270, -0.1682, -0.5820, -0.3492,  0.4958,  0.7626,\n",
      "         0.4981, -0.5523]) tensor([ 0.1577, -0.1318,  0.5405, -0.2458, -0.5733, -0.4024,  0.3806,  0.9182,\n",
      "         0.4923, -0.4061])\n",
      "R[0]\n",
      "tensor([-0.0059], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02068547384813428 0.005591431666318385 2.7084853388714694e-07 0.00412317661382258 0.1746829562187195 8.181928843259812e-05 0.008748098778887652\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5682, -0.6604, -0.1664,  0.1344, -0.8871, -0.5726,  0.7648,  0.3580,\n",
      "         0.2362, -0.1234]) tensor([-0.5425, -0.6208, -0.1531,  0.1231, -0.8469, -0.5565,  0.7489,  0.3439,\n",
      "         0.2326, -0.1320]) tensor([-0.6065, -0.6137,  0.0423,  0.0040, -0.9828, -0.6381,  0.6801,  0.5529,\n",
      "         0.2310,  0.2041])\n",
      "R[0]\n",
      "tensor([-0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020885514844208957 0.005496660771284951 2.6566562708296713e-07 0.004599267118494026 0.1768668674379587 0.00016361376643180846 0.008296882713097148\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1125,  0.4065, -0.0860, -0.3162,  0.2779,  0.7901,  0.0296, -0.8036,\n",
      "        -0.0538, -0.5609]) tensor([ 0.1390,  0.4335, -0.0406, -0.3387,  0.2916,  0.8007, -0.0022, -0.7659,\n",
      "        -0.0642, -0.5320]) tensor([-0.0302,  0.2462, -0.2691, -0.2202,  0.1566,  0.6963,  0.1351, -0.8517,\n",
      "        -0.0834, -0.5734])\n",
      "R[0]\n",
      "tensor([-0.0059], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021156059535220265 0.004994051124202087 2.7036800871371725e-07 0.00398630499583669 0.17409560342133046 0.00019451215118169784 0.008223867638967932\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1310,  0.2803,  0.3921, -0.5170, -0.1132,  0.3597, -0.0013, -0.3553,\n",
      "         0.0293,  0.2775]) tensor([-0.2024,  0.1182,  0.2810, -0.4225, -0.2332,  0.2670,  0.0455, -0.2778,\n",
      "        -0.0419,  0.3485]) tensor([-0.4778, -0.1828,  0.1679, -0.2723, -0.5541, -0.0203,  0.2393, -0.1527,\n",
      "        -0.0258,  0.4717])\n",
      "R[0]\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02081562065705657 0.005157857760401384 2.630539274690591e-07 0.004495983146072831 0.1745985408872366 0.00015682778507471084 0.008752281134424266\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0837, -0.1868,  0.2959, -0.1302, -0.4760, -0.2709,  0.5638,  0.5886,\n",
      "         0.4920, -0.7154]) tensor([ 0.0636, -0.1846,  0.2848, -0.1294, -0.4760, -0.2790,  0.5682,  0.5743,\n",
      "         0.4920, -0.6996]) tensor([ 0.2655,  0.0204,  0.5110, -0.2549, -0.3291, -0.1276,  0.4209,  0.5921,\n",
      "         0.5528, -0.7368])\n",
      "R[0]\n",
      "tensor([-0.0026], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02070795308239758 0.0056456897537354965 2.7238551325581284e-07 0.004470450839027762 0.17526799161732196 0.00020092979073524475 0.00876236136094667\n",
      "Average (on the epoch) training loss: 0.0044141260755597616\n",
      "Episode average V value: 0\n",
      "epoch 38:\n",
      "Learning rate: 2.0275559590445276e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5501, -0.7491, -0.7833,  0.5652, -0.3067, -0.0732,  0.1804,  0.2392,\n",
      "        -0.3323, -0.7315]) tensor([ 0.4883, -0.6257, -0.6386,  0.4462, -0.2918, -0.0503,  0.1724,  0.2152,\n",
      "        -0.2602, -0.6541]) tensor([ 0.6353, -0.6305, -0.5993,  0.5013, -0.3372, -0.0488, -0.0822,  0.4898,\n",
      "        -0.4896, -0.4243])\n",
      "R[0]\n",
      "tensor([0.1007], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021047254502773286 0.005848443056005635 2.7145032102282586e-07 0.004383853876497597 0.17353049056231976 0.00022227461636066436 0.008505041132564657\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3932,  0.0130, -0.1850, -0.1878, -0.2055,  0.4083,  0.0959, -0.6561,\n",
      "        -0.2897,  0.1970]) tensor([-0.3450,  0.0369, -0.1466, -0.1993, -0.1931,  0.3846,  0.0970, -0.6048,\n",
      "        -0.2567,  0.1372]) tensor([-0.4394, -0.0565, -0.4037, -0.0985, -0.2444,  0.3733,  0.1792, -0.7624,\n",
      "        -0.2466, -0.0598])\n",
      "R[0]\n",
      "tensor([-0.0114], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020861652677878737 0.005443864791377564 2.659916623457548e-07 0.004911609140282962 0.17445497207343577 0.00011728138476610184 0.008684303789050319\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5109,  0.4377,  0.9311, -0.5508, -0.0263,  0.1713,  0.1362,  0.5358,\n",
      "         0.6146, -0.6259]) tensor([ 6.1196e-01,  6.0572e-01,  9.5208e-01, -6.1731e-01,  1.4355e-01,\n",
      "         3.9483e-01, -5.4803e-04,  3.3245e-01,  5.6409e-01, -6.7775e-01]) tensor([ 0.5956,  0.6057,  0.9551, -0.6156,  0.1573,  0.3627,  0.0360,  0.3598,\n",
      "         0.5661, -0.6733])\n",
      "R[0]\n",
      "tensor([0.0004], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020903550451621414 0.005350278406709549 2.6610024374917885e-07 0.004483030677190982 0.17394574327766896 0.0001988183856010437 0.008565295646840242\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7683, -0.6885, -0.4328,  0.1378, -0.8647, -0.3003,  0.5934, -0.1215,\n",
      "        -0.1620,  0.1808]) tensor([-0.7152, -0.6159, -0.3956,  0.1076, -0.7972, -0.2584,  0.5562, -0.1410,\n",
      "        -0.1652,  0.1533]) tensor([-0.6590, -0.5191, -0.1673, -0.0037, -0.7682, -0.1839,  0.4280, -0.1709,\n",
      "        -0.1374,  0.4105])\n",
      "R[0]\n",
      "tensor([0.0162], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0205417063254863 0.006059677622419258 2.6312092759894766e-07 0.00469429773412412 0.1758998329937458 0.0002145066186785698 0.009479824239795561\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6491, -0.6296, -0.5425,  0.2036, -0.7495, -0.2923,  0.6556, -0.1029,\n",
      "        -0.0553, -0.2680]) tensor([-0.6330, -0.5795, -0.5003,  0.1748, -0.7161, -0.2670,  0.6177, -0.0984,\n",
      "        -0.0775, -0.2194]) tensor([-0.6586, -0.6966, -0.5340,  0.2415, -0.8304, -0.4011,  0.7141,  0.0063,\n",
      "         0.0286, -0.3354])\n",
      "R[0]\n",
      "tensor([0.0066], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0207793049197644 0.0044443520927234205 2.6461995105364623e-07 0.004287766321271192 0.17746401576697826 0.00015704961121082306 0.008488706663774792\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5188,  0.4740,  0.9238, -0.5550,  0.0611,  0.1874,  0.1903,  0.4523,\n",
      "         0.6409, -0.6517]) tensor([ 0.2653,  0.1040,  0.7495, -0.3783, -0.3155, -0.1855,  0.3868,  0.7215,\n",
      "         0.6245, -0.5122]) tensor([ 0.3114,  0.1144,  0.6128, -0.3313, -0.2463, -0.1738,  0.4519,  0.6552,\n",
      "         0.6505, -0.7539])\n",
      "R[0]\n",
      "tensor([-0.0054], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0206459617651999 0.0065166261112099165 2.6259935151529134e-07 0.004997038607602007 0.17662598884105682 0.00018866905570030212 0.008959972310345619\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7212, -0.5782, -0.1487,  0.0120, -0.8703, -0.3228,  0.5020,  0.0020,\n",
      "        -0.0938,  0.4116]) tensor([-0.6659, -0.4925, -0.1091, -0.0342, -0.7923, -0.2186,  0.4155, -0.0807,\n",
      "        -0.1143,  0.4342]) tensor([-0.6110, -0.3819,  0.0366, -0.1458, -0.7267, -0.1665,  0.3501, -0.0685,\n",
      "        -0.0714,  0.5119])\n",
      "R[0]\n",
      "tensor([-0.0059], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0210841425601393 0.006126624826865736 2.6263233561962807e-07 0.004956121792405611 0.1755341542363167 0.00015999262779951097 0.008724799002869986\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3916, -0.2130, -0.3647, -0.0301, -0.4861,  0.3511,  0.0069, -0.6167,\n",
      "        -0.2562,  0.0902]) tensor([-0.3809, -0.2030, -0.3351, -0.0426, -0.4800,  0.3267,  0.0200, -0.5826,\n",
      "        -0.2455,  0.0861]) tensor([-0.4735, -0.2447, -0.5528,  0.0147, -0.4590,  0.3979,  0.0517, -0.6789,\n",
      "        -0.3766,  0.0740])\n",
      "R[0]\n",
      "tensor([0.0294], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020389272570610045 0.004412434692305396 2.580194402099778e-07 0.0037647150743869134 0.17674259421229363 0.00022393520921468735 0.008718775057001039\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3053, -0.1803, -0.4325,  0.0227, -0.4320,  0.4261, -0.0437, -0.7599,\n",
      "        -0.2235, -0.0818]) tensor([-0.3024, -0.1695, -0.3967,  0.0042, -0.4284,  0.4074, -0.0381, -0.7278,\n",
      "        -0.2224, -0.0571]) tensor([-0.2105, -0.4060, -0.6656,  0.1860, -0.5212,  0.3133,  0.0266, -0.5921,\n",
      "        -0.2365, -0.2838])\n",
      "R[0]\n",
      "tensor([0.0231], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020640976663678885 0.005322519651384937 2.6105018521604963e-07 0.004362788933038246 0.17687590999901295 0.00024337925016880036 0.0089959442227846\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5178,  0.4078,  0.8609, -0.4987, -0.0233,  0.1759,  0.1616,  0.4897,\n",
      "         0.6049, -0.7204]) tensor([ 0.5376,  0.4720,  0.8454, -0.5242,  0.0636,  0.2681,  0.1240,  0.3574,\n",
      "         0.5852, -0.7349]) tensor([ 0.2379,  0.0145,  0.5271, -0.2644, -0.3557, -0.1671,  0.4385,  0.6225,\n",
      "         0.5692, -0.7186])\n",
      "R[0]\n",
      "tensor([0.0009], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020668157123029232 0.004837873699343618 2.6010439501078507e-07 0.004213934232655447 0.1771615588068962 0.0001733339801430702 0.008798856153269299\n",
      "Average (on the epoch) training loss: 0.0045055156389455075\n",
      "Episode average V value: 0\n",
      "epoch 39:\n",
      "Learning rate: 1.824800363140075e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2786,  0.0707,  0.5387, -0.4778, -0.5575, -0.0707,  0.0291,  0.3811,\n",
      "         0.1428,  0.5793]) tensor([-0.2432,  0.1407,  0.5065, -0.4930, -0.4718,  0.0649, -0.0583,  0.2336,\n",
      "         0.0966,  0.5649]) tensor([-0.1253,  0.3477,  0.6470, -0.6070, -0.2801,  0.1840, -0.1275,  0.1891,\n",
      "         0.0700,  0.5337])\n",
      "R[0]\n",
      "tensor([0.0020], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020756884571164848 0.005657481690243003 2.7099878860781245e-07 0.004349647115333937 0.17787633009254933 0.0002435363009572029 0.008818346004467457\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-3.3460e-01,  3.1912e-02,  2.7170e-01, -3.9767e-01, -3.5598e-01,\n",
      "         1.5993e-01,  1.2453e-01, -2.6061e-01, -3.3247e-04,  4.0000e-01]) tensor([-0.3656, -0.1026,  0.1819, -0.3148, -0.4460,  0.0795,  0.1696, -0.1795,\n",
      "        -0.0455,  0.4207]) tensor([-0.5798, -0.3310,  0.1044, -0.1894, -0.7046, -0.1644,  0.3260, -0.0304,\n",
      "        -0.0387,  0.5201])\n",
      "R[0]\n",
      "tensor([-0.0012], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020669293031096457 0.004808968081539205 2.6464440939832913e-07 0.004174432307307143 0.17551365067064761 0.00019356835633516312 0.008586614734609612\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4750, -0.1735,  0.1870, -0.2813, -0.5568, -0.0265,  0.2297, -0.1314,\n",
      "        -0.0264,  0.4890]) tensor([-0.4377, -0.1530,  0.1803, -0.2762, -0.5362, -0.0249,  0.2211, -0.1244,\n",
      "        -0.0179,  0.4163]) tensor([-0.3862, -0.0404,  0.2399, -0.3576, -0.4262,  0.0975,  0.1598, -0.2221,\n",
      "        -0.0111,  0.4336])\n",
      "R[0]\n",
      "tensor([-0.0034], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02089320692792535 0.005477815525890037 2.635931578822692e-07 0.004558425589930266 0.17531287531554698 0.00021065600216388703 0.009093706007639412\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1497, -0.5119, -0.4565,  0.1665, -0.4618, -0.1693,  0.5605,  0.0063,\n",
      "        -0.0139, -0.6296]) tensor([-0.0326, -0.4142, -0.3596,  0.1108, -0.3908, -0.0779,  0.4699,  0.0453,\n",
      "         0.0088, -0.6584]) tensor([-0.0760, -0.5036, -0.4418,  0.1608, -0.4410, -0.1382,  0.5239,  0.0443,\n",
      "        -0.0071, -0.6664])\n",
      "R[0]\n",
      "tensor([0.0088], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020750766485929488 0.005354689713632979 2.6457165441229335e-07 0.004227825496869627 0.17627621719241143 0.00022448230534791947 0.00837364596372936\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3903, -0.5843, -0.6988,  0.2292, -0.5775, -0.0111,  0.4004, -0.2807,\n",
      "        -0.2719, -0.2152]) tensor([-3.8690e-01, -5.3452e-01, -6.4057e-01,  1.8696e-01, -5.5102e-01,\n",
      "        -3.9758e-04,  3.7344e-01, -2.6528e-01, -2.8404e-01, -1.5461e-01]) tensor([-0.4633, -0.4753, -0.5604,  0.1634, -0.4974, -0.0072,  0.4438, -0.5175,\n",
      "        -0.2069, -0.0799])\n",
      "R[0]\n",
      "tensor([0.0282], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02089883192628622 0.0056004144151738725 2.657608439307069e-07 0.004750201226794161 0.17666556178033352 0.00018205445259809495 0.008561591555480846\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1276,  0.2685,  0.4616, -0.5284, -0.3269,  0.2662, -0.1808, -0.0033,\n",
      "         0.0540,  0.4504]) tensor([-0.1636,  0.2466,  0.4303, -0.5143, -0.3465,  0.2708, -0.1725, -0.0175,\n",
      "         0.0297,  0.4691]) tensor([-0.5195,  0.0250,  0.0600, -0.4330, -0.4741,  0.3341, -0.0576, -0.1256,\n",
      "        -0.3056,  0.5941])\n",
      "R[0]\n",
      "tensor([0.0113], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0208190412260592 0.005638179390749429 2.630782123702602e-07 0.00475710989756044 0.17839590895175933 0.0002072117403149605 0.008534553705831059\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0739,  0.5137,  0.8258, -0.6113, -0.0927,  0.3781, -0.4256,  0.2363,\n",
      "        -0.2958,  0.8599]) tensor([ 0.1050,  0.5451,  0.7789, -0.6058, -0.0342,  0.4495, -0.4567,  0.1432,\n",
      "        -0.2891,  0.7680]) tensor([ 0.0953,  0.4670,  0.8041, -0.5776, -0.1287,  0.4847, -0.4988,  0.1946,\n",
      "        -0.3254,  0.9141])\n",
      "R[0]\n",
      "tensor([-0.0051], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02096318269520998 0.005438935159414541 2.6146739689636436e-07 0.004093577327614184 0.17424854634702205 0.00016902977973222734 0.009222941933432593\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1691, -0.3501,  0.3946, -0.1703, -0.8237, -0.5796,  0.4430,  0.9330,\n",
      "         0.3449,  0.0130]) tensor([-0.1724, -0.2752,  0.4144, -0.2247, -0.7727, -0.4644,  0.3323,  0.8294,\n",
      "         0.2655,  0.1386]) tensor([-0.1751, -0.1027,  0.4059, -0.2712, -0.7235, -0.3265,  0.1493,  0.7698,\n",
      "         0.2121,  0.1062])\n",
      "R[0]\n",
      "tensor([0.0083], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02061297327838838 0.004248722307205753 2.599943339589572e-07 0.004173764646286145 0.176638196259737 0.0001976705640554428 0.008110617968952284\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.7282,  0.8644,  0.8704, -0.6761,  0.4869,  0.7062, -0.1227, -0.0459,\n",
      "         0.4452, -0.7772]) tensor([ 0.7142,  0.8766,  0.8100, -0.6601,  0.5243,  0.7949, -0.1832, -0.1577,\n",
      "         0.3463, -0.7177]) tensor([ 0.7547,  0.9167,  0.8486, -0.6916,  0.5528,  0.7830, -0.1605, -0.1367,\n",
      "         0.4222, -0.7865])\n",
      "R[0]\n",
      "tensor([0.0002], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020545887403190136 0.004611291441295179 2.588227145849942e-07 0.004158436197030824 0.17802156513929368 0.0001663860008120537 0.008715655251115095\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5961, -0.6330, -0.1604,  0.1119, -0.8574, -0.5522,  0.7792,  0.3042,\n",
      "         0.2372, -0.0950]) tensor([-0.6028, -0.5887, -0.1232,  0.0759, -0.8550, -0.4834,  0.6798,  0.2764,\n",
      "         0.1680,  0.0474]) tensor([-0.6196, -0.5796,  0.0799, -0.0408, -0.9637, -0.6270,  0.6817,  0.5484,\n",
      "         0.2520,  0.2474])\n",
      "R[0]\n",
      "tensor([-0.0084], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02089480876736343 0.005359291476834187 2.5988865029091813e-07 0.004479347284068354 0.17602143616974353 0.00015528607368469238 0.008566693419154035\n",
      "Average (on the epoch) training loss: 0.0043722767088795085\n",
      "Episode average V value: 0\n",
      "epoch 40:\n",
      "Learning rate: 1.6423203268260674e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0670, -0.2003,  0.2805, -0.1208, -0.4817, -0.2851,  0.5826,  0.5834,\n",
      "         0.4904, -0.7176]) tensor([-0.0716, -0.3500,  0.2341, -0.0684, -0.6830, -0.4405,  0.6105,  0.7253,\n",
      "         0.4284, -0.5334]) tensor([ 0.0041, -0.2583,  0.3870, -0.1577, -0.6702, -0.4803,  0.4770,  0.8549,\n",
      "         0.4498, -0.3705])\n",
      "R[0]\n",
      "tensor([-0.0080], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020635037694126366 0.005520908684062306 2.560743815962496e-07 0.004233198935224209 0.1765242883116007 0.0002065262272953987 0.009630410924437456\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0934,  0.2611,  0.5216, -0.5082, -0.0037,  0.4311,  0.0975, -0.0481,\n",
      "         0.0740,  0.1414]) tensor([ 0.1211,  0.2379,  0.5178, -0.4938, -0.0296,  0.3755,  0.1167,  0.0299,\n",
      "         0.1018,  0.0670]) tensor([ 0.0447,  0.1684,  0.6571, -0.5021, -0.1812,  0.1915,  0.1815,  0.2082,\n",
      "         0.2284,  0.1943])\n",
      "R[0]\n",
      "tensor([-0.0065], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021107335064560175 0.005655205390234187 2.5798381579988927e-07 0.004761617632640992 0.17374083665013312 0.00015151195973157883 0.00832740025484236\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1215,  0.3417, -0.0898, -0.2914,  0.2593,  0.8130,  0.0108, -0.8109,\n",
      "        -0.1062, -0.4421]) tensor([ 0.1534,  0.3729, -0.0416, -0.3182,  0.2727,  0.8206, -0.0220, -0.7678,\n",
      "        -0.1094, -0.4290]) tensor([ 0.0978,  0.2917,  0.0396, -0.3304,  0.2019,  0.7601,  0.0105, -0.6717,\n",
      "        -0.1365, -0.1756])\n",
      "R[0]\n",
      "tensor([-0.0013], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020640464805066587 0.004781768027998624 2.4668052390097727e-07 0.004450887608691119 0.17621748557686806 0.0002738727256655693 0.008865197973907925\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0991,  0.2421,  0.1698, -0.3119, -0.1820,  0.4223, -0.2225, -0.4960,\n",
      "        -0.1484,  0.2409]) tensor([-0.1055,  0.2235,  0.1721, -0.3069, -0.2029,  0.3811, -0.1959, -0.4577,\n",
      "        -0.1311,  0.2216]) tensor([-0.0239,  0.4094,  0.3977, -0.4456, -0.0983,  0.4881, -0.3955, -0.2745,\n",
      "        -0.3114,  0.5390])\n",
      "R[0]\n",
      "tensor([-0.0010], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020841128950938582 0.00492610567855445 2.578116849463186e-07 0.003859522134065628 0.17406634671986104 0.0001454065814614296 0.008532583861495368\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3965, -0.5750, -0.3894,  0.2002, -0.6662, -0.3699,  0.8137,  0.1097,\n",
      "         0.3158, -0.7963]) tensor([-0.1995, -0.4066, -0.2481,  0.1183, -0.5438, -0.2309,  0.6799,  0.1551,\n",
      "         0.3701, -0.9096]) tensor([-0.1448, -0.3919,  0.0008,  0.0212, -0.5782, -0.3516,  0.7207,  0.4187,\n",
      "         0.4202, -0.7281])\n",
      "R[0]\n",
      "tensor([-0.0040], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02064734811335802 0.005055443924240535 2.553587235851751e-07 0.004453477480448782 0.17606264857947826 0.00017711367458105086 0.008694382160552778\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1872,  0.7098,  0.6220, -0.6908,  0.2259,  0.6123, -0.2374, -0.2733,\n",
      "         0.0609,  0.1260]) tensor([ 0.0803,  0.5442,  0.4985, -0.5976,  0.0995,  0.5304, -0.1910, -0.2334,\n",
      "        -0.0305,  0.2370]) tensor([ 0.0415,  0.5721,  0.6615, -0.6874, -0.0217,  0.4311, -0.2470, -0.0476,\n",
      "         0.0399,  0.3740])\n",
      "R[0]\n",
      "tensor([0.0012], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02072705820761621 0.005864482701788802 2.4505589441048416e-07 0.004482913803658448 0.1771113605797291 0.00016790951043367387 0.008535151649615727\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0170, -0.2475,  0.2299, -0.0943, -0.5130, -0.3173,  0.6183,  0.5728,\n",
      "         0.4763, -0.7050]) tensor([ 0.1685, -0.0860,  0.3402, -0.1702, -0.4000, -0.1820,  0.5092,  0.5604,\n",
      "         0.5244, -0.8213]) tensor([ 0.2038, -0.0458,  0.4443, -0.2137, -0.3799, -0.1874,  0.4761,  0.6001,\n",
      "         0.5399, -0.7383])\n",
      "R[0]\n",
      "tensor([-0.0117], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0208247804120183 0.005083500571476179 2.398898498370272e-07 0.004686469158797991 0.17459643340110778 0.00018211636692285537 0.009574256274499931\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4802,  0.3426,  0.8233, -0.4620, -0.0766,  0.1089,  0.2130,  0.5305,\n",
      "         0.6085, -0.7123]) tensor([ 0.4651,  0.4170,  0.8042, -0.4938,  0.0024,  0.2194,  0.1615,  0.3694,\n",
      "         0.5683, -0.6864]) tensor([ 0.5738,  0.5299,  0.8986, -0.5518,  0.1074,  0.2931,  0.0993,  0.3859,\n",
      "         0.5768, -0.7318])\n",
      "R[0]\n",
      "tensor([0.0072], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020652668019756674 0.0043455703951221945 2.4535293955807446e-07 0.004224997133482247 0.1759439168572426 0.00014932022243738173 0.008754184354853351\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3248,  0.0200,  0.1021, -0.2686, -0.1730,  0.7181, -0.4230,  0.0706,\n",
      "        -0.4029,  0.1778]) tensor([ 0.3039, -0.0165,  0.1048, -0.2687, -0.1944,  0.6517, -0.3583,  0.0956,\n",
      "        -0.3473,  0.1471]) tensor([ 0.1488, -0.0100,  0.0826, -0.2462, -0.2594,  0.6644, -0.3614, -0.0269,\n",
      "        -0.3994,  0.2826])\n",
      "R[0]\n",
      "tensor([-0.0113], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02057688186876476 0.004627753176646365 2.446904947390749e-07 0.004118094149045647 0.17347413693368435 0.0001226261928677559 0.008661236920248484\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0759,  0.4096,  0.7012, -0.6380, -0.1964,  0.2387, -0.1330,  0.2204,\n",
      "         0.0431,  0.5289]) tensor([-0.0777,  0.3933,  0.6741, -0.6220, -0.2096,  0.2327, -0.1246,  0.2314,\n",
      "         0.0425,  0.4907]) tensor([ 0.0619,  0.5879,  0.7491, -0.7133, -0.0037,  0.4150, -0.2222,  0.0757,\n",
      "         0.0432,  0.4086])\n",
      "R[0]\n",
      "tensor([-0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020629215912893413 0.005105776140037051 2.456325352255817e-07 0.004475701161951292 0.17240633098781108 0.00018100743740797042 0.008440852568252012\n",
      "Average (on the epoch) training loss: 0.004374687919800635\n",
      "Episode average V value: 0\n",
      "epoch 41:\n",
      "Learning rate: 1.4780882941434607e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5501, -0.1895,  0.1848, -0.2538, -0.6557, -0.1223,  0.1851,  0.2020,\n",
      "        -0.2107,  0.6501]) tensor([-0.4724, -0.0913,  0.2004, -0.2948, -0.5478,  0.0029,  0.1052,  0.0823,\n",
      "        -0.2105,  0.5907]) tensor([-0.4407, -0.0964, -0.0509, -0.2000, -0.3746,  0.2734,  0.0940, -0.3654,\n",
      "        -0.3110,  0.4701])\n",
      "R[0]\n",
      "tensor([-0.0008], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021111387774348258 0.004928235363793647 2.400961014075165e-07 0.004686201916600112 0.17364126642048358 0.00010649415850639344 0.00972277429269161\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5067, -0.6407, -0.4774,  0.2376, -0.7232, -0.4103,  0.8430,  0.0574,\n",
      "         0.2747, -0.7238]) tensor([-0.3007, -0.4668, -0.3372,  0.1552, -0.5893, -0.2645,  0.7041,  0.0984,\n",
      "         0.3251, -0.8407]) tensor([-0.3051, -0.5005, -0.1263,  0.0981, -0.6918, -0.4287,  0.7466,  0.3517,\n",
      "         0.3638, -0.6150])\n",
      "R[0]\n",
      "tensor([-0.0015], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02049761151894927 0.00442752527398261 2.420751109610819e-07 0.004012406780209858 0.17200149700045586 0.0001572360545396805 0.009420331502449699\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1167,  0.6540,  0.5262, -0.6611,  0.2460,  0.4886, -0.0493, -0.1681,\n",
      "         0.0145,  0.0161]) tensor([ 0.0532,  0.5312,  0.4246, -0.5861,  0.1645,  0.4450, -0.0300, -0.1469,\n",
      "        -0.0707,  0.0995]) tensor([-0.1872,  0.3189,  0.5492, -0.5898, -0.2268,  0.0511,  0.0910,  0.2170,\n",
      "         0.0685,  0.3462])\n",
      "R[0]\n",
      "tensor([0.0051], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020599176447838544 0.004964609479895444 2.4674884764408487e-07 0.004335890838352498 0.17393435588479042 0.00019588308781385422 0.009422751027741469\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4325,  0.7533,  0.7164, -0.6722,  0.3979,  0.7724, -0.1772, -0.1157,\n",
      "         0.0653, -0.2491]) tensor([ 0.4514,  0.7382,  0.7374, -0.6714,  0.3816,  0.7199, -0.1408, -0.0502,\n",
      "         0.1088, -0.3046]) tensor([ 0.5139,  0.8556,  0.6670, -0.6877,  0.5061,  0.8687, -0.2179, -0.2309,\n",
      "         0.1030, -0.4454])\n",
      "R[0]\n",
      "tensor([0.0040], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02032202587649226 0.003790704589664529 2.3514627120846398e-07 0.003960541442036628 0.17620768661797045 0.00022022827714681626 0.009364782412303612\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4525,  0.2970,  0.7838, -0.4363, -0.1063,  0.0637,  0.2638,  0.5397,\n",
      "         0.6283, -0.7344]) tensor([ 0.4625,  0.3488,  0.7618, -0.4554, -0.0352,  0.1423,  0.2280,  0.4221,\n",
      "         0.6039, -0.7382]) tensor([ 0.1960, -0.0439,  0.4619, -0.2247, -0.3897, -0.2205,  0.4869,  0.6338,\n",
      "         0.5572, -0.7254])\n",
      "R[0]\n",
      "tensor([-0.0012], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02044518774934113 0.0050776862042839635 2.409205489612987e-07 0.00418917547975434 0.17531973308324814 0.0001673688217997551 0.009377713522000704\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3972, -0.0509,  0.2387, -0.3542, -0.4390,  0.0865,  0.1658, -0.2120,\n",
      "        -0.0114,  0.4438]) tensor([-0.4143, -0.1746,  0.1572, -0.2759, -0.5178,  0.0092,  0.2130, -0.1309,\n",
      "        -0.0444,  0.4427]) tensor([-0.6146, -0.3843,  0.0713, -0.1554, -0.7515, -0.2081,  0.3614, -0.0015,\n",
      "        -0.0435,  0.5230])\n",
      "R[0]\n",
      "tensor([-0.0019], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020619719685986637 0.004359798749275797 2.446631623485018e-07 0.003898760727432091 0.17411253033578397 0.00028802217543125153 0.009259342246630695\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2175, -0.4056,  0.2897, -0.1058, -0.8394, -0.5800,  0.5081,  0.8359,\n",
      "         0.3509, -0.0840]) tensor([-0.2249, -0.3372,  0.3161, -0.1588, -0.8015, -0.4767,  0.4004,  0.7506,\n",
      "         0.2742,  0.0513]) tensor([-0.3350, -0.5168,  0.0589,  0.0185, -0.8403, -0.5518,  0.6386,  0.6074,\n",
      "         0.3272, -0.2366])\n",
      "R[0]\n",
      "tensor([0.0026], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020742118399590254 0.0051891040367700045 2.3522596015368437e-07 0.004277491866552736 0.17595683632791043 0.00018390415608882904 0.009276256322686095\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3919,  0.2113,  0.7361, -0.3978, -0.1957, -0.0329,  0.3197,  0.6139,\n",
      "         0.6275, -0.7141]) tensor([ 0.3642,  0.2669,  0.7125, -0.4209, -0.1394,  0.0567,  0.2761,  0.4773,\n",
      "         0.5861, -0.6739]) tensor([ 0.4929,  0.3706,  0.8426, -0.4819, -0.0488,  0.1281,  0.2074,  0.5207,\n",
      "         0.6182, -0.7189])\n",
      "R[0]\n",
      "tensor([0.0079], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020680225742980837 0.005224936656755745 2.356641606837684e-07 0.004372555467241909 0.17572985790669918 0.00020718471705913544 0.009008621842600406\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1977,  0.3371,  0.8268, -0.5758, -0.0606,  0.3218,  0.0401,  0.4248,\n",
      "         0.1490,  0.1900]) tensor([ 0.2804,  0.4100,  0.7965, -0.5887,  0.0580,  0.4590, -0.0233,  0.2838,\n",
      "         0.1144,  0.1108]) tensor([ 0.4374,  0.4851,  0.7378, -0.5164,  0.2030,  0.6431, -0.1510,  0.0185,\n",
      "         0.0608,  0.0559])\n",
      "R[0]\n",
      "tensor([-0.0022], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02060449645295739 0.005032003159747546 2.3754196072900413e-07 0.004264586381905247 0.176990677267313 0.0001999411880970001 0.009122268696315586\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5123,  0.7341,  0.8876, -0.7630,  0.3124,  0.6817, -0.1463,  0.0065,\n",
      "         0.3067, -0.2453]) tensor([ 0.3783,  0.4773,  0.7790, -0.6335,  0.0697,  0.4244,  0.0132,  0.2056,\n",
      "         0.3498, -0.2275]) tensor([ 0.4933,  0.7568,  0.7971, -0.7697,  0.3540,  0.7670, -0.1549, -0.0954,\n",
      "         0.2793, -0.3201])\n",
      "R[0]\n",
      "tensor([0.0055], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020571897257119418 0.004533249985921429 2.3163959504302055e-07 0.004151201795815723 0.1743627150654793 0.0002324308678507805 0.008956170519231818\n",
      "Average (on the epoch) training loss: 0.004214881269590114\n",
      "Episode average V value: 0\n",
      "epoch 42:\n",
      "Learning rate: 1.3302794647291146e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6721, -0.7218, -0.6766,  0.2160, -0.8334, -0.2119,  0.5175, -0.0553,\n",
      "        -0.2348, -0.0432]) tensor([-0.6207, -0.6284, -0.6066,  0.1628, -0.7515, -0.1643,  0.4826, -0.0809,\n",
      "        -0.2279, -0.0439]) tensor([-0.4274, -0.5082, -0.7902,  0.2602, -0.5369,  0.1078,  0.2987, -0.6224,\n",
      "        -0.2676, -0.2111])\n",
      "R[0]\n",
      "tensor([0.0058], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020530499581247568 0.004436013216116408 2.3379685588054145e-07 0.00428648424660787 0.1742482605278492 0.00020683072507381438 0.009043993327999487\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1278,  0.2973,  0.8005, -0.5749, -0.2070,  0.1436,  0.1232,  0.3728,\n",
      "         0.3660, -0.0897]) tensor([ 0.1494,  0.3411,  0.7844, -0.5889, -0.1449,  0.2219,  0.0949,  0.2795,\n",
      "         0.3447, -0.1124]) tensor([ 0.3663,  0.4683,  0.9347, -0.6011, -0.0509,  0.2120,  0.0384,  0.3990,\n",
      "         0.4518, -0.3222])\n",
      "R[0]\n",
      "tensor([0.0086], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020388461984694004 0.004752031042728049 2.3381855022819308e-07 0.0039745441433624365 0.1758152579665184 0.00019322212040424346 0.008712968734675088\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6387,  0.9289,  0.5197, -0.6566,  0.6809,  0.9824, -0.1989, -0.5343,\n",
      "         0.2035, -0.7165]) tensor([ 0.5577,  0.8325,  0.4196, -0.5894,  0.6125,  0.9547, -0.2018, -0.5390,\n",
      "         0.0991, -0.6096]) tensor([ 0.3913,  0.6859,  0.2457, -0.5027,  0.4996,  0.9027, -0.0950, -0.6522,\n",
      "         0.0576, -0.6132])\n",
      "R[0]\n",
      "tensor([0.0049], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02038917777314782 0.004662746720634459 2.2227024527410323e-07 0.004093385517364368 0.17616190293431283 0.00018424343317747117 0.009036130981286987\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2821,  0.2250,  0.6729, -0.4634, -0.0817,  0.1634,  0.2209,  0.3383,\n",
      "         0.3996, -0.2103]) tensor([ 0.1768,  0.0760,  0.5921, -0.3862, -0.2338,  0.0192,  0.2905,  0.4280,\n",
      "         0.3864, -0.1508]) tensor([ 0.1961,  0.1335,  0.7374, -0.4606, -0.2185,  0.0161,  0.2689,  0.4989,\n",
      "         0.4197, -0.0572])\n",
      "R[0]\n",
      "tensor([0.0011], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02032298485375941 0.004841925424087094 2.3138003358269544e-07 0.0039829172524041495 0.17718581290543078 0.0002173621281981468 0.008448818672099151\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0237, -0.2841,  0.1779, -0.0721, -0.5288, -0.3285,  0.6504,  0.5451,\n",
      "         0.4663, -0.7098]) tensor([-0.0382, -0.2829,  0.1712, -0.0704, -0.5330, -0.3429,  0.6529,  0.5469,\n",
      "         0.4682, -0.6974]) tensor([ 0.1722, -0.0838,  0.4115, -0.1943, -0.4105, -0.2194,  0.5005,  0.6126,\n",
      "         0.5304, -0.7297])\n",
      "R[0]\n",
      "tensor([-0.0031], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020332784177735447 0.00441964645756525 2.2687404612042882e-07 0.004297279514488764 0.1783010627925396 0.00025018157064914706 0.00861723025480751\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5943, -0.4350, -0.3767,  0.2358, -0.1813, -0.1916,  0.2283,  0.5356,\n",
      "        -0.1030, -0.7309]) tensor([ 0.4764, -0.3388, -0.2751,  0.1394, -0.1832, -0.1584,  0.2428,  0.4478,\n",
      "        -0.0467, -0.6210]) tensor([ 0.5800, -0.4625, -0.4430,  0.2503, -0.1730, -0.1745,  0.2902,  0.4696,\n",
      "        -0.0521, -0.8640])\n",
      "R[0]\n",
      "tensor([0.0264], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02015784513950348 0.004784183911771833 2.2682074630608896e-07 0.003953749978973064 0.1755336240530014 0.00017187945544719696 0.008912058405578136\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1442, -0.5959, -0.4982,  0.0953, -0.4189,  0.2105,  0.4040, -0.1706,\n",
      "        -0.1498, -0.1308]) tensor([-0.1315, -0.5571, -0.4564,  0.0638, -0.4022,  0.1944,  0.3874, -0.1301,\n",
      "        -0.1419, -0.1286]) tensor([-0.1625, -0.4659, -0.4765, -0.0141, -0.4377,  0.3552,  0.1484, -0.0558,\n",
      "        -0.3984,  0.0755])\n",
      "R[0]\n",
      "tensor([0.0554], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02027099915035069 0.004047430877108127 2.296801277736904e-07 0.0038418673751293682 0.1767220818400383 0.0002588258758187294 0.009746074539143591\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0514, -0.5695, -0.6399,  0.1829, -0.5732,  0.2664, -0.0922, -0.1004,\n",
      "        -0.3498, -0.2247]) tensor([ 0.0258, -0.5428, -0.5605,  0.1474, -0.5669,  0.2766, -0.0586, -0.1004,\n",
      "        -0.3071, -0.1609]) tensor([ 0.0953, -0.6110, -0.6286,  0.1969, -0.5277,  0.1957,  0.0821, -0.0376,\n",
      "        -0.2461, -0.4017])\n",
      "R[0]\n",
      "tensor([-0.0031], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02018424948491156 0.004566318262259301 2.2469688494197726e-07 0.004128674267616588 0.17544676503539086 0.00022791529446840287 0.008374144034052733\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5047, -0.2095,  0.1353, -0.2556, -0.5699, -0.0258,  0.2615, -0.1668,\n",
      "        -0.0286,  0.4613]) tensor([-0.4499, -0.1462,  0.1478, -0.2771, -0.5000,  0.0577,  0.2092, -0.2527,\n",
      "        -0.0220,  0.4212]) tensor([-0.3928, -0.0461,  0.2159, -0.3491, -0.4162,  0.1126,  0.1725, -0.2607,\n",
      "        -0.0066,  0.4067])\n",
      "R[0]\n",
      "tensor([-0.0065], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02051633794978261 0.004724946857710165 2.3446479096378424e-07 0.004201069440110587 0.1760527597218752 0.0002741328924894333 0.009142980354488827\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0253,  0.4981,  0.4243, -0.5881, -0.0098,  0.5558, -0.2475, -0.3256,\n",
      "         0.0272,  0.2127]) tensor([ 0.0202,  0.4677,  0.3974, -0.5639, -0.0196,  0.5539, -0.2276, -0.3532,\n",
      "         0.0499,  0.1883]) tensor([-0.1773,  0.1693,  0.1326, -0.3574, -0.2635,  0.4115, -0.1367, -0.4654,\n",
      "        -0.0380,  0.2328])\n",
      "R[0]\n",
      "tensor([0.0032], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020686877181753518 0.0051120871318235004 2.2311863823176736e-07 0.004055879504769109 0.17584470181167125 0.00019593304395675658 0.008296864813310094\n",
      "Average (on the epoch) training loss: 0.004081585124082631\n",
      "Episode average V value: 0\n",
      "epoch 43:\n",
      "Learning rate: 1.1972515182562032e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4158,  0.5557,  0.7534, -0.6181,  0.2541,  0.5381,  0.0041,  0.0368,\n",
      "         0.2396, -0.1883]) tensor([ 0.4271,  0.5818,  0.7439, -0.6303,  0.2850,  0.5687, -0.0145, -0.0020,\n",
      "         0.2303, -0.2164]) tensor([ 0.4455,  0.6818,  0.8398, -0.7148,  0.3325,  0.6917, -0.1622, -0.0067,\n",
      "         0.0969,  0.0285])\n",
      "R[0]\n",
      "tensor([-0.0044], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020459398563951254 0.004768913882071501 2.2613633889534412e-07 0.004027210462198127 0.17543232695758343 0.00018386701494455337 0.0090407362980186\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2629,  0.2650,  0.2566, -0.2411,  0.0560,  0.7163, -0.2543, -0.5207,\n",
      "        -0.1326,  0.1011]) tensor([ 0.1893,  0.1474,  0.1703, -0.1755, -0.0308,  0.6409, -0.2152, -0.4722,\n",
      "        -0.1841,  0.1591]) tensor([ 0.1804,  0.1820,  0.1799, -0.1933, -0.0233,  0.6641, -0.2186, -0.5487,\n",
      "        -0.1508,  0.1288])\n",
      "R[0]\n",
      "tensor([-0.0164], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020815898086875676 0.004474740420773742 2.210806252662678e-07 0.004291484630142804 0.17403795525431634 0.00019385313987731932 0.008980361403198913\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2817,  0.2746,  0.6807, -0.4946, -0.0310,  0.2606,  0.1625,  0.2462,\n",
      "         0.3393, -0.1423]) tensor([ 0.2960,  0.3118,  0.6550, -0.5077,  0.0199,  0.3207,  0.1281,  0.1685,\n",
      "         0.3135, -0.1670]) tensor([ 0.2124,  0.1687,  0.7428, -0.4758, -0.1780,  0.0794,  0.2331,  0.4393,\n",
      "         0.3926, -0.0420])\n",
      "R[0]\n",
      "tensor([0.0016], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020619430201128126 0.005072326798646827 2.2054954622774402e-07 0.004173323249793612 0.17530774292349816 0.0001549357995390892 0.008977617346157785\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0240, -0.1019,  0.1112, -0.0419, -0.2095,  0.4919, -0.1381, -0.3508,\n",
      "        -0.3863,  0.5441]) tensor([ 0.0521, -0.0840,  0.1213, -0.0514, -0.1990,  0.4432, -0.1122, -0.3042,\n",
      "        -0.3307,  0.4271]) tensor([ 0.4362, -0.3133, -0.4153,  0.2192, -0.0687,  0.0446,  0.1517,  0.0732,\n",
      "        -0.3104, -0.5243])\n",
      "R[0]\n",
      "tensor([0.0039], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02045442421361804 0.004290945118078525 2.2716654800092328e-07 0.0037728687599883413 0.17654442545771598 0.00025235665589571 0.009425014086067676\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1791,  0.2595,  0.7549, -0.4008, -0.3170,  0.3601, -0.5346,  0.3822,\n",
      "        -0.2659,  0.8805]) tensor([ 0.1658,  0.3147,  0.7583, -0.4216, -0.2754,  0.3909, -0.5138,  0.3251,\n",
      "        -0.2210,  0.7912]) tensor([ 0.3845,  0.4527,  0.7421, -0.4619, -0.0817,  0.6759, -0.7074,  0.1690,\n",
      "        -0.3702,  0.7528])\n",
      "R[0]\n",
      "tensor([0.1014], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02062072741985321 0.004753762476902921 2.1968609786426896e-07 0.004285919912566896 0.17645443838834762 0.00017345797270536422 0.009276165131537709\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3866, -0.0434, -0.4554, -0.0778, -0.1876,  0.4002,  0.2284, -0.8236,\n",
      "        -0.1839, -0.2361]) tensor([-0.3640, -0.0855, -0.4630, -0.0451, -0.2065,  0.3951,  0.2044, -0.7515,\n",
      "        -0.2302, -0.1819]) tensor([-0.3106,  0.0999, -0.1886, -0.2021, -0.1165,  0.4937,  0.0467, -0.7385,\n",
      "        -0.2588,  0.0682])\n",
      "R[0]\n",
      "tensor([0.0127], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020529120037332177 0.005015983703531674 2.2671741872670738e-07 0.004218092435679864 0.17676633302867412 0.00017731978744268417 0.008949780338560231\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2515,  0.5328,  0.0869, -0.4081,  0.3836,  0.8400, -0.0257, -0.7258,\n",
      "         0.0026, -0.5697]) tensor([ 0.2860,  0.5379,  0.1352, -0.4197,  0.3787,  0.8070, -0.0221, -0.6480,\n",
      "         0.0292, -0.5847]) tensor([ 0.0962,  0.3332, -0.1544, -0.2711,  0.2511,  0.7596,  0.0876, -0.8404,\n",
      "        -0.0372, -0.6025])\n",
      "R[0]\n",
      "tensor([-0.0207], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020350977586582303 0.005158312990050035 2.228494874714215e-07 0.004201454488327727 0.17685810096561908 0.00019372355192899703 0.008309727394313085\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1367,  0.4118,  0.5958, -0.4313, -0.1196,  0.4417, -0.5326,  0.0118,\n",
      "        -0.3921,  0.7648]) tensor([ 0.1110,  0.4272,  0.5812, -0.4361, -0.1093,  0.4391, -0.5078, -0.0119,\n",
      "        -0.3650,  0.7067]) tensor([ 0.2380,  0.4899,  0.6081, -0.4603, -0.0730,  0.5975, -0.6907,  0.0163,\n",
      "        -0.4721,  0.7560])\n",
      "R[0]\n",
      "tensor([0.0859], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02076667940057814 0.00481377722795878 2.2326044715725856e-07 0.004433268134336686 0.17379499824345113 0.00027376092970371246 0.009002152041299269\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7625, -0.7529, -0.4867,  0.2235, -0.9217, -0.4625,  0.7291,  0.0357,\n",
      "        -0.0028, -0.1017]) tensor([-0.7580, -0.6999, -0.4171,  0.1765, -0.9176, -0.4022,  0.6380,  0.0334,\n",
      "        -0.0515,  0.0411]) tensor([-0.7524, -0.6478, -0.2640,  0.0905, -0.9004, -0.3913,  0.6013,  0.0132,\n",
      "        -0.0561,  0.2497])\n",
      "R[0]\n",
      "tensor([-0.0033], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02046004453487694 0.004324590192121832 2.2187558118957895e-07 0.0037294276113971135 0.17570138975977898 0.00026975464820861816 0.008399459097185173\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3794, -0.4422, -0.4270,  0.1614, -0.2007, -0.1923,  0.4399,  0.3468,\n",
      "         0.0581, -0.8429]) tensor([ 0.3100, -0.4184, -0.3854,  0.1135, -0.2263, -0.2022,  0.4302,  0.3469,\n",
      "         0.0623, -0.7381]) tensor([ 0.4568, -0.4683, -0.4210,  0.2189, -0.2097, -0.2808,  0.4200,  0.4722,\n",
      "         0.0335, -0.9165])\n",
      "R[0]\n",
      "tensor([0.0158], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02049847200885415 0.004628871513003105 2.1585859366268777e-07 0.004243735419644509 0.1761285465657711 0.00014683292806148528 0.008822729540872388\n",
      "Average (on the epoch) training loss: 0.004137678510407568\n",
      "Episode average V value: 0\n",
      "epoch 44:\n",
      "Learning rate: 1.0775263664305828e-06\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7308, -0.4944, -0.3133, -0.0301, -0.8678, -0.0943,  0.3020,  0.0960,\n",
      "        -0.2900,  0.3674]) tensor([-0.6602, -0.4025, -0.2644, -0.0701, -0.7730, -0.0502,  0.2916,  0.0530,\n",
      "        -0.2580,  0.2825]) tensor([-0.4986, -0.3549, -0.3410, -0.0133, -0.6640,  0.1675,  0.1074, -0.3359,\n",
      "        -0.2470,  0.2207])\n",
      "R[0]\n",
      "tensor([0.0004], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02025264902226627 0.0041055327658978055 2.1545117793664305e-07 0.003788262933085207 0.17857097209990025 0.00026002651453018187 0.009153789637726732\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5598,  0.9131,  0.8647, -0.7694,  0.5145,  0.8109, -0.2350, -0.0614,\n",
      "         0.1389, -0.2999]) tensor([ 0.5633,  0.9378,  0.8849, -0.7876,  0.5327,  0.8148, -0.2222, -0.0745,\n",
      "         0.1631, -0.3351]) tensor([ 0.6261,  0.9609,  0.7711, -0.7372,  0.5980,  0.8913, -0.2582, -0.2023,\n",
      "         0.1567, -0.4852])\n",
      "R[0]\n",
      "tensor([-0.0065], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020669222865253686 0.004732067977125553 2.1724296887271067e-07 0.004075618789327564 0.1775936936736107 0.00023229780793190003 0.009078757243172732\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0188,  0.4169,  0.5405, -0.5067, -0.0981,  0.4154, -0.3366, -0.1420,\n",
      "        -0.1913,  0.6087]) tensor([ 2.7489e-04,  3.9832e-01,  5.1737e-01, -4.9494e-01, -1.1825e-01,\n",
      "         3.8808e-01, -3.1437e-01, -1.2301e-01, -1.7757e-01,  5.6827e-01]) tensor([ 0.3297,  0.3859,  0.4450, -0.3749,  0.0033,  0.3113, -0.4366,  0.1772,\n",
      "        -0.3916,  0.3878])\n",
      "R[0]\n",
      "tensor([0.0171], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020395451249554754 0.004978165440559678 2.1868367576871606e-07 0.004080806832062081 0.1767873707264662 0.0002603932395577431 0.00860556973417988\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4564,  0.4334,  0.8061, -0.5102, -0.0359,  0.1827,  0.1377,  0.4486,\n",
      "         0.5517, -0.7030]) tensor([ 0.5493,  0.5870,  0.8346, -0.5711,  0.1172,  0.3827,  0.0178,  0.2709,\n",
      "         0.5102, -0.7450]) tensor([ 0.5633,  0.6078,  0.8590, -0.5874,  0.1615,  0.3808,  0.0320,  0.2798,\n",
      "         0.5182, -0.7329])\n",
      "R[0]\n",
      "tensor([-0.0004], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020467110646888612 0.0042981196872206055 2.221557151074194e-07 0.004109426552488003 0.17489929232001306 0.00022523233294486999 0.00905906136147678\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0419, -0.5256, -0.6525,  0.2022, -0.4769,  0.1489,  0.2111, -0.2208,\n",
      "        -0.2080, -0.4795]) tensor([-0.1002, -0.4959, -0.5860,  0.1537, -0.4920,  0.1269,  0.2158, -0.1990,\n",
      "        -0.2084, -0.3684]) tensor([-0.4062, -0.3875, -0.4389,  0.1132, -0.4990, -0.1057,  0.4460, -0.4033,\n",
      "        -0.0921, -0.2289])\n",
      "R[0]\n",
      "tensor([0.0532], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020552058013156058 0.004383984640244307 2.1468616668585127e-07 0.003971005485916976 0.178044742166996 0.0001933879107236862 0.00968100950686494\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1613,  0.2071,  0.4686, -0.4571, -0.3466,  0.0611,  0.0480,  0.3041,\n",
      "         0.0737,  0.2542]) tensor([-0.1483,  0.2408,  0.4485, -0.4624, -0.3013,  0.1378,  0.0021,  0.2083,\n",
      "         0.0505,  0.2575]) tensor([-0.1498,  0.0168,  0.1891, -0.2714, -0.4100,  0.1928, -0.0280, -0.0510,\n",
      "         0.0203,  0.2618])\n",
      "R[0]\n",
      "tensor([0.0006], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020788280306383966 0.005079897967265424 2.1947587907789057e-07 0.004649937667127233 0.17495254926383497 0.00021334443241357803 0.008779578986228444\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.7288, -0.6641, -0.3794,  0.1300, -0.8486, -0.3310,  0.6120, -0.0712,\n",
      "        -0.0983,  0.1312]) tensor([-0.5917, -0.5933, -0.3365,  0.1144, -0.7519, -0.2681,  0.5660, -0.0512,\n",
      "        -0.0780,  0.0250]) tensor([-0.5267, -0.4363, -0.4109,  0.0748, -0.5160,  0.0942,  0.4081, -0.4995,\n",
      "        -0.2141,  0.0567])\n",
      "R[0]\n",
      "tensor([0.0018], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020477958189323544 0.004301541729349993 2.202996151794423e-07 0.0039364210007479415 0.17610997734963893 0.00031788699328899383 0.008822034088138026\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0474, -0.1253,  0.0242, -0.1851, -0.3531,  0.5738, -0.2189, -0.0459,\n",
      "        -0.3284,  0.3299]) tensor([ 0.0527, -0.1503,  0.0343, -0.1687, -0.3540,  0.5398, -0.1524, -0.0182,\n",
      "        -0.2819,  0.2818]) tensor([ 0.0666, -0.0196,  0.0826, -0.2452, -0.2924,  0.6600, -0.3091, -0.1116,\n",
      "        -0.3603,  0.3725])\n",
      "R[0]\n",
      "tensor([-0.0139], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020794221874326467 0.004157631646230584 2.2088291848376685e-07 0.003712067428743467 0.17517794097959996 0.00023777709156274797 0.00855232029920444\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0090,  0.1230,  0.7661, -0.4083, -0.4386,  0.2322, -0.3498,  0.5163,\n",
      "        -0.2034,  0.9673]) tensor([ 0.0235,  0.1832,  0.7659, -0.4261, -0.3862,  0.2724, -0.3442,  0.4658,\n",
      "        -0.1648,  0.8577]) tensor([ 0.1941,  0.3173,  0.7795, -0.4593, -0.2224,  0.5065, -0.5244,  0.3128,\n",
      "        -0.3132,  0.8751])\n",
      "R[0]\n",
      "tensor([0.0486], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020623618902638554 0.004897122687576484 2.1466415483928357e-07 0.003998057525779587 0.17425738686323167 0.00021809119731187822 0.008975887405918911\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3091,  0.0781,  0.5612, -0.2877, -0.2806, -0.0904,  0.3927,  0.5850,\n",
      "         0.5690, -0.7594]) tensor([ 0.4183,  0.2350,  0.6382, -0.3592, -0.1629,  0.0673,  0.2820,  0.5003,\n",
      "         0.5764, -0.8307]) tensor([ 0.4289,  0.2438,  0.7153, -0.3888, -0.1445,  0.0452,  0.2779,  0.5421,\n",
      "         0.5903, -0.7501])\n",
      "R[0]\n",
      "tensor([-0.0070], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020779252791777253 0.004186659542352572 2.1858387741247042e-07 0.003865882222482469 0.17500596503913401 0.0002581649050116539 0.008363769542775117\n",
      "Average (on the epoch) training loss: 0.004018748643776052\n",
      "Episode average V value: 0\n",
      "epoch 45:\n",
      "Learning rate: 9.697737297875246e-07\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2669,  0.1933,  0.7437, -0.4780, -0.1423,  0.0940,  0.2369,  0.4561,\n",
      "         0.4252, -0.1402]) tensor([ 0.2954,  0.2046,  0.7140, -0.4760, -0.1067,  0.1174,  0.2307,  0.4163,\n",
      "         0.4224, -0.1975]) tensor([ 0.1759,  0.1116,  0.7799, -0.4724, -0.2643, -0.0354,  0.2817,  0.5767,\n",
      "         0.4352, -0.0061])\n",
      "R[0]\n",
      "tensor([0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02067994718439877 0.004896212354869931 2.168770443233825e-07 0.003968982430000324 0.1740129989683628 0.0002150023877620697 0.009055104834609666\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.7597,  0.9304,  0.8472, -0.6953,  0.5658,  0.7871, -0.1637, -0.1431,\n",
      "         0.4194, -0.7992]) tensor([ 0.7634,  0.9373,  0.8483, -0.6982,  0.5898,  0.7780, -0.1404, -0.1624,\n",
      "         0.4397, -0.8299]) tensor([ 0.7706,  0.9502,  0.8363, -0.7005,  0.5924,  0.8192, -0.1788, -0.1823,\n",
      "         0.4105, -0.8032])\n",
      "R[0]\n",
      "tensor([-0.0040], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020814580895006658 0.005717508542991709 2.1537375586433427e-07 0.004420804817113094 0.1739519390463829 0.00031421414017677307 0.00967530937155243\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6515, -0.5568, -0.6277,  0.0342, -0.7750,  0.1653,  0.2182, -0.0965,\n",
      "        -0.4189,  0.2486]) tensor([-0.5804, -0.4676, -0.5553, -0.0145, -0.6892,  0.1819,  0.2205, -0.1039,\n",
      "        -0.3730,  0.1701]) tensor([-0.3765, -0.3881, -0.7483,  0.1859, -0.4823,  0.2409,  0.1482, -0.6377,\n",
      "        -0.3345, -0.1955])\n",
      "R[0]\n",
      "tensor([0.0033], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020845810206606984 0.005425470467114792 2.159896834115216e-07 0.004406948037270922 0.17607076948881148 0.00026828645914793014 0.008128229964408092\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5564, -0.6090, -0.7342,  0.2631, -0.6006, -0.1159,  0.5526, -0.4096,\n",
      "        -0.1931, -0.2172]) tensor([-0.5516, -0.5646, -0.6485,  0.2161, -0.6012, -0.0774,  0.4859, -0.3914,\n",
      "        -0.2190, -0.0783]) tensor([-0.6155, -0.6441, -0.7225,  0.2489, -0.6786, -0.1606,  0.5557, -0.2944,\n",
      "        -0.2067, -0.1667])\n",
      "R[0]\n",
      "tensor([0.0075], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020758034640923143 0.005479946491370356 2.1096446041468654e-07 0.004527142232225743 0.17557921324670314 0.00021426349133253097 0.008947873958444688\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3811,  0.3926,  0.8087, -0.5386, -0.1256,  0.1145,  0.1609,  0.5289,\n",
      "         0.5712, -0.6124]) tensor([ 0.3742,  0.4623,  0.7920, -0.5683, -0.0409,  0.2263,  0.1216,  0.3647,\n",
      "         0.5379, -0.5945]) tensor([ 0.4949,  0.5195,  0.8628, -0.5759,  0.0369,  0.2685,  0.0795,  0.3887,\n",
      "         0.5570, -0.6677])\n",
      "R[0]\n",
      "tensor([0.0171], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020560040526092054 0.004649936635072663 2.1986241235083525e-07 0.004045659244933632 0.1772839000225067 0.00021825455874204637 0.009101577918976545\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5412, -0.0812, -0.0833, -0.2264, -0.4098, -0.0062,  0.3381, -0.2197,\n",
      "        -0.1192,  0.1401]) tensor([-0.5073, -0.0636, -0.0693, -0.2274, -0.3943, -0.0155,  0.3372, -0.1979,\n",
      "        -0.1079,  0.0968]) tensor([-0.5627, -0.0657,  0.1095, -0.2923, -0.4879, -0.0374,  0.2355, -0.0819,\n",
      "        -0.1744,  0.4321])\n",
      "R[0]\n",
      "tensor([-0.0021], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020746626522392033 0.004493259946430044 2.147545510240434e-07 0.0038849352655815892 0.17592130841314793 0.0002470177933573723 0.008970362340216524\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3768, -0.0142,  0.2587, -0.3802, -0.4038,  0.1002,  0.1714, -0.2219,\n",
      "         0.0196,  0.4047]) tensor([-0.3978, -0.1413,  0.1735, -0.3000, -0.4858,  0.0209,  0.2189, -0.1415,\n",
      "        -0.0161,  0.4069]) tensor([-0.3542, -0.0498,  0.1476, -0.3105, -0.3650,  0.1947,  0.1480, -0.3748,\n",
      "        -0.0420,  0.3616])\n",
      "R[0]\n",
      "tensor([-0.0048], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020927715571597217 0.004807095290769212 2.1590207576593913e-07 0.004162069882208016 0.17382599073648453 0.00024787481874227524 0.008683795067365281\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5920, -0.4702, -0.2944,  0.0812, -0.6715, -0.1889,  0.4691, -0.0692,\n",
      "        -0.1883,  0.1078]) tensor([-0.5558, -0.4021, -0.2340,  0.0338, -0.6233, -0.1208,  0.4014, -0.1085,\n",
      "        -0.1998,  0.1649]) tensor([-6.3350e-01, -5.0499e-01, -1.1599e-01, -5.0431e-04, -7.8099e-01,\n",
      "        -2.5465e-01,  4.5734e-01, -3.6143e-02, -8.4839e-02,  3.5525e-01])\n",
      "R[0]\n",
      "tensor([-0.0016], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020934321347624064 0.005652747453590564 2.1003536501496002e-07 0.0046173258282360624 0.1734495985955 0.0002285889610648155 0.00869521898124367\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5641, -0.4071, -0.3753,  0.0349, -0.5733, -0.0202,  0.3794, -0.4640,\n",
      "        -0.2003,  0.1359]) tensor([-0.5182, -0.3632, -0.3451,  0.0166, -0.5344, -0.0187,  0.3661, -0.4509,\n",
      "        -0.1855,  0.0886]) tensor([-0.3609, -0.3887, -0.5311,  0.1356, -0.5256, -0.0296,  0.3190, -0.4703,\n",
      "        -0.0862, -0.3054])\n",
      "R[0]\n",
      "tensor([0.0007], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02036931423097849 0.0045238007811385615 2.1225709878081033e-07 0.003648109115543775 0.17681176820397376 0.00022236837446689605 0.008909479015797842\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2409,  0.3854,  0.9195, -0.6516, -0.0658,  0.2881,  0.0256,  0.3561,\n",
      "         0.2869,  0.1691]) tensor([ 2.5699e-01,  4.1712e-01,  8.9692e-01, -6.6190e-01, -2.6035e-02,\n",
      "         3.4780e-01, -7.7382e-04,  2.9074e-01,  2.7047e-01,  1.2884e-01]) tensor([ 0.4202,  0.5639,  0.9577, -0.7033,  0.1342,  0.4731, -0.0756,  0.2073,\n",
      "         0.2916,  0.0128])\n",
      "R[0]\n",
      "tensor([0.0009], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020637784736230968 0.004873127472372289 2.1300603023632902e-07 0.004109833590395283 0.1771055228561163 0.00021689938753843306 0.008351297373184935\n",
      "Average (on the epoch) training loss: 0.004179181044350844\n",
      "Episode average V value: 0\n",
      "epoch 46:\n",
      "Learning rate: 8.727963568087721e-07\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4748,  0.3978,  0.7636, -0.4679, -0.0043,  0.1785,  0.2022,  0.4169,\n",
      "         0.5724, -0.7597]) tensor([ 0.2326,  0.0657,  0.6180, -0.3154, -0.3523, -0.1591,  0.3753,  0.6611,\n",
      "         0.5490, -0.5979]) tensor([ 0.2368, -0.0090,  0.4636, -0.2274, -0.3509, -0.1685,  0.4499,  0.5966,\n",
      "         0.5480, -0.7556])\n",
      "R[0]\n",
      "tensor([-0.0062], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02071552968584001 0.004476046972326003 2.0637982237303732e-07 0.0038899420020752588 0.17691990847885608 0.00019195333868265153 0.00853001249092631\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1032,  0.2051,  0.4482, -0.4678, -0.3851,  0.2456, -0.2010,  0.0522,\n",
      "         0.0488,  0.4633]) tensor([-0.1163,  0.1939,  0.4246, -0.4573, -0.3888,  0.2385, -0.1816,  0.0466,\n",
      "         0.0512,  0.4355]) tensor([ 0.1021,  0.3072,  0.7071, -0.4832, -0.2233,  0.2224, -0.3029,  0.2713,\n",
      "        -0.1053,  0.7436])\n",
      "R[0]\n",
      "tensor([0.0047], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020707747220993042 0.0054810538468154845 2.1850684593971437e-07 0.004179301774827763 0.17644762171804904 0.00032182759046554566 0.009622526456601917\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.0335,  0.0277,  0.7300, -0.3726, -0.5106,  0.1072, -0.2652,  0.6079,\n",
      "        -0.1686,  0.9615]) tensor([-0.0110,  0.0935,  0.7322, -0.3928, -0.4491,  0.1551, -0.2613,  0.5499,\n",
      "        -0.1296,  0.8482]) tensor([ 0.1280,  0.1934,  0.7376, -0.3849, -0.3629,  0.3600, -0.4881,  0.4193,\n",
      "        -0.2773,  0.9116])\n",
      "R[0]\n",
      "tensor([0.0483], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02084555903635919 0.005358241149042442 2.141330251816953e-07 0.00416387011250481 0.17462478590011596 0.00021618013083934783 0.009155465164338239\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6615, -0.4823, -0.3960,  0.2986, -0.2035, -0.1392,  0.1371,  0.5809,\n",
      "        -0.1921, -0.6520]) tensor([ 0.5461, -0.3476, -0.2625,  0.1792, -0.1842, -0.0939,  0.1621,  0.4701,\n",
      "        -0.1026, -0.5706]) tensor([ 0.6284, -0.4953, -0.4397,  0.2894, -0.1814, -0.1521,  0.2399,  0.5061,\n",
      "        -0.1098, -0.7886])\n",
      "R[0]\n",
      "tensor([0.0736], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0209638835657388 0.00483593550529622 2.1718153062977308e-07 0.003962119475065265 0.17486688160896302 0.00021424148976802827 0.008956885635037907\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3698, -0.2321,  0.4613, -0.2913, -0.8175, -0.3930,  0.1739,  0.6416,\n",
      "         0.1524,  0.6565]) tensor([-0.3542, -0.2054,  0.4424, -0.2903, -0.7696, -0.3539,  0.1806,  0.5806,\n",
      "         0.1514,  0.6103]) tensor([-0.3247, -0.2365,  0.5547, -0.2939, -0.7642, -0.3915,  0.1736,  0.7074,\n",
      "         0.0875,  0.8146])\n",
      "R[0]\n",
      "tensor([0.0049], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020353110171854497 0.004660705012207472 2.10672513389909e-07 0.003999647166754584 0.17474376825988291 0.00017577055096626283 0.00932044208131265\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5264,  0.4516,  0.9038, -0.5394,  0.0064,  0.1897,  0.1474,  0.5077,\n",
      "         0.6085, -0.6976]) tensor([ 0.6198,  0.6150,  0.9291, -0.6044,  0.1654,  0.4040,  0.0147,  0.3157,\n",
      "         0.5593, -0.7398]) tensor([ 0.6003,  0.6160,  0.9336, -0.6088,  0.1776,  0.3658,  0.0487,  0.3470,\n",
      "         0.5618, -0.7286])\n",
      "R[0]\n",
      "tensor([-0.0026], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020647564806044103 0.005358720474559959 2.1236944064639829e-07 0.003983495292137377 0.17270398753881455 0.00016299087554216384 0.008412448220013176\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4924,  0.5198,  0.8603, -0.5784,  0.0349,  0.2697,  0.0790,  0.3913,\n",
      "         0.5560, -0.6696]) tensor([ 0.2592,  0.1881,  0.7098, -0.4210, -0.2994, -0.0610,  0.2622,  0.6233,\n",
      "         0.5428, -0.5310]) tensor([ 0.1966,  0.0413,  0.5216, -0.3037, -0.3955, -0.1460,  0.3666,  0.6658,\n",
      "         0.5002, -0.6407])\n",
      "R[0]\n",
      "tensor([-0.0018], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02072231955640018 0.004730513349404646 2.1878589555512917e-07 0.003936190795619041 0.17541919219493865 0.0002640828490257263 0.008954773697711062\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1592,  0.3704,  0.7384, -0.4763, -0.1967,  0.4018, -0.5169,  0.2369,\n",
      "        -0.2853,  0.8484]) tensor([ 0.1879,  0.3993,  0.6978, -0.4765, -0.1389,  0.4603, -0.5344,  0.1537,\n",
      "        -0.2677,  0.7512]) tensor([ 0.1204,  0.2384,  0.7740, -0.4361, -0.3247,  0.2942, -0.4423,  0.4198,\n",
      "        -0.2164,  0.9030])\n",
      "R[0]\n",
      "tensor([-0.0081], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020638880567625164 0.004151405952688947 2.0657213470087753e-07 0.0036195164915989154 0.17614480765163898 0.0002520938217639923 0.009584075823309831\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2456,  0.0112, -0.2633, -0.1432, -0.3050,  0.4712, -0.0808, -0.7568,\n",
      "        -0.1165, -0.1175]) tensor([-0.2311,  0.0191, -0.2260, -0.1569, -0.3030,  0.4461, -0.0680, -0.7176,\n",
      "        -0.1008, -0.1153]) tensor([-0.1697,  0.1304, -0.0741, -0.2217, -0.2171,  0.5273, -0.2055, -0.6304,\n",
      "        -0.2427,  0.1224])\n",
      "R[0]\n",
      "tensor([-0.0014], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020951099088415505 0.004632830616890715 2.0885650239677035e-07 0.004036782138631679 0.1738410046249628 0.00024384616315364837 0.008813669638824649\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2447,  0.2181,  0.6799, -0.5793, -0.4561, -0.0177, -0.0205,  0.4604,\n",
      "         0.0975,  0.6219]) tensor([-0.2509,  0.1405,  0.6076, -0.5265, -0.4813, -0.0171,  0.0207,  0.4417,\n",
      "         0.0700,  0.6067]) tensor([-0.3300,  0.0769,  0.7150, -0.5276, -0.6012, -0.1853,  0.0645,  0.6165,\n",
      "         0.1301,  0.7621])\n",
      "R[0]\n",
      "tensor([0.0014], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020692567970603704 0.005566716230794555 2.1651374646580735e-07 0.004357745084445924 0.17463941487669946 0.00021737220138311387 0.009004407559026732\n",
      "Average (on the epoch) training loss: 0.004012861033366062\n",
      "Episode average V value: 0\n",
      "epoch 47:\n",
      "Learning rate: 7.855167211278949e-07\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4982, -0.1943, -0.0586, -0.1926, -0.4762,  0.0946,  0.2466, -0.4464,\n",
      "        -0.1034,  0.3144]) tensor([-0.4390, -0.1341, -0.0258, -0.2187, -0.4111,  0.1627,  0.2070, -0.5101,\n",
      "        -0.0804,  0.2783]) tensor([-0.3938, -0.0402,  0.0973, -0.3168, -0.3571,  0.1909,  0.1668, -0.4345,\n",
      "        -0.0526,  0.3426])\n",
      "R[0]\n",
      "tensor([-0.0055], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020870189497247338 0.00501505910607375 2.0876113640611039e-07 0.0040331790227792225 0.17219555714726448 0.00024300993978977202 0.008865135833038948\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2037,  0.4701,  0.1576, -0.4319,  0.3207,  0.8111, -0.0301, -0.6375,\n",
      "        -0.0499, -0.3312]) tensor([ 0.1482,  0.3716,  0.0798, -0.3663,  0.2516,  0.7684, -0.0268, -0.5977,\n",
      "        -0.1268, -0.2459]) tensor([ 0.3800,  0.7062,  0.3423, -0.5696,  0.4897,  0.8977, -0.1083, -0.5620,\n",
      "         0.0542, -0.4911])\n",
      "R[0]\n",
      "tensor([0.0028], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02063522491417825 0.004493700655391876 2.1276417935212067e-07 0.004218411853071302 0.17649111005663873 0.0002337692677974701 0.009228326584445312\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4359, -0.0970,  0.1568, -0.3205, -0.4509,  0.0881,  0.2100, -0.2850,\n",
      "        -0.0053,  0.3971]) tensor([-0.4252, -0.1091,  0.1373, -0.3042, -0.4626,  0.0719,  0.2069, -0.2558,\n",
      "        -0.0148,  0.3645]) tensor([-0.6378, -0.4309, -0.0170, -0.1059, -0.7563, -0.2179,  0.4139, -0.0614,\n",
      "        -0.0388,  0.4455])\n",
      "R[0]\n",
      "tensor([0.0037], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020490409079939126 0.004729342862276099 2.1119439517747197e-07 0.004322172837390099 0.17547451466321945 0.0001906038746237755 0.009152585313422605\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1855,  0.1977,  0.4018, -0.3917, -0.0210,  0.2849,  0.2292,  0.0081,\n",
      "         0.2711, -0.2691]) tensor([ 0.2607,  0.2515,  0.3761, -0.3967,  0.0691,  0.3949,  0.1501, -0.0792,\n",
      "         0.2199, -0.3041]) tensor([ 0.3402,  0.4536,  0.4921, -0.5055,  0.2365,  0.5377,  0.0944, -0.1662,\n",
      "         0.2314, -0.3662])\n",
      "R[0]\n",
      "tensor([0.0016], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02062532670237124 0.005110587815106555 2.0584362158615477e-07 0.0046010086651658635 0.17627558141946792 0.00017115592956542969 0.008794189447362441\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1502, -0.5643, -0.1509,  0.0429, -0.4949, -0.0542,  0.5302,  0.1048,\n",
      "         0.0853, -0.0554]) tensor([-0.0082, -0.4591, -0.1091,  0.0076, -0.3737,  0.0509,  0.4486,  0.0799,\n",
      "         0.1039, -0.1734]) tensor([ 0.1829, -0.4927, -0.2237,  0.0719, -0.2422,  0.1890,  0.4539, -0.0582,\n",
      "         0.1424, -0.4023])\n",
      "R[0]\n",
      "tensor([0.0017], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020594056714326143 0.005093405151950719 2.0399360748513118e-07 0.004027769119886216 0.17711930409073828 0.000269458644092083 0.009012984114233404\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6182,  0.9658,  0.6771, -0.7277,  0.6658,  0.9190, -0.2049, -0.4083,\n",
      "         0.1912, -0.5270]) tensor([ 0.5372,  0.8630,  0.5702, -0.6580,  0.5946,  0.8866, -0.2009, -0.4170,\n",
      "         0.0911, -0.4319]) tensor([ 0.3546,  0.7265,  0.3645, -0.5614,  0.4882,  0.8573, -0.1173, -0.5669,\n",
      "         0.0265, -0.4305])\n",
      "R[0]\n",
      "tensor([0.0012], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020691939491778614 0.0046162740739164295 2.0779860111019843e-07 0.0037533069723285736 0.17665528893470764 0.00026709675043821334 0.009144879964529536\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1384, -0.1133,  0.3649, -0.1753, -0.4403, -0.2404,  0.5116,  0.6141,\n",
      "         0.5123, -0.7301]) tensor([ 0.0899, -0.0939,  0.3526, -0.1848, -0.4334, -0.2132,  0.4997,  0.5534,\n",
      "         0.4912, -0.6737]) tensor([-0.0413, -0.3026,  0.1418, -0.0544, -0.5425, -0.3318,  0.6534,  0.5293,\n",
      "         0.4529, -0.7210])\n",
      "R[0]\n",
      "tensor([0.0033], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020874363711103797 0.006015136867074034 2.0964673262824363e-07 0.004827128073666245 0.17217783796787262 0.00032239247113466265 0.008630056773137766\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2981, -0.2025,  0.6030, -0.3618, -0.7887, -0.5199,  0.3434,  0.9632,\n",
      "         0.2743,  0.4818]) tensor([-0.2252, -0.2030,  0.5843, -0.3430, -0.7374, -0.4950,  0.3850,  0.9304,\n",
      "         0.3144,  0.3529]) tensor([-0.2794, -0.3433,  0.4270, -0.2128, -0.8610, -0.5956,  0.4426,  0.9257,\n",
      "         0.3136,  0.2101])\n",
      "R[0]\n",
      "tensor([0.0114], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02108143612742424 0.005672522691580525 2.1275709791268582e-07 0.0045114354013931005 0.17087526446580886 0.00017352355271577836 0.008306760744308122\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6379, -0.2123,  0.3296, -0.3794, -0.8450, -0.3523,  0.2426,  0.6038,\n",
      "        -0.0145,  0.7440]) tensor([-0.6018, -0.2745,  0.2961, -0.3317, -0.8544, -0.3779,  0.3118,  0.6300,\n",
      "         0.0092,  0.6673]) tensor([-0.7366, -0.5107, -0.0596, -0.0682, -0.9372, -0.4022,  0.4492,  0.4173,\n",
      "        -0.1411,  0.4959])\n",
      "R[0]\n",
      "tensor([0.0062], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020847402168437838 0.005124330080332583 2.1079195305162556e-07 0.0044153447453281845 0.17432723270356656 0.00023116736114025117 0.008861849255044945\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4189, -0.0645,  0.2189, -0.3473, -0.4414,  0.0632,  0.2090, -0.2199,\n",
      "         0.0158,  0.4026]) tensor([-0.4114, -0.0785,  0.1943, -0.3293, -0.4536,  0.0501,  0.2049, -0.1954,\n",
      "         0.0019,  0.3729]) tensor([-0.3148,  0.0708,  0.2847, -0.4204, -0.3096,  0.1801,  0.1383, -0.2909,\n",
      "         0.0360,  0.3451])\n",
      "R[0]\n",
      "tensor([0.0008], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020584696309641003 0.004441757158667315 2.0294325764780296e-07 0.0039036327113281004 0.17361145220696927 0.00027454208582639695 0.008872285677411128\n",
      "Average (on the epoch) training loss: 0.00426133894023369\n",
      "Episode average V value: 0\n",
      "epoch 48:\n",
      "Learning rate: 7.069650490151055e-07\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4812,  0.3325,  0.7784, -0.4422, -0.0675,  0.1258,  0.2190,  0.5067,\n",
      "         0.5872, -0.7505]) tensor([ 0.4643,  0.4076,  0.7626, -0.4750,  0.0101,  0.2342,  0.1683,  0.3492,\n",
      "         0.5492, -0.7222]) tensor([ 0.2276, -0.0182,  0.4640, -0.2326, -0.3578, -0.1585,  0.4554,  0.5999,\n",
      "         0.5412, -0.7527])\n",
      "R[0]\n",
      "tensor([0.0055], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020583905631676315 0.005489421307767771 2.062648163700942e-07 0.0048266367674223145 0.17465168924629687 0.00024051455408334731 0.009403791073185857\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.3095,  0.4520,  0.0168, -0.3623,  0.3897,  0.7718,  0.0847, -0.5809,\n",
      "         0.0164, -0.8153]) tensor([ 0.3374,  0.4648,  0.0620, -0.3751,  0.3974,  0.7470,  0.0854, -0.5214,\n",
      "         0.0384, -0.8196]) tensor([ 0.4095,  0.5331,  0.0776, -0.3931,  0.4498,  0.8156,  0.0382, -0.5933,\n",
      "         0.0895, -0.9058])\n",
      "R[0]\n",
      "tensor([-0.0198], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02077889433503151 0.005213055658270605 2.052610040266245e-07 0.004143516168929637 0.17375777105987072 0.0003104133680462837 0.009055668038374279\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4989, -0.3565,  0.4958, -0.2778, -0.9361, -0.6681,  0.4344,  0.8840,\n",
      "         0.2499,  0.6911]) tensor([-0.4437, -0.4061,  0.4559, -0.2331, -0.9287, -0.6915,  0.4989,  0.9016,\n",
      "         0.2849,  0.5728]) tensor([-0.5338, -0.5048,  0.3027, -0.1304, -0.9835, -0.7095,  0.5630,  0.7887,\n",
      "         0.2395,  0.4938])\n",
      "R[0]\n",
      "tensor([0.0092], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020827781580388546 0.005067572052423202 2.119752021627619e-07 0.004111790297029075 0.17336654315888883 0.00018581873923540117 0.00922898902237648\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.2443, -0.4082, -0.1345,  0.0874, -0.6038, -0.3027,  0.2386,  0.1393,\n",
      "        -0.2627,  0.1846]) tensor([-0.2114, -0.3927, -0.1296,  0.0829, -0.5588, -0.2720,  0.2475,  0.1124,\n",
      "        -0.2531,  0.1582]) tensor([-0.1801, -0.4195, -0.2072,  0.0759, -0.5243, -0.2268,  0.3856,  0.1006,\n",
      "        -0.1048, -0.1562])\n",
      "R[0]\n",
      "tensor([0.0039], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020770753061398865 0.00438808717489519 2.064297304258389e-07 0.004075941619812511 0.17655333596467973 0.0001879628896713257 0.008725783236674034\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3679, -0.0966,  0.4689, -0.4003, -0.6553, -0.2224,  0.2080,  0.4988,\n",
      "         0.1694,  0.5905]) tensor([-0.3179, -0.0030,  0.4517, -0.4310, -0.5523, -0.0668,  0.1004,  0.3428,\n",
      "         0.1193,  0.5803]) tensor([-0.2031,  0.2150,  0.6109, -0.5482, -0.3802,  0.0602, -0.0146,  0.3076,\n",
      "         0.0805,  0.5935])\n",
      "R[0]\n",
      "tensor([0.0011], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020661560390144586 0.004065225580452534 2.0780277641563317e-07 0.004165489674371202 0.17209277498722075 0.0002540998086333275 0.008823186329158489\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.4125, -0.1546,  0.0301, -0.2186, -0.4338,  0.0760,  0.2518, -0.3663,\n",
      "        -0.0329,  0.2598]) tensor([-0.3659, -0.1037,  0.0553, -0.2392, -0.3788,  0.1393,  0.2139, -0.4296,\n",
      "        -0.0180,  0.2386]) tensor([-0.4782, -0.3406, -0.2142, -0.0253, -0.5477, -0.0302,  0.3548, -0.3844,\n",
      "        -0.0907,  0.1100])\n",
      "R[0]\n",
      "tensor([-0.0048], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02080452604778111 0.006230686401278945 2.0643665847330794e-07 0.004934044460824225 0.17380728125572203 0.00020026619732379914 0.009202373854524921\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5237,  0.7137,  0.7149, -0.6558,  0.4191,  0.6746, -0.0642, -0.1174,\n",
      "         0.2571, -0.3895]) tensor([ 0.4024,  0.4766,  0.6292, -0.5414,  0.1907,  0.4330,  0.0839,  0.0769,\n",
      "         0.3034, -0.3623]) tensor([ 0.4129,  0.4890,  0.7203, -0.5821,  0.1897,  0.4441,  0.0728,  0.1300,\n",
      "         0.3189, -0.2929])\n",
      "R[0]\n",
      "tensor([0.0028], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020936131658032535 0.005450134585717023 2.0681771398756155e-07 0.004411554554360919 0.17150326244533062 0.00023480558395385742 0.008763088497682474\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0438, -0.4137, -0.4355,  0.1359, -0.4006,  0.0395,  0.3399, -0.0515,\n",
      "        -0.0099, -0.6800]) tensor([-0.0326, -0.4072, -0.3983,  0.1047, -0.4351,  0.0121,  0.3433, -0.0347,\n",
      "        -0.0219, -0.5587]) tensor([-0.4004, -0.4315, -0.3573,  0.1152, -0.5537, -0.2713,  0.5442, -0.1341,\n",
      "        -0.0541, -0.2724])\n",
      "R[0]\n",
      "tensor([0.0303], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.0203991242852062 0.004431424390510074 2.0700776832427436e-07 0.003928967907617334 0.17615552826225758 0.00025682182610034944 0.009543977150227874\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3266, -0.0256,  0.4143, -0.3646, -0.5995, -0.1803,  0.1640,  0.5098,\n",
      "         0.0783,  0.4188]) tensor([-0.2866,  0.0617,  0.4075, -0.3990, -0.5040, -0.0390,  0.0681,  0.3641,\n",
      "         0.0333,  0.4278]) tensor([-0.1991, -0.1736,  0.1505, -0.1789, -0.5747,  0.0259,  0.0334,  0.1223,\n",
      "         0.0082,  0.3744])\n",
      "R[0]\n",
      "tensor([0.0019], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02072349568270147 0.00515179393143626 2.0236476156298977e-07 0.004073380906076636 0.17148674435913563 0.00016611804813146592 0.00897749855130678\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1193,  0.1101,  0.8148, -0.4996, -0.3038, -0.0303,  0.2471,  0.5774,\n",
      "         0.3994,  0.1268]) tensor([ 0.1397,  0.0944,  0.7732, -0.4810, -0.2928, -0.0307,  0.2596,  0.5643,\n",
      "         0.3997,  0.0628]) tensor([ 0.0514,  0.0392,  0.7931, -0.4719, -0.3928, -0.1417,  0.2987,  0.6520,\n",
      "         0.4158,  0.1685])\n",
      "R[0]\n",
      "tensor([0.0020], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020911548925563694 0.005491538319904067 2.0567795128556554e-07 0.004299309664173052 0.17271039009094238 0.00022972628474235535 0.008424138349830173\n",
      "Average (on the epoch) training loss: 0.004297063202061691\n",
      "Episode average V value: 0\n",
      "epoch 49:\n",
      "Learning rate: 6.362685441135949e-07\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.4817, -0.3493, -0.1348,  0.0505, -0.1791, -0.3214,  0.3916,  0.7137,\n",
      "         0.0522, -0.6428]) tensor([ 0.4925, -0.2489, -0.0190, -0.0266, -0.1643, -0.2569,  0.3579,  0.7077,\n",
      "         0.1192, -0.6402]) tensor([ 0.5127, -0.1802, -0.1333, -0.0470, -0.0210,  0.0748,  0.3060,  0.2516,\n",
      "         0.1655, -0.6489])\n",
      "R[0]\n",
      "tensor([-0.0089], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02069698636047542 0.005052687564006192 2.0748368636702708e-07 0.0043193511219578795 0.17694543935358525 0.0002499006688594818 0.009169382726249752\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.1780, -0.0593, -0.5720, -0.0470, -0.0541,  0.3840,  0.4274, -0.7610,\n",
      "        -0.0123, -0.8218]) tensor([-0.1722, -0.0610, -0.5292, -0.0543, -0.0722,  0.3581,  0.4127, -0.6897,\n",
      "        -0.0211, -0.7623]) tensor([-0.1860, -0.0880, -0.5947, -0.0312, -0.0767,  0.3436,  0.4561, -0.7390,\n",
      "         0.0039, -0.8574])\n",
      "R[0]\n",
      "tensor([-0.0075], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02110236351750791 0.005712926628773858 2.1259202323165028e-07 0.004679073808016255 0.16939000679552554 0.00019995662569999694 0.009232754025200848\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.1864,  0.7468,  0.8065, -0.7767,  0.1641,  0.5303, -0.2828, -0.0174,\n",
      "         0.0538,  0.2974]) tensor([ 0.0871,  0.5939,  0.6848, -0.6908,  0.0525,  0.4658, -0.2325, -0.0019,\n",
      "        -0.0266,  0.3856]) tensor([-0.0953,  0.4420,  0.7682, -0.6896, -0.2113,  0.2126, -0.1564,  0.2569,\n",
      "         0.0507,  0.5965])\n",
      "R[0]\n",
      "tensor([-0.0005], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021079066745936872 0.005777708794961654 2.0583453040501355e-07 0.004775464497448411 0.17250176219642163 0.00021797391027212143 0.008400688939378597\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.5959, -0.5893,  0.0327, -0.0036, -0.9386, -0.5940,  0.6806,  0.5091,\n",
      "         0.2210,  0.1774]) tensor([-0.4472, -0.5009,  0.0659, -0.0208, -0.8182, -0.5142,  0.6461,  0.4873,\n",
      "         0.2629,  0.0149]) tensor([-0.2352, -0.3176,  0.3056, -0.1461, -0.7781, -0.5209,  0.4985,  0.7469,\n",
      "         0.3795, -0.1140])\n",
      "R[0]\n",
      "tensor([-0.0066], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02106969322822988 0.005409789012919646 2.033823944316282e-07 0.004106112915033009 0.17027624946832656 0.0002212644964456558 0.008854383393772878\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5193, -0.3655, -0.3651,  0.3158, -0.4118, -0.0684, -0.4931,  0.7370,\n",
      "        -0.8848,  0.1407]) tensor([ 0.5085, -0.2615, -0.1986,  0.1995, -0.3951, -0.1034, -0.4492,  0.7726,\n",
      "        -0.7483,  0.1181]) tensor([ 0.5807, -0.3119, -0.3400,  0.3094, -0.3566, -0.0316, -0.5366,  0.7172,\n",
      "        -0.8991,  0.1058])\n",
      "R[0]\n",
      "tensor([0.0642], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020755650805309413 0.0049192356687672145 2.0674332629511129e-07 0.003766666351002641 0.173596259072423 0.00022339238226413727 0.008834975231206044\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3449, -0.5791, -0.7409,  0.1825, -0.6358,  0.1243,  0.1728, -0.2205,\n",
      "        -0.4060, -0.0852]) tensor([-0.3249, -0.5250, -0.6400,  0.1213, -0.6192,  0.1462,  0.1311, -0.1924,\n",
      "        -0.3991,  0.0086]) tensor([-0.1477, -0.5528, -0.6406,  0.1903, -0.5515,  0.1370,  0.1965, -0.1817,\n",
      "        -0.2723, -0.2759])\n",
      "R[0]\n",
      "tensor([0.0047], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020944427086040378 0.0054685995172076216 2.038161177750908e-07 0.004292005415773019 0.17342324796319009 0.00024848461896181105 0.008777190317574422\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "2 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.3529,  0.0917, -0.0817, -0.2402, -0.1774,  0.3855,  0.0610, -0.6228,\n",
      "        -0.2392,  0.1949]) tensor([-0.2903,  0.1463, -0.0462, -0.2646, -0.1130,  0.4434,  0.0377, -0.6795,\n",
      "        -0.1973,  0.1385]) tensor([-0.4345, -0.0365, -0.3457, -0.1217, -0.2509,  0.3388,  0.1829, -0.7432,\n",
      "        -0.1985, -0.0655])\n",
      "R[0]\n",
      "tensor([-0.0004], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.02099033559858799 0.005107195959837554 2.1156173801273326e-07 0.004062182156834751 0.17418727296590805 0.00027426787465810774 0.008815757010946982\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.5072,  0.6766,  0.7208, -0.6447,  0.3842,  0.6368, -0.0381, -0.0773,\n",
      "         0.2697, -0.3721]) tensor([ 0.4986,  0.6613,  0.6436, -0.6151,  0.4064,  0.6970, -0.0848, -0.1616,\n",
      "         0.1804, -0.3363]) tensor([ 0.5904,  0.8354,  0.6662, -0.6816,  0.5451,  0.8050, -0.1374, -0.2781,\n",
      "         0.2416, -0.5152])\n",
      "R[0]\n",
      "tensor([0.0006], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021170978017151357 0.0053523253191488035 2.1016818507746393e-07 0.00430234653991647 0.1704192475527525 0.00028385522216558455 0.00852637054619845\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6142, -0.5186, -0.2957,  0.0606, -0.6963, -0.1252,  0.4518, -0.2871,\n",
      "        -0.1266,  0.2234]) tensor([-0.5789, -0.4808, -0.2818,  0.0487, -0.6604, -0.1181,  0.4344, -0.2816,\n",
      "        -0.1272,  0.1848]) tensor([-0.4564, -0.3383, -0.3535,  0.0194, -0.4170,  0.2206,  0.3045, -0.5798,\n",
      "        -0.2580,  0.1024])\n",
      "R[0]\n",
      "tensor([0.0049], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.021049921633675694 0.005283810812987212 2.1622898448470096e-07 0.004551875999488402 0.17335270063579084 0.0002067696526646614 0.008194214448216372\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "3 0.0 0.0\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.2111,  0.3584,  0.8949, -0.6025, -0.0760,  0.2688,  0.0408,  0.4978,\n",
      "         0.1988,  0.2023]) tensor([ 0.2961,  0.4386,  0.8607, -0.6185,  0.0517,  0.4198, -0.0276,  0.3379,\n",
      "         0.1610,  0.1180]) tensor([ 0.4612,  0.5228,  0.8287, -0.5646,  0.2079,  0.5961, -0.1331,  0.1136,\n",
      "         0.1280,  0.0426])\n",
      "R[0]\n",
      "tensor([-0.0021], grad_fn=<SelectBackward0>)\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q,                 self.loss_disentangle_t, self.loss_disambiguate1,                self.loss_disambiguate2\n",
      "0.020851646423339844 0.004715507876760967 2.1356181407838904e-07 0.004172542861255351 0.17245262709259987 0.00020063957571983337 0.00941189287789166\n",
      "Average (on the epoch) training loss: 0.004302762166672619\n",
      "Episode average V value: 0\n",
      "epoch 50:\n",
      "Learning rate: 5.726416897022355e-07\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n",
      "Testing score per episode (id: 0) is 2.0 (average over 1 episode(s))\n",
      "== Mean score per episode is 1.9998000199980002 over 1 episodes ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:351: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "/Users/chingfang/Code/deer/examples/test_CRAR/figure8_env.py:359: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(np.log(learning_algo.tracked_disamb1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best neural net obtained after 3 epochs, with validation score 102.0\n",
      "{'vs': [0.0, 1.0, 102.0, 91.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0], 'ts': []}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"params\")\n",
    "except Exception:\n",
    "    pass\n",
    "dump(vars(parameters), \"params/\" + fname + \".jldump\")\n",
    "#agent.gathering_data=False\n",
    "agent.run(parameters.epochs, parameters.steps_per_epoch)\n",
    "\n",
    "# --- Show results ---\n",
    "basename = \"scores/\" + fname\n",
    "scores = load(basename + \"_scores.jldump\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nnets/test_62977be8e45d8a56a5537c11dfd5d2fd8dda69e0.epoch=16'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnEpoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/deer/deer/agent.py:270\u001b[0m, in \u001b[0;36mNeuralAgent.setNetwork\u001b[0;34m(self, fname, nEpoch)\u001b[0m\n\u001b[1;32m    267\u001b[0m basename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnnets/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fname\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (nEpoch\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 270\u001b[0m     all_params \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.epoch=\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnEpoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     all_params \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(basename)\n",
      "File \u001b[0;32m~/miniforge3/envs/auxrl/lib/python3.10/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nnets/test_62977be8e45d8a56a5537c11dfd5d2fd8dda69e0.epoch=16'"
     ]
    }
   ],
   "source": [
    "agent.setNetwork(fname, nEpoch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._in_episode = True\n",
    "agent._mode = 0 # Testing mode with plan_depth=0\n",
    "initState = env.reset(agent._mode)\n",
    "inputDims = env.inputDimensions()\n",
    "\n",
    "for i in range(len(inputDims)):\n",
    "    if inputDims[i][0] > 1:\n",
    "        agent._state[i][1:] = initState[i][1:]\n",
    "agent._Vs_on_last_episode = []\n",
    "is_terminal = False\n",
    "reward = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame by frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for i in range(100):\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    plt.figure()\n",
    "    plt.imshow(np.flip(_obs.squeeze()))\n",
    "    plt.show()\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "    V, action, reward, _ = agent._step()\n",
    "    print(action)\n",
    "    agent._Vs_on_last_episode.append(V)\n",
    "    is_terminal = env.inTerminalState()\n",
    "    if is_terminal: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "obs = env.observe()\n",
    "_obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "_obs = np.flip(_obs.squeeze())\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "im = ax.imshow(np.zeros(_obs.shape))\n",
    "\n",
    "def init():\n",
    "    plt.cla()\n",
    "    im = ax.imshow(_obs)\n",
    "    return [im]\n",
    "\n",
    "def animate(i, *args, **kwargs):\n",
    "    plt.cla()\n",
    "    obs = env.observe()\n",
    "    _obs = obs[0].reshape((env.WIDTH, env.HEIGHT))\n",
    "    _obs = np.flip(_obs.squeeze())\n",
    "    im = ax.imshow(_obs)\n",
    "    for i in range(len(obs)):\n",
    "        agent._state[i][0:-1] = agent._state[i][1:]\n",
    "        agent._state[i][-1] = obs[i]\n",
    "        V, action, reward, _ = agent._step()\n",
    "        agent._Vs_on_last_episode.append(V)\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, init_func=init, \n",
    "     frames=100, blit=False, repeat=True)\n",
    "ani.save('behavior.gif', writer=\"ffmpeg\", fps = 15)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
