{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa004b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "from joblib import hash, dump\n",
    "import os\n",
    "\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.CRAR_torch import CRAR\n",
    "from catcher_env import MyEnv as catcher_env\n",
    "import deer.experiment.base_controllers as bc\n",
    "\n",
    "from deer.policies import EpsilonGreedyPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f01f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    steps_per_epoch = 2000\n",
    "    epochs = 50\n",
    "    steps_per_test = 500\n",
    "    period_btw_summary_perfs = 1\n",
    "    \n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    frame_skip = 2\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    update_rule = 'rmsprop'\n",
    "    learning_rate = 0.0005\n",
    "    learning_rate_decay = 0.9\n",
    "    discount = 0.9\n",
    "    discount_inc = 1\n",
    "    discount_max = 0.99\n",
    "    rms_decay = 0.9\n",
    "    rms_epsilon = 0.0001\n",
    "    momentum = 0\n",
    "    clip_norm = 1.0\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 1.0\n",
    "    epsilon_decay = 10000\n",
    "    update_frequency = 1\n",
    "    replay_memory_size = 1000000\n",
    "    batch_size = 32\n",
    "    freeze_interval = 1000\n",
    "    deterministic = False\n",
    "\n",
    "HIGHER_DIM_OBS = True\n",
    "HIGH_INT_DIM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d5dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = Defaults()\n",
    "if parameters.deterministic:\n",
    "    rng = np.random.RandomState(123456)\n",
    "else:\n",
    "    rng = np.random.RandomState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "130a13a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters hash is: 62977be8e45d8a56a5537c11dfd5d2fd8dda69e0\n",
      "The parameters are: <__main__.Defaults object at 0x2b613d43e700>\n"
     ]
    }
   ],
   "source": [
    "# --- Instantiate environment ---\n",
    "env = catcher_env(rng, higher_dim_obs=HIGHER_DIM_OBS, reverse=False)\n",
    "\n",
    "# --- Instantiate learning algorithm ---\n",
    "learning_algo = CRAR(\n",
    "    env,\n",
    "    parameters.rms_decay,\n",
    "    parameters.rms_epsilon,\n",
    "    parameters.momentum,\n",
    "    parameters.clip_norm,\n",
    "    parameters.freeze_interval,\n",
    "    parameters.batch_size,\n",
    "    parameters.update_rule,\n",
    "    rng,\n",
    "    double_Q=True,\n",
    "    high_int_dim=HIGH_INT_DIM,\n",
    "    internal_dim=3)\n",
    "\n",
    "test_policy = EpsilonGreedyPolicy(learning_algo, env.nActions(), rng, 0.1)#1.)\n",
    "\n",
    "# --- Instantiate agent ---\n",
    "agent = NeuralAgent(\n",
    "    env,\n",
    "    learning_algo,\n",
    "    parameters.replay_memory_size,\n",
    "    max(env.inputDimensions()[i][0] for i in range(len(env.inputDimensions()))),\n",
    "    parameters.batch_size,\n",
    "    rng,\n",
    "    test_policy=test_policy)\n",
    "\n",
    "# --- Create unique filename for FindBestController ---\n",
    "h = hash(vars(parameters), hash_name=\"sha1\")\n",
    "fname = \"test_\" + h\n",
    "print(\"The parameters hash is: {}\".format(h))\n",
    "print(\"The parameters are: {}\".format(parameters))\n",
    "\n",
    "# --- Bind controllers to the agent ---\n",
    "# Before every training epoch (periodicity=1), we want to print a summary of the agent's epsilon, discount and \n",
    "# learning rate as well as the training epoch number.\n",
    "agent.attach(bc.VerboseController(\n",
    "    evaluate_on='epoch', \n",
    "    periodicity=1))\n",
    "\n",
    "# As for the discount factor and the learning rate, one can update periodically the parameter of the epsilon-greedy\n",
    "# policy implemented by the agent. This controllers has a bit more capabilities, as it allows one to choose more\n",
    "# precisely when to update epsilon: after every X action, episode or epoch. This parameter can also be reset every\n",
    "# episode or epoch (or never, hence the resetEvery='none').\n",
    "agent.attach(bc.EpsilonController(\n",
    "    initial_e=parameters.epsilon_start, \n",
    "    e_decays=parameters.epsilon_decay, \n",
    "    e_min=parameters.epsilon_min,\n",
    "    evaluate_on='action',\n",
    "    periodicity=1,\n",
    "    reset_every='none'))\n",
    "\n",
    "# During training epochs, we want to train the agent after every [parameters.update_frequency] action it takes.\n",
    "# Plus, we also want to display after each training episode (!= than after every training) the average bellman\n",
    "# residual and the average of the V values obtained during the last episode, hence the two last arguments.\n",
    "agent.attach(bc.TrainerController(\n",
    "    evaluate_on='action', \n",
    "    periodicity=parameters.update_frequency, \n",
    "    show_episode_avg_V_value=True, \n",
    "    show_avg_Bellman_residual=True))\n",
    "\n",
    "# Every epoch end, one has the possibility to modify the learning rate using a LearningRateController. Here we \n",
    "# wish to update the learning rate after every training epoch (periodicity=1), according to the parameters given.\n",
    "agent.attach(bc.LearningRateController(\n",
    "    initial_learning_rate=parameters.learning_rate, \n",
    "    learning_rate_decay=parameters.learning_rate_decay,\n",
    "    periodicity=1))\n",
    "\n",
    "# Same for the discount factor.\n",
    "agent.attach(bc.DiscountFactorController(\n",
    "    initial_discount_factor=parameters.discount, \n",
    "    discount_factor_growth=parameters.discount_inc, \n",
    "    discount_factor_max=parameters.discount_max,\n",
    "    periodicity=1))\n",
    "\n",
    "# All previous controllers control the agent during the epochs it goes through. However, we want to interleave a \n",
    "# \"validation epoch\" between each training epoch (\"one of two epochs\", hence the periodicity=2). We do not want \n",
    "# these validation epoch to interfere with the training of the agent, which is well established by the \n",
    "# TrainerController, EpsilonController and alike. Therefore, we will disable these controllers for the whole \n",
    "# duration of the validation epochs interleaved this way, using the controllersToDisable argument of the \n",
    "# InterleavedTestEpochController. For each validation epoch, we want also to display the sum of all rewards \n",
    "# obtained, hence the showScore=True. Finally, we want to call the summarizePerformance method of ALE_env every \n",
    "# [parameters.period_btw_summary_perfs] *validation* epochs.\n",
    "agent.attach(bc.InterleavedTestEpochController(\n",
    "    id=catcher_env.VALIDATION_MODE, \n",
    "    epoch_length=parameters.steps_per_test,\n",
    "    periodicity=1,\n",
    "    show_score=True,\n",
    "    summarize_every=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61021848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 0.0 False\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([0.2541, 0.0169, 0.1931], device='cuda:0') tensor([ 0.4409, -0.2150,  0.0022], device='cuda:0') tensor([0.2537, 0.0178, 0.1930], device='cuda:0')\n",
      "R[0]\n",
      "tensor([-0.1063], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Average (on the epoch) training loss: 0.0003780538681894541\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.029045530211399582\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.04262078488962008\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.05514818562992981\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0639198447421553\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.06882521309911818\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.07229921795641853\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0750280910107622\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.07790412392714643\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.07709834261946485\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0794227595176139\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08097975831489714\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08244046074958469\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0815840392471291\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08193180817674581\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08207876075562286\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08509497640318985\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08428675102204466\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08581403789062546\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08717281285409331\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08856665602039318\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08846283234510485\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08808768413496403\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08801059072626062\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08803064933168311\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08874373563166604\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.08852230341001491\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09048783490401822\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.090920102839188\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0908944043687884\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09047865429361311\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09109323863722936\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09133325686392758\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09136266215479184\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09163448446416178\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09143894420588539\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09184641642892223\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09180248699501249\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09161429974998879\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09167639503704934\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09232671049492479\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0931784261287615\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09332250942517714\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09331373660839928\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09319446517899747\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09317746830466553\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09334040522176476\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09311831795317153\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09319213420557472\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09290611045811023\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09288978769901965\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09236455114114116\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09252307345337887\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09252513384640927\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09270715075019495\n",
      "Episode average V value: 0\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.032779327291995286 0.10065414298977703 0.5193750029802322 0.09287525758391711 0.3055924493819475 0.0003799918293952942 0.15169638055562973\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "0 -1.0 True\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.0208, -0.3715,  0.4815], device='cuda:0') tensor([ 0.0024, -0.5160,  0.6237], device='cuda:0') tensor([ 0.1855,  0.2299, -0.2186], device='cuda:0')\n",
      "R[0]\n",
      "tensor([-0.1238], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Average (on the epoch) training loss: 0.09301882073022592\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09292475896529595\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09288949385562897\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0927574469651895\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09301787556733518\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0930034781116389\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09335104505127276\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09315234916286737\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09311966964863645\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09300117035946978\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09293354764507304\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09311963338221182\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09287206738786741\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09300045839542778\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09321289266534045\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09340475655942178\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09365783935066217\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09343954272081624\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09329579866472001\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09323294471940012\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09306510162717387\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09298559401638205\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09294317410928336\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0929771428282576\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09279743243523411\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09275501055086792\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0925742194238637\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09289146291228428\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09288994302419601\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09267900042159359\n",
      "Episode average V value: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average (on the epoch) training loss: 0.09259345294839316\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09248162058690894\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09248784050328054\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09223797498365456\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09253621596950354\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09282596162123284\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09267384640090089\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09273470167170053\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09300407398603691\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09322550365750665\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09307410138901183\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09312032103313425\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09314668780251138\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09312848213810233\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09314890424116205\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0929386009971046\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0929966126612414\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09286234402390686\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09292072113983103\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0930128635893999\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09283378422886948\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09278521701199072\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09265773787397467\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09289631797458112\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09287789867301231\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09283161602377983\n",
      "Episode average V value: 0\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.02247823948599398 0.10315117711853236 0.5244887898564339 0.0929135136147961 0.1500104129612446 0.0005013270527124405 0.04689183901250362\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 0.0 False\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([-0.6433,  0.4050,  0.1511], device='cuda:0') tensor([-0.6154,  0.1702,  0.3449], device='cuda:0') tensor([-0.7583,  0.2303,  0.4028], device='cuda:0')\n",
      "R[0]\n",
      "tensor([-0.1179], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Average (on the epoch) training loss: 0.09282287785187114\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09314791342010034\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09314124166312424\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09306635866393119\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09299255115224928\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09281753850535325\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09294587447362646\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09294435482751796\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09302310664121509\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09297843515832938\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09295311757350008\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09312208333402362\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09297754699640987\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09294485660284149\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0930353551928882\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09286313846238625\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09290842088870907\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09274137626097995\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09267034940884654\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09278841866307784\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0926923546122631\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0927985951686095\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09283308643539517\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09262360735970727\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09259181463267945\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09248180741538913\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09239151228751535\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09233209778876043\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0923198623658266\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09233696367768814\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09227270069726273\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09230119873238923\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09225188711551144\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09219182684412833\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09217862963949713\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09215188035221014\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09205657748700868\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09213765739561441\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09193959504317077\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09175523302802611\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09178732340255596\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09182399778978305\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09177254527821525\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09176107558544219\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09155327468402372\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09168228754107069\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09161709710801764\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09153156524993825\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09140091313682969\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09136915192618555\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0913927790681434\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0914900195256392\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09140369403779791\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09138453771622075\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09130691903352779\n",
      "Episode average V value: 0\n",
      "self.loss_T, self.loss_R, self.loss_gamma, self.loss_Q, self.loss_disentangle_t, self.loss_disambiguate1, self.loss_disambiguate2\n",
      "0.015770775040611626 0.10300479375571013 0.5312866903543473 0.08777639149408788 0.13991086527705193 0.0006636285111308098 0.04191425161156803\n",
      "self.loss_interpret/500.\n",
      "0.0\n",
      "Printing a few elements useful for debugging:\n",
      "actions_val[0], rewards_val[0], terminals_val[0]\n",
      "1 -1.0 True\n",
      "Es[0], TEs[0], Esp_[0]\n",
      "tensor([ 0.6447, -0.9882,  0.8849], device='cuda:0') tensor([ 0.6561, -1.1549,  1.0957], device='cuda:0') tensor([-0.0142,  0.2352, -0.3654], device='cuda:0')\n",
      "R[0]\n",
      "tensor([-0.0437], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Average (on the epoch) training loss: 0.09115557307607652\n",
      "Episode average V value: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average (on the epoch) training loss: 0.09107767203922121\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09109434915946239\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09095292990836738\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09096505439384436\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09097631383042647\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09101106454116253\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09099002258882623\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09097762214508189\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09085767679505517\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09080154903926424\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09080093412670678\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09092289550473584\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09082332052627103\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09073540979570278\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09064164256413763\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0906591683770527\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09068297882809857\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09068469950933342\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09063439397548409\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09068735546447523\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09070328670906483\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09075970605635494\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09079204980222533\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09079165918535306\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09062354165007623\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09068429276893963\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0905325500784578\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09039616686979732\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09030375235201737\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09038648830925017\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09040519014223265\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09033963282910623\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09021013873459599\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09019571964400928\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09020269128556625\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09040154077649869\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0903753290951762\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09035126698535333\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09029821987082041\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0903334189055372\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09038741232341112\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09038383554697577\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09037334663323716\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09032067545612728\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09026866768564082\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09023178852006374\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09029533202037239\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09024563013201443\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09019920952756287\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0901461400086698\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09015682240364996\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09024280345562\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.0902160695510043\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09015360832361573\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09022012291246798\n",
      "Episode average V value: 0\n",
      "Average (on the epoch) training loss: 0.09022377613886354\n",
      "Episode average V value: 0\n",
      "epoch 1:\n",
      "Learning rate: 0.0005\n",
      "Discount factor: 0.9\n",
      "Epsilon: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cf2794/.conda/envs/auxrl/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dump(\u001b[38;5;28mvars\u001b[39m(parameters), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jldump\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/agent.py:270\u001b[0m, in \u001b[0;36mNeuralAgent.run\u001b[0;34m(self, n_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03mThis function encapsulates the inference and the learning.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03mIf the agent is in train mode (mode = -1):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m    maximum number of steps for a given epoch\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_non_train(n_epochs, epoch_length)\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/agent.py:297\u001b[0m, in \u001b[0;36mNeuralAgent._run_train\u001b[0;34m(self, n_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    295\u001b[0m         nbr_steps_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runEpisode(nbr_steps_left)\n\u001b[1;32m    296\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_controllers: \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monEpochEnd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_environment\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_controllers: c\u001b[38;5;241m.\u001b[39monEnd(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/experiment/base_controllers.py:340\u001b[0m, in \u001b[0;36mInterleavedTestEpochController.onEpochEnd\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    339\u001b[0m     agent\u001b[38;5;241m.\u001b[39mstartMode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch_length)\n\u001b[0;32m--> 340\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_non_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summary_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_score:\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/agent.py:320\u001b[0m, in \u001b[0;36mNeuralAgent._run_non_train\u001b[0;34m(self, n_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m nbr_steps_left \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_totalModeNbrEpisode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 320\u001b[0m     nbr_steps_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runEpisode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbr_steps_left\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_controllers: c\u001b[38;5;241m.\u001b[39monEpochEnd(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/agent.py:356\u001b[0m, in \u001b[0;36mNeuralAgent._runEpisode\u001b[0;34m(self, maxSteps)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state[i][\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state[i][\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state[i][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m obs[i]\n\u001b[0;32m--> 356\u001b[0m V, action, reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Vs_on_last_episode\u001b[38;5;241m.\u001b[39mappend(V)\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/agent.py:394\u001b[0m, in \u001b[0;36mNeuralAgent._step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m    This method is called at each time step and performs one action in the environment.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m        Reward obtained for the transition\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m     action, V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chooseAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msticky_action):\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/agent.py:412\u001b[0m, in \u001b[0;36mNeuralAgent._chooseAction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chooseAction\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;66;03m# Act according to the test policy if not in training mode\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m         action, V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset\u001b[38;5;241m.\u001b[39mn_elems \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replay_start_size:\n\u001b[1;32m    415\u001b[0m             \u001b[38;5;66;03m# follow the train policy\u001b[39;00m\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/policies/EpsilonGreedyPolicy.py:21\u001b[0m, in \u001b[0;36mEpsilonGreedyPolicy.action\u001b[0;34m(self, state, mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     action, V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandomAction()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     action, V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbestAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action, V\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/base_classes/policy.py:30\u001b[0m, in \u001b[0;36mPolicy.bestAction\u001b[0;34m(self, state, mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbestAction\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124;03m\"\"\" Returns the best Action for the given state. This is an additional encapsulation for q-network.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     action,V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchooseBestAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action, V\n",
      "File \u001b[0;32m/share/ctn/users/cf2794/Code/deer/deer/learning_algos/CRAR_torch.py:365\u001b[0m, in \u001b[0;36mCRAR.chooseBestAction\u001b[0;34m(self, state, mode, *args, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchooseBestAction\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, mode, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;124;03m\"\"\" Get the best action for a pseudo-state\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m    Arguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    The best action : int\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m     copy_state\u001b[38;5;241m=\u001b[39m\u001b[43mcopy\u001b[49m\u001b[38;5;241m.\u001b[39mdeepcopy(state) \u001b[38;5;66;03m#Required because of the \"hack\" below\u001b[39;00m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(mode\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    368\u001b[0m         mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Run the experiment ---\n",
    "try:\n",
    "    os.mkdir(\"params\")\n",
    "except Exception:\n",
    "    pass\n",
    "dump(vars(parameters), \"params/\" + fname + \".jldump\")\n",
    "agent.run(parameters.epochs, parameters.steps_per_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
