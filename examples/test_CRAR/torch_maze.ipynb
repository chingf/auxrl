{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "from joblib import hash, dump, load\n",
    "import os\n",
    "\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.CRAR_torch import CRAR\n",
    "from simple_maze_env_3d import MyEnv as simple_maze_env\n",
    "import deer.experiment.base_controllers as bc\n",
    "\n",
    "from deer.policies import EpsilonGreedyPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    steps_per_epoch = 5000\n",
    "    epochs = 50\n",
    "    steps_per_test = 1000\n",
    "    period_btw_summary_perfs = 1\n",
    "    \n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    frame_skip = 2\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    update_rule = 'rmsprop'\n",
    "    learning_rate = 5 * 1E-4 # 1E-4\n",
    "    learning_rate_decay = 0.9\n",
    "    discount = 0.9\n",
    "    discount_inc = 1\n",
    "    discount_max = 0.99\n",
    "    rms_decay = 0.9\n",
    "    rms_epsilon = 0.0001\n",
    "    momentum = 0\n",
    "    clip_norm = 1.0\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_min = 1.0\n",
    "    epsilon_decay = 10000\n",
    "    update_frequency = 1\n",
    "    replay_memory_size = 1000000 #replacing with 200000 will works just fine (in case you dont have 18gb of memory)\n",
    "    batch_size = 32\n",
    "    freeze_interval = 1000\n",
    "    deterministic = False\n",
    "\n",
    "higher_dim_obs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters hash is: 62977be8e45d8a56a5537c11dfd5d2fd8dda69e0\n",
      "The parameters are: <__main__.Defaults object at 0x11dd38430>\n",
      "end gathering data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters = Defaults()\n",
    "if parameters.deterministic:\n",
    "    rng = np.random.RandomState(123456)\n",
    "else:\n",
    "    rng = np.random.RandomState()\n",
    "\n",
    "# --- Instantiate environment ---\n",
    "env = simple_maze_env(rng, higher_dim_obs=higher_dim_obs)\n",
    "\n",
    "# --- Instantiate learning_algo ---\n",
    "learning_algo = CRAR(\n",
    "    env,\n",
    "    parameters.rms_decay,\n",
    "    parameters.rms_epsilon,\n",
    "    parameters.momentum,\n",
    "    parameters.clip_norm,\n",
    "    parameters.freeze_interval,\n",
    "    parameters.batch_size,\n",
    "    parameters.update_rule,\n",
    "    rng,\n",
    "    high_int_dim=False,\n",
    "    internal_dim=3, lr=parameters.learning_rate)\n",
    "\n",
    "test_policy = EpsilonGreedyPolicy(learning_algo, env.nActions(), rng, 1.)\n",
    "\n",
    "# --- Instantiate agent ---\n",
    "agent = NeuralAgent(\n",
    "    env,\n",
    "    learning_algo,\n",
    "    parameters.replay_memory_size,\n",
    "    max(env.inputDimensions()[i][0] for i in range(len(env.inputDimensions()))),\n",
    "    parameters.batch_size,\n",
    "    rng,\n",
    "    test_policy=test_policy)\n",
    "\n",
    "# --- Create unique filename for FindBestController ---\n",
    "h = hash(vars(parameters), hash_name=\"sha1\")\n",
    "fname = \"test_\" + h\n",
    "print(\"The parameters hash is: {}\".format(h))\n",
    "print(\"The parameters are: {}\".format(parameters))\n",
    "\n",
    "# As for the discount factor and the learning rate, one can update periodically the parameter of the epsilon-greedy\n",
    "# policy implemented by the agent. This controllers has a bit more capabilities, as it allows one to choose more\n",
    "# precisely when to update epsilon: after every X action, episode or epoch. This parameter can also be reset every\n",
    "# episode or epoch (or never, hence the resetEvery='none').\n",
    "agent.attach(bc.EpsilonController(\n",
    "    initial_e=parameters.epsilon_start,\n",
    "    e_decays=parameters.epsilon_decay,\n",
    "    e_min=parameters.epsilon_min,\n",
    "    evaluate_on='action',\n",
    "    periodicity=1,\n",
    "    reset_every='none'))\n",
    "\n",
    "agent.run(10, 500)\n",
    "print(\"end gathering data\")\n",
    "\n",
    "# --- Bind controllers to the agent ---\n",
    "# Before every training epoch (periodicity=1), we want to print a summary of the agent's epsilon, discount and \n",
    "# learning rate as well as the training epoch number.\n",
    "agent.attach(bc.VerboseController(\n",
    "    evaluate_on='epoch', \n",
    "    periodicity=1))\n",
    "\n",
    "# Every epoch end, one has the possibility to modify the learning rate using a LearningRateController. Here we \n",
    "# wish to update the learning rate after every training epoch (periodicity=1), according to the parameters given.\n",
    "agent.attach(bc.LearningRateController(\n",
    "    initial_learning_rate=parameters.learning_rate, \n",
    "    learning_rate_decay=parameters.learning_rate_decay,\n",
    "    periodicity=1))\n",
    "\n",
    "# Same for the discount factor.\n",
    "agent.attach(bc.DiscountFactorController(\n",
    "    initial_discount_factor=parameters.discount, \n",
    "    discount_factor_growth=parameters.discount_inc, \n",
    "    discount_factor_max=parameters.discount_max,\n",
    "    periodicity=1))\n",
    "\n",
    "# During training epochs, we want to train the agent after every [parameters.update_frequency] action it takes.\n",
    "# Plus, we also want to display after each training episode (!= than after every training) the average bellman\n",
    "# residual and the average of the V values obtained during the last episode, hence the two last arguments.\n",
    "agent.attach(bc.TrainerController(\n",
    "    evaluate_on='action', \n",
    "    periodicity=parameters.update_frequency, \n",
    "    show_episode_avg_V_value=True, \n",
    "    show_avg_Bellman_residual=True))\n",
    "\n",
    "# We wish to discover, among all versions of our neural network (i.e., after every training epoch), which one \n",
    "# has the highest validation score.\n",
    "# To achieve this goal, one can use the FindBestController along with an InterleavedTestEpochControllers. It is \n",
    "# important that the validationID is the same than the id argument of the InterleavedTestEpochController.\n",
    "# The FindBestController will dump on disk the validation scores for each and every network, as well as the \n",
    "# structure of the neural network having the best validation score. These dumps can then used to plot the evolution \n",
    "# of the validation and test scores (see below) or simply recover the resulting neural network for your \n",
    "# application.\n",
    "agent.attach(bc.FindBestController(\n",
    "    validationID=simple_maze_env.VALIDATION_MODE,\n",
    "    testID=None,\n",
    "    unique_fname=fname))\n",
    "\n",
    "# All previous controllers control the agent during the epochs it goes through. However, we want to interleave a \n",
    "# \"validation epoch\" between each training epoch. For each validation epoch, we want also to display the sum of all \n",
    "# rewards obtained, hence the showScore=True. Finally, we want to call the summarizePerformance method of ALE_env \n",
    "# every [parameters.period_btw_summary_perfs] *validation* epochs.\n",
    "agent.attach(bc.InterleavedTestEpochController(\n",
    "    id=simple_maze_env.VALIDATION_MODE, \n",
    "    epoch_length=parameters.steps_per_test,\n",
    "    periodicity=1,\n",
    "    show_score=True,\n",
    "    summarize_every=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<simple_maze_env_3d.MyEnv at 0x11dd384c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/chingfang/Code/deer/deer/learning_algos/CRAR_torch.py\u001b[0m(168)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    166 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    167 \u001b[0;31m        \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 168 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    169 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    170 \u001b[0;31m        \u001b[0monehot_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 100, got 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m dump(\u001b[38;5;28mvars\u001b[39m(parameters), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jldump\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m agent\u001b[38;5;241m.\u001b[39mgathering_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# --- Show results ---\u001b[39;00m\n\u001b[1;32m     10\u001b[0m basename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fname\n",
      "File \u001b[0;32m~/Code/deer/deer/agent.py:269\u001b[0m, in \u001b[0;36mNeuralAgent.run\u001b[0;34m(self, n_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03mThis function encapsulates the inference and the learning.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03mIf the agent is in train mode (mode = -1):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m    maximum number of steps for a given epoch\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_non_train(n_epochs, epoch_length)\n",
      "File \u001b[0;32m~/Code/deer/deer/agent.py:294\u001b[0m, in \u001b[0;36mNeuralAgent._run_train\u001b[0;34m(self, n_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_loss_averages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m nbr_steps_left \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# run new episodes until the number of steps left for the epoch has reached 0\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m     nbr_steps_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runEpisode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbr_steps_left\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_controllers: c\u001b[38;5;241m.\u001b[39monEpochEnd(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Code/deer/deer/agent.py:373\u001b[0m, in \u001b[0;36mNeuralAgent._runEpisode\u001b[0;34m(self, maxSteps)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_addSample(obs, action, reward, \u001b[38;5;28;01mTrue\u001b[39;00m)      \u001b[38;5;66;03m# If the episode ends because max number of steps is reached, mark the transition as terminal\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_controllers: \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monActionTaken\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_terminal:\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/deer/deer/experiment/base_controllers.py:407\u001b[0m, in \u001b[0;36mTrainerController.onActionTaken\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_action:\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/deer/deer/experiment/base_controllers.py:411\u001b[0m, in \u001b[0;36mTrainerController._update\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent):\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_periodicity \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_periodicity \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 411\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Code/deer/deer/agent.py:193\u001b[0m, in \u001b[0;36mNeuralAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     states, actions, rewards, next_states, terminals, rndValidIndices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset\u001b[38;5;241m.\u001b[39mrandomBatch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exp_priority)\n\u001b[0;32m--> 193\u001b[0m     loss, loss_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learning_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_loss_averages\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exp_priority):\n",
      "File \u001b[0;32m~/Code/deer/deer/learning_algos/CRAR_torch.py:181\u001b[0m, in \u001b[0;36mCRAR.train\u001b[0;34m(self, states_val, actions_val, rewards_val, next_states_val, terminals_val)\u001b[0m\n\u001b[1;32m    178\u001b[0m states_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(states_val, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    179\u001b[0m next_states_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(next_states_val, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m--> 181\u001b[0m Esp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m Es \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrar\u001b[38;5;241m.\u001b[39mencoder(states_val)\n\u001b[1;32m    183\u001b[0m Es_and_actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([Es, onehot_actions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Code/deer/deer/learning_algos/NN_CRAR_torch.py:134\u001b[0m, in \u001b[0;36mNN.encoder_model.<locals>.Encoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/nn/modules/rnn.py:767\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 767\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    769\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    770\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/nn/modules/rnn.py:692\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    688\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    689\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    690\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    691\u001b[0m                        ):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    694\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    696\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/auxrl/lib/python3.10/site-packages/torch/nn/modules/rnn.py:205\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    203\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    207\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 100, got 3"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"params\")\n",
    "except Exception:\n",
    "    pass\n",
    "dump(vars(parameters), \"params/\" + fname + \".jldump\")\n",
    "agent.gathering_data=False\n",
    "agent.run(parameters.epochs, parameters.steps_per_epoch)\n",
    "\n",
    "# --- Show results ---\n",
    "basename = \"scores/\" + fname\n",
    "scores = load(basename + \"_scores.jldump\")\n",
    "print (scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(env.inputDimensions()[i][0] for i in range(len(env.inputDimensions())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent._dataset._batch_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.inputDimensions()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
